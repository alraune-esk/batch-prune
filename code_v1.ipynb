{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from xml.dom import minidom\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from pthflops import count_ops\n",
    "import math\n",
    "import seaborn as sns\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset class for train, val and test data\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        result = {'inputs' : self.inputs[i], 'label': self.labels[i]}\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "# Extract appropriate label information regarding the 11 vehicle types\n",
    "# parse the xml file for label data\n",
    "\n",
    "root = \"./\"\n",
    "typeID_dict = defaultdict(list)\n",
    "labels = minidom.parse(\"finegrained_label.xml\")\n",
    "items = labels.getElementsByTagName('Item')\n",
    "\n",
    "type_id = {}\n",
    "\n",
    "for label in items:\n",
    "    type_id[label.attributes['imageName'].value] = label.attributes['typeID'].value\n",
    "\n",
    "# create datasets for train, val and test, extracting the npy files\n",
    "# and creating corresponding label dataset\n",
    "x_dict = defaultdict(list)\n",
    "y_dict = defaultdict(list)\n",
    "file_name = defaultdict(list)\n",
    "typeID_dict = defaultdict(list)\n",
    "dirs = ['train', 'val', 'test']\n",
    "for directory in dirs:\n",
    "    for file in os.listdir(\"./\" + directory):\n",
    "        data = np.load(\"./\" + directory + \"/\" + file )\n",
    "        x_dict[directory].append(data)\n",
    "        vehicle_id = file.split('_')\n",
    "        vehicle_id = vehicle_id[0].lstrip('0')\n",
    "        file_name[directory].append(file)\n",
    "        typeID_dict[directory].append(int(type_id[file]))\n",
    "        y_dict[directory].append(int(vehicle_id)-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for the train, val and test sets\n",
    "data_dict = {}    \n",
    "\n",
    "train_dataset = MyDataset(x_dict['train'], typeID_dict['train'])\n",
    "val_dataset = MyDataset(x_dict['val'], typeID_dict['val'])\n",
    "test_dataset = MyDataset(x_dict['test'], typeID_dict['test'])\n",
    "data_dict['train'] = train_dataset\n",
    "data_dict['val'] = val_dataset\n",
    "data_dict['test'] = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size= 1024\n",
    "# Dataloader for the sub datasets\n",
    "dataloader = {x: torch.utils.data.DataLoader(data_dict[x], batch_size=batch_size, shuffle=True) \n",
    "                    for x in ['train', 'val', 'test']}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the structure of the Neural Network\n",
    "# 1 hidden layer with dropout and relu \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, p):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(p) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(self.relu(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# conduct one train of the neural network \n",
    "\n",
    "def train(net, optimizer):\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # store all losses for visualisation and early stopping\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    lowest_val_loss = 999\n",
    "    # train the model by batch\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        total_loss = 0\n",
    "        train_batches = 0\n",
    "        for _, batch in enumerate(dataloader['train']):\n",
    "            X = batch['inputs'].to(device)\n",
    "            Y = batch['label'].to(device)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = net(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            train_batches += 1\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # calculate and print accuracy\n",
    "            train_total = train_total + predicted.size(0)\n",
    "            train_correct = train_correct + sum(predicted.data.cpu().numpy() == Y.data.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        \n",
    "        print('Epoch [%d/%d], Loss: %.4f, Accuracy: %.2f %%'\n",
    "                  % (epoch + 1, num_epochs,\n",
    "                     total_loss, 100 * train_correct/train_total))\n",
    "        all_train_losses.append(total_loss)\n",
    "        # Calculate validation loss for early stopping and model analysis\n",
    "        net.eval()\n",
    "        val_total_loss = 0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        val_batches = 0\n",
    "        for _, batch in enumerate(dataloader['val']):\n",
    "            X = batch['inputs'].to(device)\n",
    "            Y = batch['label'].to(device)\n",
    "            outputs = net(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "\n",
    "            val_total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += predicted.size(0)\n",
    "            val_correct += sum(predicted.data.cpu().numpy() == Y.data.cpu().numpy())\n",
    "            val_batches += 1\n",
    "        val_total_loss = (train_batches/val_batches) * val_total_loss\n",
    "        all_val_losses.append(val_total_loss)\n",
    "        print('Validation Accuracy: %.2f %%' % (100 * val_correct/val_total))\n",
    "        print(\"Validation Loss: \", val_total_loss)\n",
    "        \n",
    "        # early stopping, if the validation loss doesnt decrease from its lowest after 20 epochs then stop training\n",
    "        if val_total_loss < lowest_val_loss:\n",
    "            lowest_val_loss = val_total_loss\n",
    "            early_stopping_patience = 0\n",
    "        else:\n",
    "            early_stopping_patience += 1\n",
    "            if early_stopping_patience == patience:\n",
    "                break\n",
    "\n",
    "        \n",
    "    # Evaluation on test set\n",
    "    test_input = x_dict['test']\n",
    "    test_target = typeID_dict['test']\n",
    "\n",
    "    test_inputs = torch.Tensor(test_input).float().to(device)\n",
    "    test_targets = torch.Tensor(test_target).long().to(device)\n",
    "\n",
    "    test_outputs = net(test_inputs)\n",
    "\n",
    "\n",
    "    _, test_predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "    test_total = test_predicted.size(0)\n",
    "    test_correct = test_predicted.data.cpu().numpy() == test_targets.data.cpu().numpy()\n",
    "    test_acc = 100 * (sum(test_correct)/test_total)\n",
    "    print('Testing Accuracy: %.2f %%' % test_acc)\n",
    "    return test_acc        \n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(all_train_losses)\n",
    "#     plt.plot(all_test_losses)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.\"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning method for the pruning of the most similar neuron and most complementary neuron\n",
    "\n",
    "def prune_two_sim_comp(neural_net, hidden_size):\n",
    "    smallest_ang = 999\n",
    "    smallest_ang_pair = ()\n",
    "    largest_ang_pair = ()\n",
    "    largest_ang = 0\n",
    "    print(\"pruning hidden size: \", hidden_size)\n",
    "    print(\"with hidden layer: \", neural_net.fc1.weight.shape[0])\n",
    "    # iterate through for each neuron pair\n",
    "    for a in range(0, neural_net.fc1.weight.shape[0]):\n",
    "        for b in range(0, neural_net.fc1.weight.shape[0]):\n",
    "            if a != b:\n",
    "                neuron_a = neural_net.fc1.weight[a]\n",
    "                neuron_b = neural_net.fc1.weight[b]\n",
    "                # normalise the output of the neuron and calculate the angle \n",
    "                angle = np.degrees(angle_between(neuron_a.detach().cpu() -neuron_a.detach().cpu().mean(), \n",
    "                                                 neuron_b.detach().cpu() -neuron_b.detach().cpu().mean()))\n",
    "                # Store the smallest and largest\n",
    "                if angle < smallest_ang:\n",
    "                    smallest_ang = angle\n",
    "                    smallest_ang_pair = (a, b)\n",
    "                if angle > largest_ang:\n",
    "                    largest_ang = angle\n",
    "                    largest_ang_pair = (a, b)\n",
    "    \n",
    "    print(\"removing: \", smallest_ang_pair, \" and \", largest_ang_pair)\n",
    "    neurons_to_keep = list(range(0, hidden_size))\n",
    "    # Add one of the smallest and largest angle pairs to the other and remove it\n",
    "    neural_net.fc1.weight.data[smallest_ang_pair[1]] += neural_net.fc1.weight.data[smallest_ang_pair[0]]\n",
    "    neural_net.fc1.weight.data[largest_ang_pair[1]] += neural_net.fc1.weight.data[largest_ang_pair[0]] \n",
    "    neurons_to_keep.pop(smallest_ang_pair[0])\n",
    "    neurons_to_keep.pop(largest_ang_pair[0])\n",
    "    \n",
    "    neural_net.fc1.weight.data = neural_net.fc1.weight.data[neurons_to_keep]\n",
    "    neural_net.fc2.weight.data = neural_net.fc2.weight.data[:, neurons_to_keep]\n",
    "    # update the bias matrix\n",
    "    neural_net.fc1.bias.data = neural_net.fc1.bias.data[neurons_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning of only 1 of the most similar neurons\n",
    "def prune_one_sim(neural_net, hidden_size):\n",
    "    smallest_ang = 999\n",
    "    smallest_ang_pair = ()\n",
    "\n",
    "    print(\"pruning hidden size: \", hidden_size)\n",
    "    print(\"with hidden layer: \", neural_net.fc1.weight.shape[0])\n",
    "    for a in range(0, neural_net.fc1.weight.shape[0]):\n",
    "        for b in range(0, neural_net.fc1.weight.shape[0]):\n",
    "            if a != b:\n",
    "                neuron_a = neural_net.fc1.weight[a]\n",
    "                neuron_b = neural_net.fc1.weight[b]\n",
    "\n",
    "                angle = np.degrees(angle_between(neuron_a.detach().cpu() -neuron_a.detach().cpu().mean(), \n",
    "                                                 neuron_b.detach().cpu() -neuron_b.detach().cpu().mean()))\n",
    "                # store the smallest angle pair\n",
    "                if angle < smallest_ang:\n",
    "                    smallest_ang = angle\n",
    "                    smallest_ang_pair = (a, b)\n",
    "\n",
    "    # Add one of the smallest to the other and remove it\n",
    "    # update the bias and layer weight matrices\n",
    "    print(\"removing: \", smallest_ang_pair)\n",
    "    neurons_to_keep = list(range(0, hidden_size))\n",
    "    neural_net.fc1.weight.data[smallest_ang_pair[1]] += neural_net.fc1.weight.data[smallest_ang_pair[0]]\n",
    "\n",
    "    neurons_to_keep.pop(smallest_ang_pair[0])\n",
    "\n",
    "    neural_net.fc1.weight.data = neural_net.fc1.weight.data[neurons_to_keep]\n",
    "    neural_net.fc2.weight.data = neural_net.fc2.weight.data[:, neurons_to_keep]\n",
    "\n",
    "    neural_net.fc1.bias.data = neural_net.fc1.bias.data[neurons_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning for 2 similar neurons from a set of 3, find a neuron with the most similarity with 2 others, remove those 2 and \n",
    "# add them to the neuron\n",
    "def batch_prune_two_sim(neural_net, hidden_size):\n",
    "    smallest_ang = 999\n",
    "    smallest_ang_trio = ()\n",
    "\n",
    "    print(\"pruning hidden size: \", hidden_size)\n",
    "    print(\"with hidden layer: \", neural_net.fc1.weight.shape[0])\n",
    "    neuron_angle_dict = defaultdict(list)\n",
    "    smallest_angle = 999\n",
    "    smallest_angles = ()\n",
    "    middle = 0\n",
    "    for a in range(0, neural_net.fc1.weight.shape[0]):\n",
    "        neuron_angle_dict[a].append((0,999))\n",
    "        neuron_angle_dict[a].append((0,999))\n",
    "        for b in range(0, neural_net.fc1.weight.shape[0]):\n",
    "            \n",
    "            neuron_a = neural_net.fc1.weight[a]\n",
    "            neuron_b = neural_net.fc1.weight[b]\n",
    "            \n",
    "\n",
    "            angle = np.degrees(angle_between(neuron_a.detach().cpu() -neuron_a.detach().cpu().mean(), \n",
    "                                                   neuron_b.detach().cpu() -neuron_b.detach().cpu().mean()\n",
    "                                                   ))\n",
    "            # store the smallest 2 angle pairs for each neuron\n",
    "            if angle < neuron_angle_dict[a][0][1]:\n",
    "                if angle < neuron_angle_dict[a][1][1]:\n",
    "                    neuron_angle_dict[a][1] = (b, angle)\n",
    "                else:\n",
    "                    neuron_angle_dict[a][0] = (b, angle)\n",
    "    # find the smallest angle set                \n",
    "    for key, values in neuron_angle_dict.items():\n",
    "        angle = values[0][1] + values[1][1]\n",
    "        if angle < smallest_angle:\n",
    "            smallest_angle = angle\n",
    "            smallest_angles = (values[0][0], values[1][0])\n",
    "            middle = key\n",
    "\n",
    "    # remove the pair of smallest angles and add them to the remaining neuron, update the weight matrices\n",
    "    print(\"removing: \", smallest_angles)\n",
    "    neurons_to_keep = list(range(0, hidden_size))\n",
    "    neural_net.fc1.weight.data[middle] += neural_net.fc1.weight.data[smallest_angles[0]]\n",
    "    neural_net.fc1.weight.data[middle] += neural_net.fc1.weight.data[smallest_angles[1]]\n",
    "    neurons_to_keep.pop(smallest_angles[0])\n",
    "    neurons_to_keep.pop(smallest_angles[1])\n",
    "\n",
    "    neural_net.fc1.weight.data = neural_net.fc1.weight.data[neurons_to_keep]\n",
    "    neural_net.fc2.weight.data = neural_net.fc2.weight.data[:, neurons_to_keep]\n",
    "\n",
    "    neural_net.fc1.bias.data = neural_net.fc1.bias.data[neurons_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 96.9242, Accuracy: 20.62 %\n",
      "Validation Accuracy: 21.20 %\n",
      "Validation Loss:  94.29285907745361\n",
      "Epoch [2/500], Loss: 93.4041, Accuracy: 23.02 %\n",
      "Validation Accuracy: 25.78 %\n",
      "Validation Loss:  91.41974651813507\n",
      "Epoch [3/500], Loss: 90.5132, Accuracy: 26.46 %\n",
      "Validation Accuracy: 27.72 %\n",
      "Validation Loss:  88.71852672100067\n",
      "Epoch [4/500], Loss: 88.2619, Accuracy: 28.71 %\n",
      "Validation Accuracy: 30.61 %\n",
      "Validation Loss:  86.9157246351242\n",
      "Epoch [5/500], Loss: 86.5979, Accuracy: 30.44 %\n",
      "Validation Accuracy: 31.33 %\n",
      "Validation Loss:  85.3935899734497\n",
      "Epoch [6/500], Loss: 85.3232, Accuracy: 31.72 %\n",
      "Validation Accuracy: 32.14 %\n",
      "Validation Loss:  84.52500700950623\n",
      "Epoch [7/500], Loss: 84.4219, Accuracy: 32.29 %\n",
      "Validation Accuracy: 32.40 %\n",
      "Validation Loss:  83.68267714977264\n",
      "Epoch [8/500], Loss: 83.4492, Accuracy: 33.48 %\n",
      "Validation Accuracy: 34.63 %\n",
      "Validation Loss:  82.56753873825073\n",
      "Epoch [9/500], Loss: 82.6507, Accuracy: 34.24 %\n",
      "Validation Accuracy: 34.75 %\n",
      "Validation Loss:  82.06835889816284\n",
      "Epoch [10/500], Loss: 82.0222, Accuracy: 34.70 %\n",
      "Validation Accuracy: 34.86 %\n",
      "Validation Loss:  81.78329014778137\n",
      "Epoch [11/500], Loss: 81.3494, Accuracy: 35.24 %\n",
      "Validation Accuracy: 35.24 %\n",
      "Validation Loss:  81.06196296215057\n",
      "Epoch [12/500], Loss: 80.7860, Accuracy: 35.84 %\n",
      "Validation Accuracy: 36.20 %\n",
      "Validation Loss:  80.26284563541412\n",
      "Epoch [13/500], Loss: 80.5757, Accuracy: 35.95 %\n",
      "Validation Accuracy: 35.83 %\n",
      "Validation Loss:  80.3166321516037\n",
      "Epoch [14/500], Loss: 79.7014, Accuracy: 36.84 %\n",
      "Validation Accuracy: 36.38 %\n",
      "Validation Loss:  79.86811029911041\n",
      "Epoch [15/500], Loss: 79.4201, Accuracy: 37.09 %\n",
      "Validation Accuracy: 37.06 %\n",
      "Validation Loss:  79.24264633655548\n",
      "Epoch [16/500], Loss: 79.0168, Accuracy: 37.41 %\n",
      "Validation Accuracy: 37.01 %\n",
      "Validation Loss:  79.0293220281601\n",
      "Epoch [17/500], Loss: 78.6175, Accuracy: 37.91 %\n",
      "Validation Accuracy: 37.06 %\n",
      "Validation Loss:  78.71977579593658\n",
      "Epoch [18/500], Loss: 78.3109, Accuracy: 37.72 %\n",
      "Validation Accuracy: 36.52 %\n",
      "Validation Loss:  79.07413351535797\n",
      "Epoch [19/500], Loss: 78.0706, Accuracy: 38.11 %\n",
      "Validation Accuracy: 37.69 %\n",
      "Validation Loss:  78.3646205663681\n",
      "Epoch [20/500], Loss: 77.6700, Accuracy: 38.53 %\n",
      "Validation Accuracy: 37.92 %\n",
      "Validation Loss:  77.72954213619232\n",
      "Epoch [21/500], Loss: 77.3040, Accuracy: 38.64 %\n",
      "Validation Accuracy: 37.87 %\n",
      "Validation Loss:  77.83419620990753\n",
      "Epoch [22/500], Loss: 77.1123, Accuracy: 38.71 %\n",
      "Validation Accuracy: 38.97 %\n",
      "Validation Loss:  77.39660310745239\n",
      "Epoch [23/500], Loss: 76.7392, Accuracy: 39.47 %\n",
      "Validation Accuracy: 38.82 %\n",
      "Validation Loss:  77.03635311126709\n",
      "Epoch [24/500], Loss: 76.4535, Accuracy: 39.59 %\n",
      "Validation Accuracy: 39.10 %\n",
      "Validation Loss:  76.82497322559357\n",
      "Epoch [25/500], Loss: 76.2277, Accuracy: 39.81 %\n",
      "Validation Accuracy: 39.17 %\n",
      "Validation Loss:  76.65167033672333\n",
      "Epoch [26/500], Loss: 75.9026, Accuracy: 39.89 %\n",
      "Validation Accuracy: 39.35 %\n",
      "Validation Loss:  76.47840785980225\n",
      "Epoch [27/500], Loss: 75.7566, Accuracy: 39.80 %\n",
      "Validation Accuracy: 39.35 %\n",
      "Validation Loss:  76.41477477550507\n",
      "Epoch [28/500], Loss: 75.5511, Accuracy: 40.11 %\n",
      "Validation Accuracy: 39.03 %\n",
      "Validation Loss:  76.48707926273346\n",
      "Epoch [29/500], Loss: 75.3771, Accuracy: 40.05 %\n",
      "Validation Accuracy: 39.64 %\n",
      "Validation Loss:  75.9159586429596\n",
      "Epoch [30/500], Loss: 74.9654, Accuracy: 40.48 %\n",
      "Validation Accuracy: 40.00 %\n",
      "Validation Loss:  75.86608350276947\n",
      "Epoch [31/500], Loss: 74.6665, Accuracy: 41.04 %\n",
      "Validation Accuracy: 39.54 %\n",
      "Validation Loss:  75.8136066198349\n",
      "Epoch [32/500], Loss: 74.3453, Accuracy: 41.07 %\n",
      "Validation Accuracy: 40.03 %\n",
      "Validation Loss:  75.58357036113739\n",
      "Epoch [33/500], Loss: 74.2843, Accuracy: 40.99 %\n",
      "Validation Accuracy: 40.21 %\n",
      "Validation Loss:  75.45818102359772\n",
      "Epoch [34/500], Loss: 74.0193, Accuracy: 41.43 %\n",
      "Validation Accuracy: 39.80 %\n",
      "Validation Loss:  75.57064604759216\n",
      "Epoch [35/500], Loss: 73.9492, Accuracy: 41.36 %\n",
      "Validation Accuracy: 39.32 %\n",
      "Validation Loss:  76.17960119247437\n",
      "Epoch [36/500], Loss: 73.8971, Accuracy: 41.43 %\n",
      "Validation Accuracy: 39.21 %\n",
      "Validation Loss:  75.88621866703033\n",
      "Epoch [37/500], Loss: 73.8373, Accuracy: 41.05 %\n",
      "Validation Accuracy: 40.67 %\n",
      "Validation Loss:  74.81377637386322\n",
      "Epoch [38/500], Loss: 73.1348, Accuracy: 41.94 %\n",
      "Validation Accuracy: 40.16 %\n",
      "Validation Loss:  75.0879020690918\n",
      "Epoch [39/500], Loss: 73.3386, Accuracy: 41.85 %\n",
      "Validation Accuracy: 40.94 %\n",
      "Validation Loss:  74.2555603981018\n",
      "Epoch [40/500], Loss: 72.9988, Accuracy: 42.14 %\n",
      "Validation Accuracy: 40.03 %\n",
      "Validation Loss:  75.07981646060944\n",
      "Epoch [41/500], Loss: 72.9866, Accuracy: 42.24 %\n",
      "Validation Accuracy: 40.35 %\n",
      "Validation Loss:  74.52412462234497\n",
      "Epoch [42/500], Loss: 72.7034, Accuracy: 42.28 %\n",
      "Validation Accuracy: 40.41 %\n",
      "Validation Loss:  74.67395460605621\n",
      "Epoch [43/500], Loss: 72.3956, Accuracy: 42.53 %\n",
      "Validation Accuracy: 40.61 %\n",
      "Validation Loss:  74.47865867614746\n",
      "Epoch [44/500], Loss: 72.4201, Accuracy: 42.55 %\n",
      "Validation Accuracy: 41.47 %\n",
      "Validation Loss:  73.70316088199615\n",
      "Epoch [45/500], Loss: 72.0033, Accuracy: 42.98 %\n",
      "Validation Accuracy: 41.70 %\n",
      "Validation Loss:  73.89774441719055\n",
      "Epoch [46/500], Loss: 71.7276, Accuracy: 43.23 %\n",
      "Validation Accuracy: 41.62 %\n",
      "Validation Loss:  73.56169760227203\n",
      "Epoch [47/500], Loss: 71.6988, Accuracy: 43.02 %\n",
      "Validation Accuracy: 40.72 %\n",
      "Validation Loss:  73.91529965400696\n",
      "Epoch [48/500], Loss: 71.9309, Accuracy: 42.61 %\n",
      "Validation Accuracy: 40.18 %\n",
      "Validation Loss:  75.21694779396057\n",
      "Epoch [49/500], Loss: 71.6419, Accuracy: 43.13 %\n",
      "Validation Accuracy: 42.15 %\n",
      "Validation Loss:  73.06572461128235\n",
      "Epoch [50/500], Loss: 71.4920, Accuracy: 42.76 %\n",
      "Validation Accuracy: 42.18 %\n",
      "Validation Loss:  72.71682465076447\n",
      "Epoch [51/500], Loss: 71.1048, Accuracy: 43.55 %\n",
      "Validation Accuracy: 41.62 %\n",
      "Validation Loss:  73.3196393251419\n",
      "Epoch [52/500], Loss: 70.7008, Accuracy: 43.93 %\n",
      "Validation Accuracy: 41.04 %\n",
      "Validation Loss:  73.25927174091339\n",
      "Epoch [53/500], Loss: 70.4538, Accuracy: 44.11 %\n",
      "Validation Accuracy: 40.81 %\n",
      "Validation Loss:  73.20762884616852\n",
      "Epoch [54/500], Loss: 70.5090, Accuracy: 43.91 %\n",
      "Validation Accuracy: 42.47 %\n",
      "Validation Loss:  72.40845143795013\n",
      "Epoch [55/500], Loss: 70.1163, Accuracy: 44.47 %\n",
      "Validation Accuracy: 42.06 %\n",
      "Validation Loss:  72.87552094459534\n",
      "Epoch [56/500], Loss: 70.2406, Accuracy: 44.20 %\n",
      "Validation Accuracy: 42.17 %\n",
      "Validation Loss:  72.42831444740295\n",
      "Epoch [57/500], Loss: 69.7890, Accuracy: 44.67 %\n",
      "Validation Accuracy: 42.07 %\n",
      "Validation Loss:  72.40115296840668\n",
      "Epoch [58/500], Loss: 69.6759, Accuracy: 45.00 %\n",
      "Validation Accuracy: 42.81 %\n",
      "Validation Loss:  72.0621896982193\n",
      "Epoch [59/500], Loss: 69.3596, Accuracy: 44.92 %\n",
      "Validation Accuracy: 42.29 %\n",
      "Validation Loss:  72.10608100891113\n",
      "Epoch [60/500], Loss: 69.6357, Accuracy: 44.84 %\n",
      "Validation Accuracy: 43.00 %\n",
      "Validation Loss:  71.6220874786377\n",
      "Epoch [61/500], Loss: 68.8746, Accuracy: 45.64 %\n",
      "Validation Accuracy: 42.74 %\n",
      "Validation Loss:  71.49948370456696\n",
      "Epoch [62/500], Loss: 68.6078, Accuracy: 45.46 %\n",
      "Validation Accuracy: 42.45 %\n",
      "Validation Loss:  71.91844582557678\n",
      "Epoch [63/500], Loss: 68.8152, Accuracy: 45.44 %\n",
      "Validation Accuracy: 42.33 %\n",
      "Validation Loss:  71.9138925075531\n",
      "Epoch [64/500], Loss: 68.5614, Accuracy: 45.55 %\n",
      "Validation Accuracy: 43.24 %\n",
      "Validation Loss:  71.36497235298157\n",
      "Epoch [65/500], Loss: 68.6176, Accuracy: 45.55 %\n",
      "Validation Accuracy: 42.77 %\n",
      "Validation Loss:  71.49724209308624\n",
      "Epoch [66/500], Loss: 68.0808, Accuracy: 46.08 %\n",
      "Validation Accuracy: 42.99 %\n",
      "Validation Loss:  71.48466503620148\n",
      "Epoch [67/500], Loss: 68.5306, Accuracy: 45.85 %\n",
      "Validation Accuracy: 43.64 %\n",
      "Validation Loss:  71.06499516963959\n",
      "Epoch [68/500], Loss: 68.1417, Accuracy: 45.93 %\n",
      "Validation Accuracy: 42.57 %\n",
      "Validation Loss:  71.74834656715393\n",
      "Epoch [69/500], Loss: 67.9417, Accuracy: 46.25 %\n",
      "Validation Accuracy: 43.65 %\n",
      "Validation Loss:  70.82299017906189\n",
      "Epoch [70/500], Loss: 67.8029, Accuracy: 46.09 %\n",
      "Validation Accuracy: 42.84 %\n",
      "Validation Loss:  71.30172300338745\n",
      "Epoch [71/500], Loss: 67.2749, Accuracy: 46.89 %\n",
      "Validation Accuracy: 43.85 %\n",
      "Validation Loss:  70.6240918636322\n",
      "Epoch [72/500], Loss: 67.0606, Accuracy: 46.86 %\n",
      "Validation Accuracy: 43.21 %\n",
      "Validation Loss:  71.02383935451508\n",
      "Epoch [73/500], Loss: 66.9705, Accuracy: 46.68 %\n",
      "Validation Accuracy: 43.98 %\n",
      "Validation Loss:  70.31711053848267\n",
      "Epoch [74/500], Loss: 67.2054, Accuracy: 46.49 %\n",
      "Validation Accuracy: 42.68 %\n",
      "Validation Loss:  71.03964221477509\n",
      "Epoch [75/500], Loss: 66.8436, Accuracy: 46.69 %\n",
      "Validation Accuracy: 43.02 %\n",
      "Validation Loss:  70.87410986423492\n",
      "Epoch [76/500], Loss: 66.8769, Accuracy: 46.97 %\n",
      "Validation Accuracy: 43.53 %\n",
      "Validation Loss:  70.57703268527985\n",
      "Epoch [77/500], Loss: 66.1453, Accuracy: 47.54 %\n",
      "Validation Accuracy: 43.01 %\n",
      "Validation Loss:  70.87571775913239\n",
      "Epoch [78/500], Loss: 66.8395, Accuracy: 46.86 %\n",
      "Validation Accuracy: 43.59 %\n",
      "Validation Loss:  70.46220767498016\n",
      "Epoch [79/500], Loss: 66.2056, Accuracy: 47.51 %\n",
      "Validation Accuracy: 42.58 %\n",
      "Validation Loss:  70.94859409332275\n",
      "Epoch [80/500], Loss: 66.2608, Accuracy: 47.23 %\n",
      "Validation Accuracy: 42.83 %\n",
      "Validation Loss:  71.24958407878876\n",
      "Epoch [81/500], Loss: 65.7968, Accuracy: 47.67 %\n",
      "Validation Accuracy: 41.60 %\n",
      "Validation Loss:  72.84076201915741\n",
      "Epoch [82/500], Loss: 65.8403, Accuracy: 47.94 %\n",
      "Validation Accuracy: 43.57 %\n",
      "Validation Loss:  70.73308718204498\n",
      "Epoch [83/500], Loss: 65.5622, Accuracy: 48.06 %\n",
      "Validation Accuracy: 43.47 %\n",
      "Validation Loss:  70.16558182239532\n",
      "Epoch [84/500], Loss: 65.4387, Accuracy: 48.11 %\n",
      "Validation Accuracy: 44.68 %\n",
      "Validation Loss:  69.45507252216339\n",
      "Epoch [85/500], Loss: 64.8145, Accuracy: 48.72 %\n",
      "Validation Accuracy: 44.09 %\n",
      "Validation Loss:  69.74866139888763\n",
      "Epoch [86/500], Loss: 64.8603, Accuracy: 48.51 %\n",
      "Validation Accuracy: 42.53 %\n",
      "Validation Loss:  70.96603953838348\n",
      "Epoch [87/500], Loss: 64.7575, Accuracy: 48.95 %\n",
      "Validation Accuracy: 44.73 %\n",
      "Validation Loss:  69.31068813800812\n",
      "Epoch [88/500], Loss: 64.5356, Accuracy: 49.01 %\n",
      "Validation Accuracy: 44.35 %\n",
      "Validation Loss:  69.30161368846893\n",
      "Epoch [89/500], Loss: 64.6478, Accuracy: 48.89 %\n",
      "Validation Accuracy: 43.06 %\n",
      "Validation Loss:  71.16028690338135\n",
      "Epoch [90/500], Loss: 64.2680, Accuracy: 49.07 %\n",
      "Validation Accuracy: 45.00 %\n",
      "Validation Loss:  69.1407401561737\n",
      "Epoch [91/500], Loss: 63.7521, Accuracy: 49.66 %\n",
      "Validation Accuracy: 44.78 %\n",
      "Validation Loss:  69.04646515846252\n",
      "Epoch [92/500], Loss: 64.0645, Accuracy: 49.36 %\n",
      "Validation Accuracy: 45.05 %\n",
      "Validation Loss:  69.22399234771729\n",
      "Epoch [93/500], Loss: 64.3103, Accuracy: 49.45 %\n",
      "Validation Accuracy: 43.43 %\n",
      "Validation Loss:  70.05981874465942\n",
      "Epoch [94/500], Loss: 63.5761, Accuracy: 49.53 %\n",
      "Validation Accuracy: 44.17 %\n",
      "Validation Loss:  69.24488639831543\n",
      "Epoch [95/500], Loss: 63.2641, Accuracy: 49.97 %\n",
      "Validation Accuracy: 43.73 %\n",
      "Validation Loss:  69.59498119354248\n",
      "Epoch [96/500], Loss: 63.0864, Accuracy: 50.11 %\n",
      "Validation Accuracy: 45.27 %\n",
      "Validation Loss:  68.51989209651947\n",
      "Epoch [97/500], Loss: 63.4629, Accuracy: 49.20 %\n",
      "Validation Accuracy: 44.39 %\n",
      "Validation Loss:  69.3106062412262\n",
      "Epoch [98/500], Loss: 63.0346, Accuracy: 50.25 %\n",
      "Validation Accuracy: 43.31 %\n",
      "Validation Loss:  69.85234701633453\n",
      "Epoch [99/500], Loss: 62.6005, Accuracy: 50.41 %\n",
      "Validation Accuracy: 45.57 %\n",
      "Validation Loss:  68.15496003627777\n",
      "Epoch [100/500], Loss: 62.5534, Accuracy: 50.56 %\n",
      "Validation Accuracy: 45.01 %\n",
      "Validation Loss:  68.35446524620056\n",
      "Epoch [101/500], Loss: 62.3659, Accuracy: 50.77 %\n",
      "Validation Accuracy: 44.44 %\n",
      "Validation Loss:  68.84562313556671\n",
      "Epoch [102/500], Loss: 63.5469, Accuracy: 49.56 %\n",
      "Validation Accuracy: 43.74 %\n",
      "Validation Loss:  69.70584404468536\n",
      "Epoch [103/500], Loss: 62.3219, Accuracy: 50.68 %\n",
      "Validation Accuracy: 45.09 %\n",
      "Validation Loss:  68.19316327571869\n",
      "Epoch [104/500], Loss: 61.7066, Accuracy: 51.44 %\n",
      "Validation Accuracy: 45.03 %\n",
      "Validation Loss:  68.85262191295624\n",
      "Epoch [105/500], Loss: 61.8274, Accuracy: 50.93 %\n",
      "Validation Accuracy: 44.68 %\n",
      "Validation Loss:  68.49756860733032\n",
      "Epoch [106/500], Loss: 61.7091, Accuracy: 51.32 %\n",
      "Validation Accuracy: 45.07 %\n",
      "Validation Loss:  68.20612871646881\n",
      "Epoch [107/500], Loss: 62.0600, Accuracy: 50.93 %\n",
      "Validation Accuracy: 45.70 %\n",
      "Validation Loss:  67.87554466724396\n",
      "Epoch [108/500], Loss: 61.6612, Accuracy: 51.25 %\n",
      "Validation Accuracy: 43.73 %\n",
      "Validation Loss:  69.43428575992584\n",
      "Epoch [109/500], Loss: 61.2017, Accuracy: 51.55 %\n",
      "Validation Accuracy: 44.98 %\n",
      "Validation Loss:  68.15807569026947\n",
      "Epoch [110/500], Loss: 61.0686, Accuracy: 51.53 %\n",
      "Validation Accuracy: 44.74 %\n",
      "Validation Loss:  68.52287757396698\n",
      "Epoch [111/500], Loss: 60.8367, Accuracy: 52.18 %\n",
      "Validation Accuracy: 45.89 %\n",
      "Validation Loss:  67.34554946422577\n",
      "Epoch [112/500], Loss: 60.7651, Accuracy: 52.06 %\n",
      "Validation Accuracy: 45.54 %\n",
      "Validation Loss:  67.78823590278625\n",
      "Epoch [113/500], Loss: 60.8957, Accuracy: 51.96 %\n",
      "Validation Accuracy: 44.03 %\n",
      "Validation Loss:  68.9699399471283\n",
      "Epoch [114/500], Loss: 60.5496, Accuracy: 52.25 %\n",
      "Validation Accuracy: 44.93 %\n",
      "Validation Loss:  68.09030413627625\n",
      "Epoch [115/500], Loss: 60.5864, Accuracy: 52.21 %\n",
      "Validation Accuracy: 45.19 %\n",
      "Validation Loss:  67.90526962280273\n",
      "Epoch [116/500], Loss: 59.9819, Accuracy: 52.84 %\n",
      "Validation Accuracy: 45.77 %\n",
      "Validation Loss:  67.36302280426025\n",
      "Epoch [117/500], Loss: 60.0693, Accuracy: 52.54 %\n",
      "Validation Accuracy: 45.65 %\n",
      "Validation Loss:  67.27860724925995\n",
      "Epoch [118/500], Loss: 59.2808, Accuracy: 53.32 %\n",
      "Validation Accuracy: 45.85 %\n",
      "Validation Loss:  67.55129635334015\n",
      "Epoch [119/500], Loss: 59.4596, Accuracy: 53.19 %\n",
      "Validation Accuracy: 45.84 %\n",
      "Validation Loss:  67.23563396930695\n",
      "Epoch [120/500], Loss: 59.4369, Accuracy: 52.82 %\n",
      "Validation Accuracy: 46.08 %\n",
      "Validation Loss:  67.29481601715088\n",
      "Epoch [121/500], Loss: 59.8877, Accuracy: 52.90 %\n",
      "Validation Accuracy: 45.25 %\n",
      "Validation Loss:  67.99997806549072\n",
      "Epoch [122/500], Loss: 60.1300, Accuracy: 52.62 %\n",
      "Validation Accuracy: 45.03 %\n",
      "Validation Loss:  68.32980465888977\n",
      "Epoch [123/500], Loss: 58.9044, Accuracy: 53.68 %\n",
      "Validation Accuracy: 46.16 %\n",
      "Validation Loss:  66.7232894897461\n",
      "Epoch [124/500], Loss: 58.9630, Accuracy: 53.62 %\n",
      "Validation Accuracy: 46.08 %\n",
      "Validation Loss:  67.60168790817261\n",
      "Epoch [125/500], Loss: 58.6081, Accuracy: 53.80 %\n",
      "Validation Accuracy: 45.98 %\n",
      "Validation Loss:  67.069455742836\n",
      "Epoch [126/500], Loss: 58.4837, Accuracy: 53.70 %\n",
      "Validation Accuracy: 45.82 %\n",
      "Validation Loss:  67.10803270339966\n",
      "Epoch [127/500], Loss: 58.2146, Accuracy: 54.21 %\n",
      "Validation Accuracy: 46.57 %\n",
      "Validation Loss:  66.56625366210938\n",
      "Epoch [128/500], Loss: 58.5896, Accuracy: 53.70 %\n",
      "Validation Accuracy: 46.52 %\n",
      "Validation Loss:  66.6925345659256\n",
      "Epoch [129/500], Loss: 57.9682, Accuracy: 54.10 %\n",
      "Validation Accuracy: 46.63 %\n",
      "Validation Loss:  66.4912201166153\n",
      "Epoch [130/500], Loss: 57.4581, Accuracy: 54.95 %\n",
      "Validation Accuracy: 45.31 %\n",
      "Validation Loss:  68.38242852687836\n",
      "Epoch [131/500], Loss: 57.8189, Accuracy: 54.47 %\n",
      "Validation Accuracy: 45.07 %\n",
      "Validation Loss:  68.07589495182037\n",
      "Epoch [132/500], Loss: 58.0788, Accuracy: 54.35 %\n",
      "Validation Accuracy: 46.07 %\n",
      "Validation Loss:  67.13246762752533\n",
      "Epoch [133/500], Loss: 57.9894, Accuracy: 54.18 %\n",
      "Validation Accuracy: 44.03 %\n",
      "Validation Loss:  68.72303116321564\n",
      "Epoch [134/500], Loss: 57.9162, Accuracy: 54.16 %\n",
      "Validation Accuracy: 46.13 %\n",
      "Validation Loss:  67.2408002614975\n",
      "Epoch [135/500], Loss: 57.2211, Accuracy: 55.10 %\n",
      "Validation Accuracy: 44.70 %\n",
      "Validation Loss:  68.44818985462189\n",
      "Epoch [136/500], Loss: 57.5020, Accuracy: 54.70 %\n",
      "Validation Accuracy: 45.25 %\n",
      "Validation Loss:  68.23065948486328\n",
      "Epoch [137/500], Loss: 56.9945, Accuracy: 54.98 %\n",
      "Validation Accuracy: 46.29 %\n",
      "Validation Loss:  66.49333727359772\n",
      "Epoch [138/500], Loss: 57.3877, Accuracy: 54.37 %\n",
      "Validation Accuracy: 46.65 %\n",
      "Validation Loss:  66.40573632717133\n",
      "Epoch [139/500], Loss: 56.2037, Accuracy: 56.10 %\n",
      "Validation Accuracy: 46.75 %\n",
      "Validation Loss:  66.42545771598816\n",
      "Epoch [140/500], Loss: 56.2226, Accuracy: 55.89 %\n",
      "Validation Accuracy: 46.16 %\n",
      "Validation Loss:  66.84382510185242\n",
      "Epoch [141/500], Loss: 56.4568, Accuracy: 55.17 %\n",
      "Validation Accuracy: 45.53 %\n",
      "Validation Loss:  67.0320292711258\n",
      "Epoch [142/500], Loss: 55.9319, Accuracy: 56.06 %\n",
      "Validation Accuracy: 46.24 %\n",
      "Validation Loss:  66.61286473274231\n",
      "Epoch [143/500], Loss: 55.7426, Accuracy: 56.53 %\n",
      "Validation Accuracy: 46.43 %\n",
      "Validation Loss:  66.68374478816986\n",
      "Epoch [144/500], Loss: 56.1021, Accuracy: 55.75 %\n",
      "Validation Accuracy: 45.63 %\n",
      "Validation Loss:  67.86547493934631\n",
      "Epoch [145/500], Loss: 56.4810, Accuracy: 55.57 %\n",
      "Validation Accuracy: 45.79 %\n",
      "Validation Loss:  67.39443898200989\n",
      "Epoch [146/500], Loss: 55.8187, Accuracy: 56.22 %\n",
      "Validation Accuracy: 46.55 %\n",
      "Validation Loss:  65.99822115898132\n",
      "Epoch [147/500], Loss: 55.6014, Accuracy: 56.27 %\n",
      "Validation Accuracy: 46.10 %\n",
      "Validation Loss:  67.04033374786377\n",
      "Epoch [148/500], Loss: 55.9604, Accuracy: 56.07 %\n",
      "Validation Accuracy: 46.10 %\n",
      "Validation Loss:  66.68424797058105\n",
      "Epoch [149/500], Loss: 56.5883, Accuracy: 55.04 %\n",
      "Validation Accuracy: 45.21 %\n",
      "Validation Loss:  67.58100879192352\n",
      "Epoch [150/500], Loss: 55.7760, Accuracy: 56.21 %\n",
      "Validation Accuracy: 46.36 %\n",
      "Validation Loss:  66.09567368030548\n",
      "Epoch [151/500], Loss: 54.9079, Accuracy: 56.95 %\n",
      "Validation Accuracy: 46.32 %\n",
      "Validation Loss:  66.98645782470703\n",
      "Epoch [152/500], Loss: 54.9122, Accuracy: 56.90 %\n",
      "Validation Accuracy: 45.16 %\n",
      "Validation Loss:  67.74647533893585\n",
      "Epoch [153/500], Loss: 54.8741, Accuracy: 57.00 %\n",
      "Validation Accuracy: 46.36 %\n",
      "Validation Loss:  66.41970419883728\n",
      "Epoch [154/500], Loss: 54.2950, Accuracy: 57.24 %\n",
      "Validation Accuracy: 46.58 %\n",
      "Validation Loss:  67.4084061384201\n",
      "Epoch [155/500], Loss: 54.3521, Accuracy: 57.29 %\n",
      "Validation Accuracy: 46.78 %\n",
      "Validation Loss:  66.25144672393799\n",
      "Epoch [156/500], Loss: 54.6203, Accuracy: 56.90 %\n",
      "Validation Accuracy: 46.06 %\n",
      "Validation Loss:  66.78150165081024\n",
      "Epoch [157/500], Loss: 54.2842, Accuracy: 57.39 %\n",
      "Validation Accuracy: 47.10 %\n",
      "Validation Loss:  66.10273218154907\n",
      "Epoch [158/500], Loss: 54.1550, Accuracy: 57.48 %\n",
      "Validation Accuracy: 46.12 %\n",
      "Validation Loss:  67.77602076530457\n",
      "Epoch [159/500], Loss: 54.0857, Accuracy: 57.82 %\n",
      "Validation Accuracy: 46.24 %\n",
      "Validation Loss:  66.90393018722534\n",
      "Epoch [160/500], Loss: 54.5274, Accuracy: 57.20 %\n",
      "Validation Accuracy: 46.22 %\n",
      "Validation Loss:  67.11760210990906\n",
      "Epoch [161/500], Loss: 53.3319, Accuracy: 58.05 %\n",
      "Validation Accuracy: 47.42 %\n",
      "Validation Loss:  65.09832465648651\n",
      "Epoch [162/500], Loss: 53.7324, Accuracy: 57.67 %\n",
      "Validation Accuracy: 45.63 %\n",
      "Validation Loss:  67.98038685321808\n",
      "Epoch [163/500], Loss: 53.1210, Accuracy: 58.51 %\n",
      "Validation Accuracy: 45.21 %\n",
      "Validation Loss:  67.83945107460022\n",
      "Epoch [164/500], Loss: 53.8612, Accuracy: 57.93 %\n",
      "Validation Accuracy: 46.41 %\n",
      "Validation Loss:  67.06163120269775\n",
      "Epoch [165/500], Loss: 53.7346, Accuracy: 57.95 %\n",
      "Validation Accuracy: 47.11 %\n",
      "Validation Loss:  65.80631303787231\n",
      "Epoch [166/500], Loss: 52.8592, Accuracy: 58.63 %\n",
      "Validation Accuracy: 46.95 %\n",
      "Validation Loss:  66.1650595664978\n",
      "Epoch [167/500], Loss: 52.3111, Accuracy: 59.27 %\n",
      "Validation Accuracy: 46.93 %\n",
      "Validation Loss:  66.05648446083069\n",
      "Epoch [168/500], Loss: 52.0000, Accuracy: 59.47 %\n",
      "Validation Accuracy: 46.56 %\n",
      "Validation Loss:  66.48714816570282\n",
      "Epoch [169/500], Loss: 52.4434, Accuracy: 58.86 %\n",
      "Validation Accuracy: 45.56 %\n",
      "Validation Loss:  67.47183001041412\n",
      "Epoch [170/500], Loss: 52.7292, Accuracy: 58.60 %\n",
      "Validation Accuracy: 46.38 %\n",
      "Validation Loss:  67.2598375082016\n",
      "Epoch [171/500], Loss: 52.2238, Accuracy: 59.03 %\n",
      "Validation Accuracy: 47.70 %\n",
      "Validation Loss:  65.4214586019516\n",
      "Epoch [172/500], Loss: 51.2519, Accuracy: 60.25 %\n",
      "Validation Accuracy: 47.69 %\n",
      "Validation Loss:  65.49491107463837\n",
      "Epoch [173/500], Loss: 51.9190, Accuracy: 59.39 %\n",
      "Validation Accuracy: 47.32 %\n",
      "Validation Loss:  65.83757650852203\n",
      "Epoch [174/500], Loss: 51.6758, Accuracy: 59.63 %\n",
      "Validation Accuracy: 46.44 %\n",
      "Validation Loss:  66.3286406993866\n",
      "Epoch [175/500], Loss: 51.5811, Accuracy: 59.55 %\n",
      "Validation Accuracy: 47.07 %\n",
      "Validation Loss:  66.57703220844269\n",
      "Epoch [176/500], Loss: 52.0049, Accuracy: 59.42 %\n",
      "Validation Accuracy: 46.99 %\n",
      "Validation Loss:  66.49642074108124\n",
      "Epoch [177/500], Loss: 52.0015, Accuracy: 59.25 %\n",
      "Validation Accuracy: 46.90 %\n",
      "Validation Loss:  66.40115582942963\n",
      "Epoch [178/500], Loss: 51.5920, Accuracy: 59.81 %\n",
      "Validation Accuracy: 45.13 %\n",
      "Validation Loss:  68.9292243719101\n",
      "Epoch [179/500], Loss: 51.2246, Accuracy: 59.90 %\n",
      "Validation Accuracy: 46.61 %\n",
      "Validation Loss:  67.4039740562439\n",
      "Epoch [180/500], Loss: 51.1277, Accuracy: 59.77 %\n",
      "Validation Accuracy: 44.96 %\n",
      "Validation Loss:  67.98878931999207\n",
      "Epoch [181/500], Loss: 50.8888, Accuracy: 60.10 %\n",
      "Validation Accuracy: 45.90 %\n",
      "Validation Loss:  67.12539839744568\n",
      "Testing Accuracy: 46.84 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.843217540615505"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the base model with tuned hyper-parameters\n",
    "input_size = 2048\n",
    "hidden_size = 500\n",
    "num_classes = 11\n",
    "num_epochs = 500\n",
    "batch_size = 1024\n",
    "learning_rate = 0.01\n",
    "patience = 20\n",
    "dropout = 0.2\n",
    "neural_net = Net(input_size, hidden_size, num_classes, dropout).to(device)\n",
    "optimizer = torch.optim.SGD(neural_net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train(neural_net, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 97.1663, Accuracy: 20.14 %\n",
      "Testing Accuracy: 23.03 %\n",
      "Test Loss:  94.49194550514221\n",
      "Epoch [2/500], Loss: 93.7913, Accuracy: 22.86 %\n",
      "Testing Accuracy: 25.14 %\n",
      "Test Loss:  91.8515875339508\n",
      "Epoch [3/500], Loss: 90.8308, Accuracy: 25.89 %\n",
      "Testing Accuracy: 27.49 %\n",
      "Test Loss:  88.78058230876923\n",
      "Epoch [4/500], Loss: 88.4299, Accuracy: 28.25 %\n",
      "Testing Accuracy: 30.74 %\n",
      "Test Loss:  86.87129259109497\n",
      "Epoch [5/500], Loss: 86.7042, Accuracy: 30.38 %\n",
      "Testing Accuracy: 31.44 %\n",
      "Test Loss:  85.44707643985748\n",
      "Epoch [6/500], Loss: 85.5796, Accuracy: 31.08 %\n",
      "Testing Accuracy: 32.33 %\n",
      "Test Loss:  84.15449595451355\n",
      "Epoch [7/500], Loss: 84.5567, Accuracy: 32.15 %\n",
      "Testing Accuracy: 34.24 %\n",
      "Test Loss:  83.23789036273956\n",
      "Epoch [8/500], Loss: 83.6804, Accuracy: 32.72 %\n",
      "Testing Accuracy: 34.10 %\n",
      "Test Loss:  82.47943782806396\n",
      "Epoch [9/500], Loss: 82.6946, Accuracy: 33.90 %\n",
      "Testing Accuracy: 33.41 %\n",
      "Test Loss:  81.90673577785492\n",
      "Epoch [10/500], Loss: 82.1035, Accuracy: 34.55 %\n",
      "Testing Accuracy: 35.54 %\n",
      "Test Loss:  81.21183586120605\n",
      "Epoch [11/500], Loss: 81.5017, Accuracy: 35.44 %\n",
      "Testing Accuracy: 35.91 %\n",
      "Test Loss:  80.55127394199371\n",
      "Epoch [12/500], Loss: 81.0179, Accuracy: 35.65 %\n",
      "Testing Accuracy: 37.02 %\n",
      "Test Loss:  80.01392841339111\n",
      "Epoch [13/500], Loss: 80.4826, Accuracy: 35.90 %\n",
      "Testing Accuracy: 36.15 %\n",
      "Test Loss:  80.17462921142578\n",
      "Epoch [14/500], Loss: 80.1457, Accuracy: 36.53 %\n",
      "Testing Accuracy: 37.70 %\n",
      "Test Loss:  79.83324444293976\n",
      "Epoch [15/500], Loss: 79.4572, Accuracy: 37.06 %\n",
      "Testing Accuracy: 37.82 %\n",
      "Test Loss:  78.99716055393219\n",
      "Epoch [16/500], Loss: 79.2960, Accuracy: 37.04 %\n",
      "Testing Accuracy: 37.89 %\n",
      "Test Loss:  78.36818075180054\n",
      "Epoch [17/500], Loss: 78.8973, Accuracy: 37.48 %\n",
      "Testing Accuracy: 37.84 %\n",
      "Test Loss:  78.25269269943237\n",
      "Epoch [18/500], Loss: 78.5930, Accuracy: 37.64 %\n",
      "Testing Accuracy: 38.38 %\n",
      "Test Loss:  78.02130675315857\n",
      "Epoch [19/500], Loss: 77.9684, Accuracy: 38.52 %\n",
      "Testing Accuracy: 39.09 %\n",
      "Test Loss:  77.5186836719513\n",
      "Epoch [20/500], Loss: 77.7627, Accuracy: 38.63 %\n",
      "Testing Accuracy: 39.08 %\n",
      "Test Loss:  77.40339875221252\n",
      "Epoch [21/500], Loss: 77.4763, Accuracy: 38.79 %\n",
      "Testing Accuracy: 38.94 %\n",
      "Test Loss:  77.32436764240265\n",
      "Epoch [22/500], Loss: 77.1526, Accuracy: 39.07 %\n",
      "Testing Accuracy: 38.67 %\n",
      "Test Loss:  77.10057020187378\n",
      "Epoch [23/500], Loss: 77.0407, Accuracy: 38.78 %\n",
      "Testing Accuracy: 38.17 %\n",
      "Test Loss:  77.66047489643097\n",
      "Epoch [24/500], Loss: 76.8839, Accuracy: 39.11 %\n",
      "Testing Accuracy: 38.97 %\n",
      "Test Loss:  76.89429903030396\n",
      "Epoch [25/500], Loss: 76.4758, Accuracy: 39.71 %\n",
      "Testing Accuracy: 39.80 %\n",
      "Test Loss:  76.33961749076843\n",
      "Epoch [26/500], Loss: 76.2054, Accuracy: 39.73 %\n",
      "Testing Accuracy: 39.45 %\n",
      "Test Loss:  76.47052931785583\n",
      "Epoch [27/500], Loss: 75.9774, Accuracy: 39.88 %\n",
      "Testing Accuracy: 40.22 %\n",
      "Test Loss:  75.67441499233246\n",
      "Epoch [28/500], Loss: 75.6290, Accuracy: 40.28 %\n",
      "Testing Accuracy: 40.30 %\n",
      "Test Loss:  75.58561491966248\n",
      "Epoch [29/500], Loss: 75.4131, Accuracy: 40.25 %\n",
      "Testing Accuracy: 39.73 %\n",
      "Test Loss:  75.91720354557037\n",
      "Epoch [30/500], Loss: 74.9566, Accuracy: 40.60 %\n",
      "Testing Accuracy: 40.40 %\n",
      "Test Loss:  75.45808231830597\n",
      "Epoch [31/500], Loss: 74.9832, Accuracy: 40.42 %\n",
      "Testing Accuracy: 40.55 %\n",
      "Test Loss:  75.04733920097351\n",
      "Epoch [32/500], Loss: 75.0923, Accuracy: 40.25 %\n",
      "Testing Accuracy: 40.42 %\n",
      "Test Loss:  75.33853161334991\n",
      "Epoch [33/500], Loss: 74.6375, Accuracy: 40.64 %\n",
      "Testing Accuracy: 40.42 %\n",
      "Test Loss:  74.99993455410004\n",
      "Epoch [34/500], Loss: 74.4780, Accuracy: 40.92 %\n",
      "Testing Accuracy: 40.50 %\n",
      "Test Loss:  75.03125059604645\n",
      "Epoch [35/500], Loss: 73.9454, Accuracy: 41.32 %\n",
      "Testing Accuracy: 41.02 %\n",
      "Test Loss:  74.17969501018524\n",
      "Epoch [36/500], Loss: 73.6569, Accuracy: 41.61 %\n",
      "Testing Accuracy: 40.72 %\n",
      "Test Loss:  74.67596018314362\n",
      "Epoch [37/500], Loss: 73.7374, Accuracy: 41.52 %\n",
      "Testing Accuracy: 41.19 %\n",
      "Test Loss:  74.47593748569489\n",
      "Epoch [38/500], Loss: 73.3415, Accuracy: 42.01 %\n",
      "Testing Accuracy: 41.61 %\n",
      "Test Loss:  73.76365220546722\n",
      "Epoch [39/500], Loss: 73.3320, Accuracy: 41.75 %\n",
      "Testing Accuracy: 40.05 %\n",
      "Test Loss:  74.6880476474762\n",
      "Epoch [40/500], Loss: 73.1561, Accuracy: 41.78 %\n",
      "Testing Accuracy: 40.43 %\n",
      "Test Loss:  75.00172448158264\n",
      "Epoch [41/500], Loss: 73.0347, Accuracy: 41.99 %\n",
      "Testing Accuracy: 41.75 %\n",
      "Test Loss:  73.62049233913422\n",
      "Epoch [42/500], Loss: 72.5977, Accuracy: 42.32 %\n",
      "Testing Accuracy: 40.95 %\n",
      "Test Loss:  74.45935535430908\n",
      "Epoch [43/500], Loss: 72.6914, Accuracy: 42.10 %\n",
      "Testing Accuracy: 41.88 %\n",
      "Test Loss:  73.56968772411346\n",
      "Epoch [44/500], Loss: 72.2556, Accuracy: 42.76 %\n",
      "Testing Accuracy: 42.07 %\n",
      "Test Loss:  73.13467168807983\n",
      "Epoch [45/500], Loss: 72.1588, Accuracy: 42.73 %\n",
      "Testing Accuracy: 41.95 %\n",
      "Test Loss:  73.0194947719574\n",
      "Epoch [46/500], Loss: 71.6938, Accuracy: 43.20 %\n",
      "Testing Accuracy: 42.35 %\n",
      "Test Loss:  72.86374247074127\n",
      "Epoch [47/500], Loss: 71.4083, Accuracy: 43.35 %\n",
      "Testing Accuracy: 42.58 %\n",
      "Test Loss:  72.39109575748444\n",
      "Epoch [48/500], Loss: 71.5415, Accuracy: 43.25 %\n",
      "Testing Accuracy: 41.90 %\n",
      "Test Loss:  72.89020192623138\n",
      "Epoch [49/500], Loss: 71.4038, Accuracy: 43.22 %\n",
      "Testing Accuracy: 42.02 %\n",
      "Test Loss:  72.90851140022278\n",
      "Epoch [50/500], Loss: 71.3111, Accuracy: 43.14 %\n",
      "Testing Accuracy: 42.24 %\n",
      "Test Loss:  72.76815676689148\n",
      "Epoch [51/500], Loss: 71.1585, Accuracy: 43.43 %\n",
      "Testing Accuracy: 41.45 %\n",
      "Test Loss:  73.4160908460617\n",
      "Epoch [52/500], Loss: 71.0280, Accuracy: 43.55 %\n",
      "Testing Accuracy: 41.84 %\n",
      "Test Loss:  72.97682583332062\n",
      "Epoch [53/500], Loss: 70.8430, Accuracy: 43.62 %\n",
      "Testing Accuracy: 42.93 %\n",
      "Test Loss:  72.09987831115723\n",
      "Epoch [54/500], Loss: 70.3481, Accuracy: 44.19 %\n",
      "Testing Accuracy: 42.36 %\n",
      "Test Loss:  72.35408091545105\n",
      "Epoch [55/500], Loss: 69.9675, Accuracy: 44.54 %\n",
      "Testing Accuracy: 41.92 %\n",
      "Test Loss:  72.23602080345154\n",
      "Epoch [56/500], Loss: 69.8275, Accuracy: 44.51 %\n",
      "Testing Accuracy: 42.27 %\n",
      "Test Loss:  72.18620932102203\n",
      "Epoch [57/500], Loss: 69.9285, Accuracy: 44.47 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  71.6862895488739\n",
      "Epoch [58/500], Loss: 69.7426, Accuracy: 44.52 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  71.53971469402313\n",
      "Epoch [59/500], Loss: 69.3519, Accuracy: 44.95 %\n",
      "Testing Accuracy: 43.08 %\n",
      "Test Loss:  71.71421492099762\n",
      "Epoch [60/500], Loss: 69.3058, Accuracy: 44.80 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  71.10907673835754\n",
      "Epoch [61/500], Loss: 69.4444, Accuracy: 44.98 %\n",
      "Testing Accuracy: 43.25 %\n",
      "Test Loss:  71.52759110927582\n",
      "Epoch [62/500], Loss: 69.3130, Accuracy: 44.72 %\n",
      "Testing Accuracy: 42.44 %\n",
      "Test Loss:  71.55464458465576\n",
      "Epoch [63/500], Loss: 68.8244, Accuracy: 45.44 %\n",
      "Testing Accuracy: 43.06 %\n",
      "Test Loss:  71.0112715959549\n",
      "Epoch [64/500], Loss: 68.5764, Accuracy: 45.11 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  70.70976305007935\n",
      "Epoch [65/500], Loss: 68.4491, Accuracy: 45.49 %\n",
      "Testing Accuracy: 42.03 %\n",
      "Test Loss:  72.31688404083252\n",
      "Epoch [66/500], Loss: 68.2870, Accuracy: 45.79 %\n",
      "Testing Accuracy: 41.48 %\n",
      "Test Loss:  72.61908280849457\n",
      "Epoch [67/500], Loss: 68.1955, Accuracy: 45.96 %\n",
      "Testing Accuracy: 43.94 %\n",
      "Test Loss:  70.65911293029785\n",
      "Epoch [68/500], Loss: 68.2159, Accuracy: 45.62 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  70.58228802680969\n",
      "Epoch [69/500], Loss: 68.0134, Accuracy: 45.87 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  70.329274892807\n",
      "Epoch [70/500], Loss: 67.8836, Accuracy: 46.26 %\n",
      "Testing Accuracy: 43.19 %\n",
      "Test Loss:  71.07405960559845\n",
      "Epoch [71/500], Loss: 67.5898, Accuracy: 46.21 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  70.46486556529999\n",
      "Epoch [72/500], Loss: 66.9979, Accuracy: 46.78 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  70.36532270908356\n",
      "Epoch [73/500], Loss: 67.2567, Accuracy: 46.76 %\n",
      "Testing Accuracy: 44.43 %\n",
      "Test Loss:  69.86474168300629\n",
      "Epoch [74/500], Loss: 66.8718, Accuracy: 46.91 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  70.8610850572586\n",
      "Epoch [75/500], Loss: 66.8866, Accuracy: 46.84 %\n",
      "Testing Accuracy: 43.35 %\n",
      "Test Loss:  71.26183211803436\n",
      "Epoch [76/500], Loss: 66.6722, Accuracy: 46.87 %\n",
      "Testing Accuracy: 41.65 %\n",
      "Test Loss:  72.05529034137726\n",
      "Epoch [77/500], Loss: 66.5766, Accuracy: 47.16 %\n",
      "Testing Accuracy: 44.08 %\n",
      "Test Loss:  70.20144474506378\n",
      "Epoch [78/500], Loss: 66.4245, Accuracy: 47.26 %\n",
      "Testing Accuracy: 44.40 %\n",
      "Test Loss:  69.95940148830414\n",
      "Epoch [79/500], Loss: 65.9809, Accuracy: 47.37 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  69.42140221595764\n",
      "Epoch [80/500], Loss: 65.7287, Accuracy: 47.91 %\n",
      "Testing Accuracy: 42.21 %\n",
      "Test Loss:  71.64522993564606\n",
      "Epoch [81/500], Loss: 65.9092, Accuracy: 47.82 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  69.51422131061554\n",
      "Epoch [82/500], Loss: 65.4252, Accuracy: 48.13 %\n",
      "Testing Accuracy: 44.31 %\n",
      "Test Loss:  69.44491589069366\n",
      "Epoch [83/500], Loss: 65.5184, Accuracy: 48.11 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  69.91235840320587\n",
      "Epoch [84/500], Loss: 65.4114, Accuracy: 47.94 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  70.82504653930664\n",
      "Epoch [85/500], Loss: 65.1242, Accuracy: 48.55 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  68.62065052986145\n",
      "Epoch [86/500], Loss: 65.2549, Accuracy: 48.25 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  70.01758825778961\n",
      "Epoch [87/500], Loss: 65.5373, Accuracy: 47.66 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  68.90141129493713\n",
      "Epoch [88/500], Loss: 64.6637, Accuracy: 48.80 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  68.9055061340332\n",
      "Epoch [89/500], Loss: 64.5242, Accuracy: 48.53 %\n",
      "Testing Accuracy: 44.57 %\n",
      "Test Loss:  69.45528781414032\n",
      "Epoch [90/500], Loss: 64.7668, Accuracy: 48.66 %\n",
      "Testing Accuracy: 45.43 %\n",
      "Test Loss:  68.94419646263123\n",
      "Epoch [91/500], Loss: 64.2169, Accuracy: 48.95 %\n",
      "Testing Accuracy: 44.39 %\n",
      "Test Loss:  69.31140911579132\n",
      "Epoch [92/500], Loss: 63.9725, Accuracy: 49.21 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  69.179514169693\n",
      "Epoch [93/500], Loss: 63.6253, Accuracy: 49.70 %\n",
      "Testing Accuracy: 44.84 %\n",
      "Test Loss:  69.35118842124939\n",
      "Epoch [94/500], Loss: 63.7980, Accuracy: 49.52 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  70.00397872924805\n",
      "Epoch [95/500], Loss: 63.3455, Accuracy: 49.82 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  68.31970167160034\n",
      "Epoch [96/500], Loss: 63.3796, Accuracy: 49.95 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  69.94540679454803\n",
      "Epoch [97/500], Loss: 63.5201, Accuracy: 49.70 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  68.53580510616302\n",
      "Epoch [98/500], Loss: 63.1415, Accuracy: 49.92 %\n",
      "Testing Accuracy: 43.35 %\n",
      "Test Loss:  69.5197137594223\n",
      "Epoch [99/500], Loss: 62.9756, Accuracy: 50.12 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  68.10238873958588\n",
      "Epoch [100/500], Loss: 63.0893, Accuracy: 49.67 %\n",
      "Testing Accuracy: 45.38 %\n",
      "Test Loss:  68.88248348236084\n",
      "Epoch [101/500], Loss: 62.6134, Accuracy: 50.47 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  68.59389996528625\n",
      "Epoch [102/500], Loss: 62.0282, Accuracy: 51.00 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  67.92647874355316\n",
      "Epoch [103/500], Loss: 62.0464, Accuracy: 51.24 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  68.24056649208069\n",
      "Epoch [104/500], Loss: 61.8729, Accuracy: 50.81 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  70.13848721981049\n",
      "Epoch [105/500], Loss: 62.4928, Accuracy: 50.30 %\n",
      "Testing Accuracy: 45.00 %\n",
      "Test Loss:  68.5576536655426\n",
      "Epoch [106/500], Loss: 62.5128, Accuracy: 50.33 %\n",
      "Testing Accuracy: 46.34 %\n",
      "Test Loss:  67.66136062145233\n",
      "Epoch [107/500], Loss: 61.9334, Accuracy: 50.93 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  69.92467761039734\n",
      "Epoch [108/500], Loss: 61.8005, Accuracy: 51.23 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  68.35731410980225\n",
      "Epoch [109/500], Loss: 62.0561, Accuracy: 50.57 %\n",
      "Testing Accuracy: 45.78 %\n",
      "Test Loss:  67.78594422340393\n",
      "Epoch [110/500], Loss: 60.9527, Accuracy: 51.92 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  68.62281060218811\n",
      "Epoch [111/500], Loss: 61.5944, Accuracy: 50.73 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  69.6212317943573\n",
      "Epoch [112/500], Loss: 61.1928, Accuracy: 51.32 %\n",
      "Testing Accuracy: 46.02 %\n",
      "Test Loss:  68.26552069187164\n",
      "Epoch [113/500], Loss: 61.2590, Accuracy: 51.48 %\n",
      "Testing Accuracy: 46.12 %\n",
      "Test Loss:  67.62746071815491\n",
      "Epoch [114/500], Loss: 60.9379, Accuracy: 51.99 %\n",
      "Testing Accuracy: 46.57 %\n",
      "Test Loss:  67.11576962471008\n",
      "Epoch [115/500], Loss: 60.2924, Accuracy: 52.31 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  68.77168321609497\n",
      "Epoch [116/500], Loss: 60.8668, Accuracy: 51.88 %\n",
      "Testing Accuracy: 41.85 %\n",
      "Test Loss:  70.83105611801147\n",
      "Epoch [117/500], Loss: 60.4685, Accuracy: 52.17 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  68.18188798427582\n",
      "Epoch [118/500], Loss: 60.6644, Accuracy: 51.50 %\n",
      "Testing Accuracy: 46.80 %\n",
      "Test Loss:  66.85093867778778\n",
      "Epoch [119/500], Loss: 60.6204, Accuracy: 52.15 %\n",
      "Testing Accuracy: 46.14 %\n",
      "Test Loss:  67.20479035377502\n",
      "Epoch [120/500], Loss: 59.4956, Accuracy: 52.75 %\n",
      "Testing Accuracy: 46.66 %\n",
      "Test Loss:  66.69752669334412\n",
      "Epoch [121/500], Loss: 59.6737, Accuracy: 52.71 %\n",
      "Testing Accuracy: 46.10 %\n",
      "Test Loss:  67.40460348129272\n",
      "Epoch [122/500], Loss: 59.5114, Accuracy: 52.86 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  68.20367217063904\n",
      "Epoch [123/500], Loss: 59.4673, Accuracy: 53.09 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  68.48448979854584\n",
      "Epoch [124/500], Loss: 59.0159, Accuracy: 53.37 %\n",
      "Testing Accuracy: 46.76 %\n",
      "Test Loss:  66.75360989570618\n",
      "Epoch [125/500], Loss: 58.3100, Accuracy: 54.27 %\n",
      "Testing Accuracy: 46.82 %\n",
      "Test Loss:  66.79128813743591\n",
      "Epoch [126/500], Loss: 58.8451, Accuracy: 53.47 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  69.02203667163849\n",
      "Epoch [127/500], Loss: 59.1437, Accuracy: 53.05 %\n",
      "Testing Accuracy: 47.07 %\n",
      "Test Loss:  66.51002562046051\n",
      "Epoch [128/500], Loss: 58.7161, Accuracy: 53.61 %\n",
      "Testing Accuracy: 46.21 %\n",
      "Test Loss:  67.35576903820038\n",
      "Epoch [129/500], Loss: 58.7904, Accuracy: 53.79 %\n",
      "Testing Accuracy: 44.93 %\n",
      "Test Loss:  67.67315876483917\n",
      "Epoch [130/500], Loss: 58.8509, Accuracy: 53.54 %\n",
      "Testing Accuracy: 45.47 %\n",
      "Test Loss:  67.74383497238159\n",
      "Epoch [131/500], Loss: 58.5247, Accuracy: 53.79 %\n",
      "Testing Accuracy: 46.85 %\n",
      "Test Loss:  66.28406095504761\n",
      "Epoch [132/500], Loss: 58.3495, Accuracy: 53.64 %\n",
      "Testing Accuracy: 41.85 %\n",
      "Test Loss:  72.06810021400452\n",
      "Epoch [133/500], Loss: 58.6164, Accuracy: 53.48 %\n",
      "Testing Accuracy: 45.99 %\n",
      "Test Loss:  67.39118242263794\n",
      "Epoch [134/500], Loss: 57.7793, Accuracy: 54.15 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  69.25917685031891\n",
      "Epoch [135/500], Loss: 58.8992, Accuracy: 52.83 %\n",
      "Testing Accuracy: 46.25 %\n",
      "Test Loss:  67.04196560382843\n",
      "Epoch [136/500], Loss: 57.8836, Accuracy: 54.29 %\n",
      "Testing Accuracy: 45.89 %\n",
      "Test Loss:  67.82096564769745\n",
      "Epoch [137/500], Loss: 57.4631, Accuracy: 54.57 %\n",
      "Testing Accuracy: 47.19 %\n",
      "Test Loss:  66.3005143404007\n",
      "Epoch [138/500], Loss: 57.8598, Accuracy: 53.93 %\n",
      "Testing Accuracy: 46.61 %\n",
      "Test Loss:  66.58802318572998\n",
      "Epoch [139/500], Loss: 57.1889, Accuracy: 54.55 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  68.45101833343506\n",
      "Epoch [140/500], Loss: 57.0067, Accuracy: 55.08 %\n",
      "Testing Accuracy: 47.14 %\n",
      "Test Loss:  66.04438841342926\n",
      "Epoch [141/500], Loss: 56.4305, Accuracy: 55.56 %\n",
      "Testing Accuracy: 46.66 %\n",
      "Test Loss:  66.59474265575409\n",
      "Epoch [142/500], Loss: 56.7466, Accuracy: 55.25 %\n",
      "Testing Accuracy: 45.88 %\n",
      "Test Loss:  67.16182744503021\n",
      "Epoch [143/500], Loss: 56.9718, Accuracy: 55.06 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  69.30074000358582\n",
      "Epoch [144/500], Loss: 57.3090, Accuracy: 54.34 %\n",
      "Testing Accuracy: 46.06 %\n",
      "Test Loss:  66.84956073760986\n",
      "Epoch [145/500], Loss: 56.1597, Accuracy: 55.86 %\n",
      "Testing Accuracy: 47.07 %\n",
      "Test Loss:  66.01250374317169\n",
      "Epoch [146/500], Loss: 55.9393, Accuracy: 56.19 %\n",
      "Testing Accuracy: 45.96 %\n",
      "Test Loss:  67.78796231746674\n",
      "Epoch [147/500], Loss: 56.9384, Accuracy: 54.81 %\n",
      "Testing Accuracy: 46.20 %\n",
      "Test Loss:  67.00378596782684\n",
      "Epoch [148/500], Loss: 55.7992, Accuracy: 56.11 %\n",
      "Testing Accuracy: 47.13 %\n",
      "Test Loss:  66.10848104953766\n",
      "Epoch [149/500], Loss: 56.1029, Accuracy: 55.70 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  67.2554669380188\n",
      "Epoch [150/500], Loss: 56.2735, Accuracy: 55.49 %\n",
      "Testing Accuracy: 46.00 %\n",
      "Test Loss:  67.18557286262512\n",
      "Epoch [151/500], Loss: 55.8102, Accuracy: 55.65 %\n",
      "Testing Accuracy: 46.37 %\n",
      "Test Loss:  66.87046086788177\n",
      "Epoch [152/500], Loss: 55.3656, Accuracy: 56.42 %\n",
      "Testing Accuracy: 46.70 %\n",
      "Test Loss:  66.85837483406067\n",
      "Epoch [153/500], Loss: 55.1419, Accuracy: 56.77 %\n",
      "Testing Accuracy: 46.99 %\n",
      "Test Loss:  66.36480045318604\n",
      "Epoch [154/500], Loss: 55.1346, Accuracy: 56.46 %\n",
      "Testing Accuracy: 46.07 %\n",
      "Test Loss:  66.84742605686188\n",
      "Epoch [155/500], Loss: 54.8542, Accuracy: 57.01 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  67.47613656520844\n",
      "Epoch [156/500], Loss: 55.1254, Accuracy: 56.59 %\n",
      "Testing Accuracy: 46.43 %\n",
      "Test Loss:  66.77013373374939\n",
      "Epoch [157/500], Loss: 55.2957, Accuracy: 56.54 %\n",
      "Testing Accuracy: 46.54 %\n",
      "Test Loss:  66.18544042110443\n",
      "Epoch [158/500], Loss: 54.5785, Accuracy: 57.10 %\n",
      "Testing Accuracy: 46.47 %\n",
      "Test Loss:  66.58587169647217\n",
      "Epoch [159/500], Loss: 54.5352, Accuracy: 57.24 %\n",
      "Testing Accuracy: 47.01 %\n",
      "Test Loss:  66.06845283508301\n",
      "Epoch [160/500], Loss: 55.3603, Accuracy: 56.31 %\n",
      "Testing Accuracy: 46.72 %\n",
      "Test Loss:  66.13908362388611\n",
      "Epoch [161/500], Loss: 54.7322, Accuracy: 56.95 %\n",
      "Testing Accuracy: 46.63 %\n",
      "Test Loss:  67.34521293640137\n",
      "Epoch [162/500], Loss: 54.6983, Accuracy: 56.59 %\n",
      "Testing Accuracy: 46.96 %\n",
      "Test Loss:  65.84212374687195\n",
      "Epoch [163/500], Loss: 53.7887, Accuracy: 57.81 %\n",
      "Testing Accuracy: 46.48 %\n",
      "Test Loss:  66.50658738613129\n",
      "Epoch [164/500], Loss: 54.0073, Accuracy: 57.50 %\n",
      "Testing Accuracy: 46.72 %\n",
      "Test Loss:  66.46043157577515\n",
      "Epoch [165/500], Loss: 54.0620, Accuracy: 57.53 %\n",
      "Testing Accuracy: 45.93 %\n",
      "Test Loss:  66.45163750648499\n",
      "Epoch [166/500], Loss: 54.1073, Accuracy: 57.07 %\n",
      "Testing Accuracy: 45.95 %\n",
      "Test Loss:  67.21727049350739\n",
      "Epoch [167/500], Loss: 53.9659, Accuracy: 57.21 %\n",
      "Testing Accuracy: 47.27 %\n",
      "Test Loss:  66.38992488384247\n",
      "Epoch [168/500], Loss: 53.7820, Accuracy: 57.55 %\n",
      "Testing Accuracy: 46.42 %\n",
      "Test Loss:  66.83507001399994\n",
      "Epoch [169/500], Loss: 53.3584, Accuracy: 57.87 %\n",
      "Testing Accuracy: 47.34 %\n",
      "Test Loss:  66.28960525989532\n",
      "Epoch [170/500], Loss: 53.6224, Accuracy: 57.67 %\n",
      "Testing Accuracy: 47.11 %\n",
      "Test Loss:  66.17849636077881\n",
      "Epoch [171/500], Loss: 53.2770, Accuracy: 58.00 %\n",
      "Testing Accuracy: 47.27 %\n",
      "Test Loss:  66.36410665512085\n",
      "Epoch [172/500], Loss: 53.3286, Accuracy: 57.70 %\n",
      "Testing Accuracy: 47.43 %\n",
      "Test Loss:  65.5224541425705\n",
      "Epoch [173/500], Loss: 52.9389, Accuracy: 58.15 %\n",
      "Testing Accuracy: 46.90 %\n",
      "Test Loss:  66.1276627779007\n",
      "Epoch [174/500], Loss: 53.0204, Accuracy: 58.36 %\n",
      "Testing Accuracy: 46.83 %\n",
      "Test Loss:  66.34079825878143\n",
      "Epoch [175/500], Loss: 52.9283, Accuracy: 58.34 %\n",
      "Testing Accuracy: 46.94 %\n",
      "Test Loss:  66.39313101768494\n",
      "Epoch [176/500], Loss: 52.2263, Accuracy: 58.95 %\n",
      "Testing Accuracy: 46.79 %\n",
      "Test Loss:  66.25554406642914\n",
      "Epoch [177/500], Loss: 52.5861, Accuracy: 58.67 %\n",
      "Testing Accuracy: 46.41 %\n",
      "Test Loss:  66.63562631607056\n",
      "Epoch [178/500], Loss: 52.4849, Accuracy: 58.61 %\n",
      "Testing Accuracy: 47.44 %\n",
      "Test Loss:  65.88956487178802\n",
      "Epoch [179/500], Loss: 51.3901, Accuracy: 59.50 %\n",
      "Testing Accuracy: 46.38 %\n",
      "Test Loss:  67.83969855308533\n",
      "Epoch [180/500], Loss: 51.9787, Accuracy: 59.11 %\n",
      "Testing Accuracy: 47.10 %\n",
      "Test Loss:  67.10011911392212\n",
      "Epoch [181/500], Loss: 51.9800, Accuracy: 59.10 %\n",
      "Testing Accuracy: 47.33 %\n",
      "Test Loss:  66.15201652050018\n",
      "Epoch [182/500], Loss: 51.4350, Accuracy: 59.44 %\n",
      "Testing Accuracy: 47.18 %\n",
      "Test Loss:  65.74470484256744\n",
      "Epoch [183/500], Loss: 51.3282, Accuracy: 59.61 %\n",
      "Testing Accuracy: 46.00 %\n",
      "Test Loss:  67.67259407043457\n",
      "Epoch [184/500], Loss: 51.9620, Accuracy: 59.00 %\n",
      "Testing Accuracy: 45.48 %\n",
      "Test Loss:  68.53022003173828\n",
      "Epoch [185/500], Loss: 51.0993, Accuracy: 59.67 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  68.60130143165588\n",
      "Epoch [186/500], Loss: 51.5391, Accuracy: 59.46 %\n",
      "Testing Accuracy: 47.23 %\n",
      "Test Loss:  65.70766031742096\n",
      "Epoch [187/500], Loss: 50.8841, Accuracy: 59.98 %\n",
      "Testing Accuracy: 47.38 %\n",
      "Test Loss:  65.96304488182068\n",
      "Epoch [188/500], Loss: 50.0595, Accuracy: 60.99 %\n",
      "Testing Accuracy: 47.13 %\n",
      "Test Loss:  66.10103952884674\n",
      "Epoch [189/500], Loss: 49.8584, Accuracy: 61.03 %\n",
      "Testing Accuracy: 47.40 %\n",
      "Test Loss:  65.863818526268\n",
      "Epoch [190/500], Loss: 50.2887, Accuracy: 60.61 %\n",
      "Testing Accuracy: 47.29 %\n",
      "Test Loss:  65.73288774490356\n",
      "Epoch [191/500], Loss: 49.9240, Accuracy: 60.86 %\n",
      "Testing Accuracy: 45.08 %\n",
      "Test Loss:  68.62120020389557\n",
      "Epoch [192/500], Loss: 50.6300, Accuracy: 60.36 %\n",
      "Testing Accuracy: 47.54 %\n",
      "Test Loss:  65.7712619304657\n",
      "Testing Accuracy: 47.54 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS       \n",
      "--------------------------------  --------  \n",
      "Net/Linear[fc1]/onnx::Gemm        1024000   \n",
      "Net/Dropout[dropout]/onnx::Relu   1000      \n",
      "Net/Linear[fc2]/onnx::Gemm        5500      \n",
      "-------------------------------   -------   \n",
      "Input size: (1, 2048)\n",
      "1,030,500 FLOPs or approx. 0.00 GFLOPs\n",
      "pruning hidden size:  500\n",
      "with hidden layer:  500\n",
      "removing:  (499, 254, 220)\n",
      "--- 73.38346552848816 seconds ---\n",
      "Epoch [1/500], Loss: 50.8277, Accuracy: 60.09 %\n",
      "Testing Accuracy: 46.84 %\n",
      "Test Loss:  66.06661319732666\n",
      "Epoch [2/500], Loss: 50.6901, Accuracy: 60.47 %\n",
      "Testing Accuracy: 47.22 %\n",
      "Test Loss:  66.04057431221008\n",
      "Epoch [3/500], Loss: 50.4530, Accuracy: 60.08 %\n",
      "Testing Accuracy: 48.09 %\n",
      "Test Loss:  65.30411159992218\n",
      "Epoch [4/500], Loss: 49.8087, Accuracy: 61.22 %\n",
      "Testing Accuracy: 46.70 %\n",
      "Test Loss:  66.81214320659637\n",
      "Epoch [5/500], Loss: 49.5255, Accuracy: 61.33 %\n",
      "Testing Accuracy: 46.99 %\n",
      "Test Loss:  66.66231858730316\n",
      "Epoch [6/500], Loss: 49.6243, Accuracy: 61.17 %\n",
      "Testing Accuracy: 47.73 %\n",
      "Test Loss:  66.06831514835358\n",
      "Epoch [7/500], Loss: 49.5887, Accuracy: 61.30 %\n",
      "Testing Accuracy: 46.65 %\n",
      "Test Loss:  67.37337756156921\n",
      "Epoch [8/500], Loss: 49.3452, Accuracy: 61.32 %\n",
      "Testing Accuracy: 45.96 %\n",
      "Test Loss:  68.5009024143219\n",
      "Epoch [9/500], Loss: 49.7659, Accuracy: 60.91 %\n",
      "Testing Accuracy: 47.73 %\n",
      "Test Loss:  65.54539132118225\n",
      "Epoch [10/500], Loss: 50.0327, Accuracy: 60.52 %\n",
      "Testing Accuracy: 46.82 %\n",
      "Test Loss:  66.72477078437805\n",
      "Epoch [11/500], Loss: 50.0654, Accuracy: 60.35 %\n",
      "Testing Accuracy: 46.17 %\n",
      "Test Loss:  67.48220479488373\n",
      "Epoch [12/500], Loss: 49.3185, Accuracy: 61.14 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  69.52353072166443\n",
      "Epoch [13/500], Loss: 50.2766, Accuracy: 60.44 %\n",
      "Testing Accuracy: 45.93 %\n",
      "Test Loss:  68.11038422584534\n",
      "Epoch [14/500], Loss: 49.8125, Accuracy: 60.78 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  69.00230169296265\n",
      "Epoch [15/500], Loss: 48.5019, Accuracy: 61.96 %\n",
      "Testing Accuracy: 47.52 %\n",
      "Test Loss:  65.87756431102753\n",
      "Epoch [16/500], Loss: 48.8324, Accuracy: 61.69 %\n",
      "Testing Accuracy: 47.50 %\n",
      "Test Loss:  65.64638435840607\n",
      "Epoch [17/500], Loss: 48.1041, Accuracy: 62.42 %\n",
      "Testing Accuracy: 47.97 %\n",
      "Test Loss:  65.52145707607269\n",
      "Epoch [18/500], Loss: 48.0803, Accuracy: 62.62 %\n",
      "Testing Accuracy: 46.60 %\n",
      "Test Loss:  67.50337707996368\n",
      "Epoch [19/500], Loss: 48.4343, Accuracy: 61.99 %\n",
      "Testing Accuracy: 47.36 %\n",
      "Test Loss:  66.1842645406723\n",
      "Epoch [20/500], Loss: 48.3353, Accuracy: 61.98 %\n",
      "Testing Accuracy: 47.57 %\n",
      "Test Loss:  66.14133167266846\n",
      "Epoch [21/500], Loss: 48.2016, Accuracy: 62.24 %\n",
      "Testing Accuracy: 46.04 %\n",
      "Test Loss:  67.45952010154724\n",
      "Epoch [22/500], Loss: 48.1792, Accuracy: 62.43 %\n",
      "Testing Accuracy: 45.48 %\n",
      "Test Loss:  68.90569567680359\n",
      "Epoch [23/500], Loss: 47.7620, Accuracy: 62.37 %\n",
      "Testing Accuracy: 47.77 %\n",
      "Test Loss:  65.53198707103729\n",
      "Testing Accuracy: 47.77 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS       \n",
      "--------------------------------  --------  \n",
      "Net/Linear[fc1]/onnx::Gemm        1017856   \n",
      "Net/Dropout[dropout]/onnx::Relu   994       \n",
      "Net/Linear[fc2]/onnx::Gemm        5467      \n",
      "-------------------------------   -------   \n",
      "Input size: (1, 2048)\n",
      "1,024,317 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989]\n",
      "pruning hidden size:  497\n",
      "with hidden layer:  497\n",
      "removing:  (479, 449, 124)\n",
      "--- 72.81164836883545 seconds ---\n",
      "Epoch [1/500], Loss: 47.1652, Accuracy: 63.43 %\n",
      "Testing Accuracy: 47.97 %\n",
      "Test Loss:  66.04170048236847\n",
      "Epoch [2/500], Loss: 47.5059, Accuracy: 62.86 %\n",
      "Testing Accuracy: 46.84 %\n",
      "Test Loss:  66.68557012081146\n",
      "Epoch [3/500], Loss: 47.5828, Accuracy: 62.63 %\n",
      "Testing Accuracy: 46.69 %\n",
      "Test Loss:  66.79269540309906\n",
      "Epoch [4/500], Loss: 47.0450, Accuracy: 63.33 %\n",
      "Testing Accuracy: 47.13 %\n",
      "Test Loss:  66.53073048591614\n",
      "Epoch [5/500], Loss: 47.2776, Accuracy: 63.19 %\n",
      "Testing Accuracy: 45.93 %\n",
      "Test Loss:  68.244105219841\n",
      "Epoch [6/500], Loss: 47.6390, Accuracy: 62.48 %\n",
      "Testing Accuracy: 47.22 %\n",
      "Test Loss:  66.1270694732666\n",
      "Epoch [7/500], Loss: 47.5841, Accuracy: 62.78 %\n",
      "Testing Accuracy: 47.48 %\n",
      "Test Loss:  65.74435865879059\n",
      "Epoch [8/500], Loss: 46.5658, Accuracy: 63.99 %\n",
      "Testing Accuracy: 47.44 %\n",
      "Test Loss:  66.1533168554306\n",
      "Epoch [9/500], Loss: 46.8230, Accuracy: 63.45 %\n",
      "Testing Accuracy: 46.43 %\n",
      "Test Loss:  67.53291070461273\n",
      "Epoch [10/500], Loss: 47.0952, Accuracy: 63.05 %\n",
      "Testing Accuracy: 47.27 %\n",
      "Test Loss:  66.8729932308197\n",
      "Epoch [11/500], Loss: 47.2252, Accuracy: 63.02 %\n",
      "Testing Accuracy: 46.51 %\n",
      "Test Loss:  68.18139266967773\n",
      "Epoch [12/500], Loss: 46.8586, Accuracy: 63.27 %\n",
      "Testing Accuracy: 47.04 %\n",
      "Test Loss:  66.99060380458832\n",
      "Epoch [13/500], Loss: 46.7260, Accuracy: 63.38 %\n",
      "Testing Accuracy: 46.26 %\n",
      "Test Loss:  68.05449056625366\n",
      "Epoch [14/500], Loss: 47.6082, Accuracy: 62.58 %\n",
      "Testing Accuracy: 47.60 %\n",
      "Test Loss:  66.31076323986053\n",
      "Epoch [15/500], Loss: 46.7837, Accuracy: 63.46 %\n",
      "Testing Accuracy: 47.64 %\n",
      "Test Loss:  66.7236624956131\n",
      "Epoch [16/500], Loss: 46.1993, Accuracy: 63.83 %\n",
      "Testing Accuracy: 46.49 %\n",
      "Test Loss:  68.43807971477509\n",
      "Epoch [17/500], Loss: 45.7832, Accuracy: 64.21 %\n",
      "Testing Accuracy: 46.52 %\n",
      "Test Loss:  67.23811447620392\n",
      "Epoch [18/500], Loss: 46.2801, Accuracy: 63.85 %\n",
      "Testing Accuracy: 46.78 %\n",
      "Test Loss:  68.34131562709808\n",
      "Epoch [19/500], Loss: 45.8054, Accuracy: 64.41 %\n",
      "Testing Accuracy: 47.15 %\n",
      "Test Loss:  66.45607280731201\n",
      "Epoch [20/500], Loss: 45.7100, Accuracy: 64.11 %\n",
      "Testing Accuracy: 47.87 %\n",
      "Test Loss:  66.2020161151886\n",
      "Epoch [21/500], Loss: 45.9203, Accuracy: 64.23 %\n",
      "Testing Accuracy: 46.56 %\n",
      "Test Loss:  67.12171018123627\n",
      "Epoch [22/500], Loss: 46.2137, Accuracy: 63.98 %\n",
      "Testing Accuracy: 46.60 %\n",
      "Test Loss:  67.9134818315506\n",
      "Epoch [23/500], Loss: 45.1270, Accuracy: 64.94 %\n",
      "Testing Accuracy: 46.83 %\n",
      "Test Loss:  67.32905316352844\n",
      "Epoch [24/500], Loss: 45.1989, Accuracy: 64.68 %\n",
      "Testing Accuracy: 47.73 %\n",
      "Test Loss:  66.34927833080292\n",
      "Epoch [25/500], Loss: 45.4674, Accuracy: 64.48 %\n",
      "Testing Accuracy: 47.51 %\n",
      "Test Loss:  66.22564136981964\n",
      "Epoch [26/500], Loss: 44.9916, Accuracy: 64.87 %\n",
      "Testing Accuracy: 47.67 %\n",
      "Test Loss:  67.22288918495178\n",
      "Epoch [27/500], Loss: 44.5734, Accuracy: 65.13 %\n",
      "Testing Accuracy: 46.04 %\n",
      "Test Loss:  68.28377115726471\n",
      "Testing Accuracy: 46.04 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS       \n",
      "--------------------------------  --------  \n",
      "Net/Linear[fc1]/onnx::Gemm        1011712   \n",
      "Net/Dropout[dropout]/onnx::Relu   988       \n",
      "Net/Linear[fc2]/onnx::Gemm        5434      \n",
      "-------------------------------   -------   \n",
      "Input size: (1, 2048)\n",
      "1,018,134 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796]\n",
      "pruning hidden size:  494\n",
      "with hidden layer:  494\n",
      "removing:  (416, 319, 242)\n",
      "--- 73.6481921672821 seconds ---\n",
      "Epoch [1/500], Loss: 45.3672, Accuracy: 64.58 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  68.91765940189362\n",
      "Epoch [2/500], Loss: 45.5205, Accuracy: 64.55 %\n",
      "Testing Accuracy: 47.22 %\n",
      "Test Loss:  67.25472736358643\n",
      "Epoch [3/500], Loss: 45.2299, Accuracy: 64.46 %\n",
      "Testing Accuracy: 47.13 %\n",
      "Test Loss:  66.81203377246857\n",
      "Epoch [4/500], Loss: 45.0915, Accuracy: 64.78 %\n",
      "Testing Accuracy: 46.87 %\n",
      "Test Loss:  66.97941219806671\n",
      "Epoch [5/500], Loss: 44.9519, Accuracy: 65.03 %\n",
      "Testing Accuracy: 46.86 %\n",
      "Test Loss:  68.17129576206207\n",
      "Epoch [6/500], Loss: 45.5336, Accuracy: 64.07 %\n",
      "Testing Accuracy: 46.17 %\n",
      "Test Loss:  69.06172800064087\n",
      "Epoch [7/500], Loss: 45.1797, Accuracy: 64.93 %\n",
      "Testing Accuracy: 47.75 %\n",
      "Test Loss:  65.98964846134186\n",
      "Epoch [8/500], Loss: 45.0749, Accuracy: 64.55 %\n",
      "Testing Accuracy: 47.38 %\n",
      "Test Loss:  67.26506745815277\n",
      "Epoch [9/500], Loss: 44.4720, Accuracy: 65.43 %\n",
      "Testing Accuracy: 46.59 %\n",
      "Test Loss:  67.63548302650452\n",
      "Epoch [10/500], Loss: 44.7707, Accuracy: 64.89 %\n",
      "Testing Accuracy: 46.80 %\n",
      "Test Loss:  66.55244600772858\n",
      "Epoch [11/500], Loss: 44.2588, Accuracy: 65.38 %\n",
      "Testing Accuracy: 47.54 %\n",
      "Test Loss:  66.75819718837738\n",
      "Epoch [12/500], Loss: 43.3784, Accuracy: 66.36 %\n",
      "Testing Accuracy: 46.99 %\n",
      "Test Loss:  66.86055529117584\n",
      "Epoch [13/500], Loss: 44.2260, Accuracy: 65.52 %\n",
      "Testing Accuracy: 47.11 %\n",
      "Test Loss:  67.44425940513611\n",
      "Epoch [14/500], Loss: 44.4014, Accuracy: 65.24 %\n",
      "Testing Accuracy: 47.89 %\n",
      "Test Loss:  66.29187619686127\n",
      "Epoch [15/500], Loss: 43.4374, Accuracy: 66.28 %\n",
      "Testing Accuracy: 47.23 %\n",
      "Test Loss:  66.57663559913635\n",
      "Epoch [16/500], Loss: 43.5662, Accuracy: 66.14 %\n",
      "Testing Accuracy: 47.38 %\n",
      "Test Loss:  67.18133211135864\n",
      "Epoch [17/500], Loss: 43.4551, Accuracy: 66.22 %\n",
      "Testing Accuracy: 46.59 %\n",
      "Test Loss:  68.25717151165009\n",
      "Epoch [18/500], Loss: 43.5445, Accuracy: 66.16 %\n",
      "Testing Accuracy: 45.73 %\n",
      "Test Loss:  68.39685559272766\n",
      "Epoch [19/500], Loss: 43.4562, Accuracy: 66.12 %\n",
      "Testing Accuracy: 46.85 %\n",
      "Test Loss:  67.73453664779663\n",
      "Epoch [20/500], Loss: 43.0880, Accuracy: 66.56 %\n",
      "Testing Accuracy: 47.20 %\n",
      "Test Loss:  67.1578016281128\n",
      "Epoch [21/500], Loss: 44.0733, Accuracy: 65.42 %\n",
      "Testing Accuracy: 47.86 %\n",
      "Test Loss:  66.71695482730865\n",
      "Epoch [22/500], Loss: 43.6947, Accuracy: 65.81 %\n",
      "Testing Accuracy: 45.58 %\n",
      "Test Loss:  71.56687760353088\n",
      "Epoch [23/500], Loss: 42.8201, Accuracy: 66.72 %\n",
      "Testing Accuracy: 47.67 %\n",
      "Test Loss:  66.61132800579071\n",
      "Epoch [24/500], Loss: 42.6797, Accuracy: 66.95 %\n",
      "Testing Accuracy: 46.84 %\n",
      "Test Loss:  67.8381153345108\n",
      "Epoch [25/500], Loss: 41.8876, Accuracy: 67.77 %\n",
      "Testing Accuracy: 47.70 %\n",
      "Test Loss:  66.47643256187439\n",
      "Epoch [26/500], Loss: 42.7176, Accuracy: 66.77 %\n",
      "Testing Accuracy: 46.80 %\n",
      "Test Loss:  67.07414102554321\n",
      "Epoch [27/500], Loss: 42.3496, Accuracy: 67.27 %\n",
      "Testing Accuracy: 47.47 %\n",
      "Test Loss:  66.56451630592346\n",
      "Testing Accuracy: 47.47 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS       \n",
      "--------------------------------  --------  \n",
      "Net/Linear[fc1]/onnx::Gemm        1005568   \n",
      "Net/Dropout[dropout]/onnx::Relu   982       \n",
      "Net/Linear[fc2]/onnx::Gemm        5401      \n",
      "-------------------------------   -------   \n",
      "Input size: (1, 2048)\n",
      "1,011,951 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966]\n",
      "pruning hidden size:  491\n",
      "with hidden layer:  491\n",
      "removing:  (427, 335, 211)\n",
      "--- 71.00053381919861 seconds ---\n",
      "Epoch [1/500], Loss: 42.1984, Accuracy: 67.04 %\n",
      "Testing Accuracy: 47.05 %\n",
      "Test Loss:  68.29665863513947\n",
      "Epoch [2/500], Loss: 42.8931, Accuracy: 66.68 %\n",
      "Testing Accuracy: 44.87 %\n",
      "Test Loss:  71.46440935134888\n",
      "Epoch [3/500], Loss: 43.2387, Accuracy: 66.26 %\n",
      "Testing Accuracy: 46.53 %\n",
      "Test Loss:  68.25204992294312\n",
      "Epoch [4/500], Loss: 42.7318, Accuracy: 67.01 %\n",
      "Testing Accuracy: 47.01 %\n",
      "Test Loss:  67.83751881122589\n",
      "Epoch [5/500], Loss: 42.5362, Accuracy: 66.65 %\n",
      "Testing Accuracy: 46.77 %\n",
      "Test Loss:  67.75265979766846\n",
      "Epoch [6/500], Loss: 42.2800, Accuracy: 67.02 %\n",
      "Testing Accuracy: 47.60 %\n",
      "Test Loss:  66.85859084129333\n",
      "Epoch [7/500], Loss: 42.5718, Accuracy: 66.78 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  71.7972024679184\n",
      "Epoch [8/500], Loss: 42.1877, Accuracy: 66.99 %\n",
      "Testing Accuracy: 47.83 %\n",
      "Test Loss:  66.89880180358887\n",
      "Epoch [9/500], Loss: 41.2281, Accuracy: 68.12 %\n",
      "Testing Accuracy: 47.05 %\n",
      "Test Loss:  68.53009843826294\n",
      "Epoch [10/500], Loss: 41.3868, Accuracy: 67.66 %\n",
      "Testing Accuracy: 47.31 %\n",
      "Test Loss:  67.04634439945221\n",
      "Epoch [11/500], Loss: 41.4097, Accuracy: 67.59 %\n",
      "Testing Accuracy: 46.30 %\n",
      "Test Loss:  68.72576880455017\n",
      "Epoch [12/500], Loss: 41.4121, Accuracy: 68.04 %\n",
      "Testing Accuracy: 46.80 %\n",
      "Test Loss:  68.36109495162964\n",
      "Epoch [13/500], Loss: 41.0140, Accuracy: 68.28 %\n",
      "Testing Accuracy: 46.10 %\n",
      "Test Loss:  69.24689948558807\n",
      "Epoch [14/500], Loss: 42.2292, Accuracy: 66.97 %\n",
      "Testing Accuracy: 46.82 %\n",
      "Test Loss:  69.5930939912796\n",
      "Epoch [15/500], Loss: 40.5251, Accuracy: 68.92 %\n",
      "Testing Accuracy: 47.24 %\n",
      "Test Loss:  67.0215790271759\n",
      "Epoch [16/500], Loss: 40.4444, Accuracy: 68.53 %\n",
      "Testing Accuracy: 46.96 %\n",
      "Test Loss:  68.30038046836853\n",
      "Epoch [17/500], Loss: 40.4844, Accuracy: 68.90 %\n",
      "Testing Accuracy: 47.37 %\n",
      "Test Loss:  67.49051284790039\n",
      "Epoch [18/500], Loss: 40.3438, Accuracy: 68.90 %\n",
      "Testing Accuracy: 46.25 %\n",
      "Test Loss:  69.35691726207733\n",
      "Epoch [19/500], Loss: 41.1931, Accuracy: 67.94 %\n",
      "Testing Accuracy: 46.74 %\n",
      "Test Loss:  68.12172424793243\n",
      "Epoch [20/500], Loss: 40.5864, Accuracy: 68.47 %\n",
      "Testing Accuracy: 46.92 %\n",
      "Test Loss:  67.87096631526947\n",
      "Epoch [21/500], Loss: 40.5816, Accuracy: 68.76 %\n",
      "Testing Accuracy: 45.77 %\n",
      "Test Loss:  69.41414165496826\n",
      "Epoch [22/500], Loss: 41.0820, Accuracy: 67.86 %\n",
      "Testing Accuracy: 47.43 %\n",
      "Test Loss:  67.21533715724945\n",
      "Epoch [23/500], Loss: 40.3538, Accuracy: 68.79 %\n",
      "Testing Accuracy: 46.32 %\n",
      "Test Loss:  68.69590866565704\n",
      "Epoch [24/500], Loss: 40.5068, Accuracy: 68.48 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  72.0619786977768\n",
      "Epoch [25/500], Loss: 41.0663, Accuracy: 68.12 %\n",
      "Testing Accuracy: 46.92 %\n",
      "Test Loss:  69.27978050708771\n",
      "Epoch [26/500], Loss: 40.3517, Accuracy: 68.35 %\n",
      "Testing Accuracy: 46.42 %\n",
      "Test Loss:  69.83298218250275\n",
      "Testing Accuracy: 46.42 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        999424   \n",
      "Net/Dropout[dropout]/onnx::Relu   976      \n",
      "Net/Linear[fc2]/onnx::Gemm        5368     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "1,005,768 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302]\n",
      "pruning hidden size:  488\n",
      "with hidden layer:  488\n",
      "removing:  (298, 132, 13)\n",
      "--- 70.447518825531 seconds ---\n",
      "Epoch [1/500], Loss: 40.0211, Accuracy: 68.93 %\n",
      "Testing Accuracy: 47.22 %\n",
      "Test Loss:  68.93111050128937\n",
      "Epoch [2/500], Loss: 40.3907, Accuracy: 68.58 %\n",
      "Testing Accuracy: 47.25 %\n",
      "Test Loss:  67.7397780418396\n",
      "Epoch [3/500], Loss: 40.2344, Accuracy: 68.96 %\n",
      "Testing Accuracy: 47.42 %\n",
      "Test Loss:  67.56756842136383\n",
      "Epoch [4/500], Loss: 39.9671, Accuracy: 68.82 %\n",
      "Testing Accuracy: 47.99 %\n",
      "Test Loss:  67.44242584705353\n",
      "Epoch [5/500], Loss: 40.1129, Accuracy: 68.58 %\n",
      "Testing Accuracy: 45.98 %\n",
      "Test Loss:  70.39101541042328\n",
      "Epoch [6/500], Loss: 41.0976, Accuracy: 67.80 %\n",
      "Testing Accuracy: 46.05 %\n",
      "Test Loss:  69.2353423833847\n",
      "Epoch [7/500], Loss: 40.6351, Accuracy: 68.35 %\n",
      "Testing Accuracy: 46.45 %\n",
      "Test Loss:  68.12222456932068\n",
      "Epoch [8/500], Loss: 40.0028, Accuracy: 68.62 %\n",
      "Testing Accuracy: 47.53 %\n",
      "Test Loss:  67.4772527217865\n",
      "Epoch [9/500], Loss: 39.4200, Accuracy: 69.57 %\n",
      "Testing Accuracy: 46.12 %\n",
      "Test Loss:  68.4598263502121\n",
      "Epoch [10/500], Loss: 39.3856, Accuracy: 69.62 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  75.24790728092194\n",
      "Epoch [11/500], Loss: 40.9983, Accuracy: 67.72 %\n",
      "Testing Accuracy: 47.77 %\n",
      "Test Loss:  68.01144003868103\n",
      "Epoch [12/500], Loss: 38.8642, Accuracy: 69.87 %\n",
      "Testing Accuracy: 46.11 %\n",
      "Test Loss:  69.19655764102936\n",
      "Epoch [13/500], Loss: 40.2307, Accuracy: 68.46 %\n",
      "Testing Accuracy: 47.00 %\n",
      "Test Loss:  68.16762006282806\n",
      "Epoch [14/500], Loss: 40.1623, Accuracy: 68.56 %\n",
      "Testing Accuracy: 47.25 %\n",
      "Test Loss:  68.59192407131195\n",
      "Epoch [15/500], Loss: 40.2264, Accuracy: 68.56 %\n",
      "Testing Accuracy: 46.14 %\n",
      "Test Loss:  68.7457423210144\n",
      "Epoch [16/500], Loss: 38.5439, Accuracy: 70.22 %\n",
      "Testing Accuracy: 46.61 %\n",
      "Test Loss:  69.90656089782715\n",
      "Epoch [17/500], Loss: 38.7563, Accuracy: 69.99 %\n",
      "Testing Accuracy: 47.11 %\n",
      "Test Loss:  68.16296875476837\n",
      "Epoch [18/500], Loss: 39.1960, Accuracy: 69.41 %\n",
      "Testing Accuracy: 46.31 %\n",
      "Test Loss:  68.95202600955963\n",
      "Epoch [19/500], Loss: 38.9874, Accuracy: 69.71 %\n",
      "Testing Accuracy: 45.66 %\n",
      "Test Loss:  70.75926625728607\n",
      "Epoch [20/500], Loss: 38.9983, Accuracy: 69.57 %\n",
      "Testing Accuracy: 46.58 %\n",
      "Test Loss:  69.2502669095993\n",
      "Epoch [21/500], Loss: 37.9746, Accuracy: 70.65 %\n",
      "Testing Accuracy: 46.14 %\n",
      "Test Loss:  69.92370307445526\n",
      "Epoch [22/500], Loss: 37.9507, Accuracy: 70.57 %\n",
      "Testing Accuracy: 47.35 %\n",
      "Test Loss:  68.50831460952759\n",
      "Epoch [23/500], Loss: 38.0663, Accuracy: 70.43 %\n",
      "Testing Accuracy: 46.37 %\n",
      "Test Loss:  69.3222645521164\n",
      "Epoch [24/500], Loss: 38.0340, Accuracy: 70.71 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  73.01730787754059\n",
      "Testing Accuracy: 45.17 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        993280   \n",
      "Net/Dropout[dropout]/onnx::Relu   970      \n",
      "Net/Linear[fc2]/onnx::Gemm        5335     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "999,585 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471]\n",
      "pruning hidden size:  485\n",
      "with hidden layer:  485\n",
      "removing:  (248, 164, 8)\n",
      "--- 69.77896690368652 seconds ---\n",
      "Epoch [1/500], Loss: 40.3789, Accuracy: 68.31 %\n",
      "Testing Accuracy: 46.20 %\n",
      "Test Loss:  69.89473950862885\n",
      "Epoch [2/500], Loss: 38.6993, Accuracy: 69.84 %\n",
      "Testing Accuracy: 46.86 %\n",
      "Test Loss:  69.25233113765717\n",
      "Epoch [3/500], Loss: 38.2404, Accuracy: 70.64 %\n",
      "Testing Accuracy: 45.74 %\n",
      "Test Loss:  71.37566041946411\n",
      "Epoch [4/500], Loss: 37.8844, Accuracy: 70.79 %\n",
      "Testing Accuracy: 47.19 %\n",
      "Test Loss:  68.44471299648285\n",
      "Epoch [5/500], Loss: 38.6002, Accuracy: 69.79 %\n",
      "Testing Accuracy: 47.21 %\n",
      "Test Loss:  68.63979291915894\n",
      "Epoch [6/500], Loss: 38.3803, Accuracy: 70.15 %\n",
      "Testing Accuracy: 46.58 %\n",
      "Test Loss:  69.2689915895462\n",
      "Epoch [7/500], Loss: 38.4443, Accuracy: 69.99 %\n",
      "Testing Accuracy: 46.24 %\n",
      "Test Loss:  69.7662878036499\n",
      "Epoch [8/500], Loss: 38.0284, Accuracy: 70.57 %\n",
      "Testing Accuracy: 46.59 %\n",
      "Test Loss:  69.84690821170807\n",
      "Epoch [9/500], Loss: 37.4399, Accuracy: 71.01 %\n",
      "Testing Accuracy: 47.09 %\n",
      "Test Loss:  69.16302824020386\n",
      "Epoch [10/500], Loss: 37.0364, Accuracy: 71.48 %\n",
      "Testing Accuracy: 47.07 %\n",
      "Test Loss:  68.53658473491669\n",
      "Epoch [11/500], Loss: 37.4573, Accuracy: 70.81 %\n",
      "Testing Accuracy: 45.82 %\n",
      "Test Loss:  70.31396448612213\n",
      "Epoch [12/500], Loss: 38.2770, Accuracy: 70.08 %\n",
      "Testing Accuracy: 46.02 %\n",
      "Test Loss:  71.33034789562225\n",
      "Epoch [13/500], Loss: 38.8817, Accuracy: 69.61 %\n",
      "Testing Accuracy: 46.20 %\n",
      "Test Loss:  71.85301959514618\n",
      "Epoch [14/500], Loss: 37.9082, Accuracy: 70.46 %\n",
      "Testing Accuracy: 46.53 %\n",
      "Test Loss:  69.9805748462677\n",
      "Epoch [15/500], Loss: 37.2694, Accuracy: 71.18 %\n",
      "Testing Accuracy: 46.02 %\n",
      "Test Loss:  70.62757301330566\n",
      "Epoch [16/500], Loss: 37.6532, Accuracy: 70.63 %\n",
      "Testing Accuracy: 45.91 %\n",
      "Test Loss:  71.69489800930023\n",
      "Epoch [17/500], Loss: 37.5863, Accuracy: 70.86 %\n",
      "Testing Accuracy: 47.38 %\n",
      "Test Loss:  69.47626090049744\n",
      "Epoch [18/500], Loss: 36.8678, Accuracy: 71.62 %\n",
      "Testing Accuracy: 45.25 %\n",
      "Test Loss:  72.38754200935364\n",
      "Epoch [19/500], Loss: 36.9396, Accuracy: 71.41 %\n",
      "Testing Accuracy: 47.50 %\n",
      "Test Loss:  69.56690490245819\n",
      "Epoch [20/500], Loss: 36.7611, Accuracy: 71.57 %\n",
      "Testing Accuracy: 45.99 %\n",
      "Test Loss:  70.18822145462036\n",
      "Epoch [21/500], Loss: 37.7785, Accuracy: 70.27 %\n",
      "Testing Accuracy: 45.67 %\n",
      "Test Loss:  72.86608350276947\n",
      "Epoch [22/500], Loss: 36.9957, Accuracy: 71.21 %\n",
      "Testing Accuracy: 47.40 %\n",
      "Test Loss:  68.27292823791504\n",
      "Epoch [23/500], Loss: 36.0168, Accuracy: 72.30 %\n",
      "Testing Accuracy: 46.37 %\n",
      "Test Loss:  71.88479840755463\n",
      "Epoch [24/500], Loss: 36.3515, Accuracy: 71.78 %\n",
      "Testing Accuracy: 47.33 %\n",
      "Test Loss:  68.69150698184967\n",
      "Epoch [25/500], Loss: 36.3429, Accuracy: 71.71 %\n",
      "Testing Accuracy: 47.01 %\n",
      "Test Loss:  69.74481797218323\n",
      "Epoch [26/500], Loss: 35.6157, Accuracy: 72.55 %\n",
      "Testing Accuracy: 46.22 %\n",
      "Test Loss:  70.72221529483795\n",
      "Epoch [27/500], Loss: 36.6258, Accuracy: 71.41 %\n",
      "Testing Accuracy: 46.39 %\n",
      "Test Loss:  71.1919115781784\n",
      "Epoch [28/500], Loss: 35.9530, Accuracy: 72.27 %\n",
      "Testing Accuracy: 47.74 %\n",
      "Test Loss:  68.39463257789612\n",
      "Epoch [29/500], Loss: 35.4370, Accuracy: 72.58 %\n",
      "Testing Accuracy: 46.99 %\n",
      "Test Loss:  70.11453080177307\n",
      "Epoch [30/500], Loss: 35.5896, Accuracy: 72.62 %\n",
      "Testing Accuracy: 47.29 %\n",
      "Test Loss:  70.4798458814621\n",
      "Epoch [31/500], Loss: 35.8292, Accuracy: 72.18 %\n",
      "Testing Accuracy: 45.86 %\n",
      "Test Loss:  70.53713750839233\n",
      "Epoch [32/500], Loss: 35.5564, Accuracy: 72.42 %\n",
      "Testing Accuracy: 47.12 %\n",
      "Test Loss:  69.78149950504303\n",
      "Epoch [33/500], Loss: 35.5124, Accuracy: 72.74 %\n",
      "Testing Accuracy: 46.41 %\n",
      "Test Loss:  70.44313931465149\n",
      "Epoch [34/500], Loss: 35.6931, Accuracy: 72.26 %\n",
      "Testing Accuracy: 45.39 %\n",
      "Test Loss:  73.56532502174377\n",
      "Epoch [35/500], Loss: 36.3364, Accuracy: 71.75 %\n",
      "Testing Accuracy: 46.00 %\n",
      "Test Loss:  71.35237956047058\n",
      "Epoch [36/500], Loss: 36.5051, Accuracy: 71.46 %\n",
      "Testing Accuracy: 45.43 %\n",
      "Test Loss:  71.78011929988861\n",
      "Epoch [37/500], Loss: 35.5650, Accuracy: 72.55 %\n",
      "Testing Accuracy: 46.61 %\n",
      "Test Loss:  70.78551828861237\n",
      "Epoch [38/500], Loss: 35.6859, Accuracy: 72.11 %\n",
      "Testing Accuracy: 46.44 %\n",
      "Test Loss:  70.36979627609253\n",
      "Epoch [39/500], Loss: 35.3045, Accuracy: 72.67 %\n",
      "Testing Accuracy: 46.17 %\n",
      "Test Loss:  72.04657745361328\n",
      "Epoch [40/500], Loss: 35.0464, Accuracy: 72.77 %\n",
      "Testing Accuracy: 46.97 %\n",
      "Test Loss:  70.26178085803986\n",
      "Epoch [41/500], Loss: 34.6141, Accuracy: 73.48 %\n",
      "Testing Accuracy: 47.03 %\n",
      "Test Loss:  69.55739486217499\n",
      "Epoch [42/500], Loss: 35.6808, Accuracy: 72.54 %\n",
      "Testing Accuracy: 46.26 %\n",
      "Test Loss:  71.21107006072998\n",
      "Testing Accuracy: 46.26 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        987136   \n",
      "Net/Dropout[dropout]/onnx::Relu   964      \n",
      "Net/Linear[fc2]/onnx::Gemm        5302     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "993,402 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877]\n",
      "pruning hidden size:  482\n",
      "with hidden layer:  482\n",
      "removing:  (449, 368, 345)\n",
      "--- 67.7921667098999 seconds ---\n",
      "Epoch [1/500], Loss: 34.7258, Accuracy: 73.28 %\n",
      "Testing Accuracy: 45.85 %\n",
      "Test Loss:  72.44215214252472\n",
      "Epoch [2/500], Loss: 34.7478, Accuracy: 73.24 %\n",
      "Testing Accuracy: 47.23 %\n",
      "Test Loss:  69.63633012771606\n",
      "Epoch [3/500], Loss: 34.5981, Accuracy: 73.39 %\n",
      "Testing Accuracy: 47.03 %\n",
      "Test Loss:  69.22196137905121\n",
      "Epoch [4/500], Loss: 34.8150, Accuracy: 73.22 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  71.72252261638641\n",
      "Epoch [5/500], Loss: 35.2405, Accuracy: 72.83 %\n",
      "Testing Accuracy: 46.28 %\n",
      "Test Loss:  70.12973749637604\n",
      "Epoch [6/500], Loss: 34.7517, Accuracy: 73.14 %\n",
      "Testing Accuracy: 44.21 %\n",
      "Test Loss:  75.23904025554657\n",
      "Epoch [7/500], Loss: 36.3159, Accuracy: 71.53 %\n",
      "Testing Accuracy: 46.51 %\n",
      "Test Loss:  71.02018332481384\n",
      "Epoch [8/500], Loss: 34.5582, Accuracy: 73.43 %\n",
      "Testing Accuracy: 46.16 %\n",
      "Test Loss:  71.87688767910004\n",
      "Epoch [9/500], Loss: 34.9149, Accuracy: 72.72 %\n",
      "Testing Accuracy: 46.01 %\n",
      "Test Loss:  72.49437761306763\n",
      "Epoch [10/500], Loss: 34.3742, Accuracy: 73.47 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  70.86964094638824\n",
      "Epoch [11/500], Loss: 34.3538, Accuracy: 73.39 %\n",
      "Testing Accuracy: 46.94 %\n",
      "Test Loss:  70.03798484802246\n",
      "Epoch [12/500], Loss: 34.6326, Accuracy: 73.27 %\n",
      "Testing Accuracy: 47.44 %\n",
      "Test Loss:  70.34908246994019\n",
      "Epoch [13/500], Loss: 34.5564, Accuracy: 73.46 %\n",
      "Testing Accuracy: 46.85 %\n",
      "Test Loss:  69.70900046825409\n",
      "Epoch [14/500], Loss: 34.0921, Accuracy: 73.51 %\n",
      "Testing Accuracy: 46.41 %\n",
      "Test Loss:  70.77220022678375\n",
      "Epoch [15/500], Loss: 34.9639, Accuracy: 73.07 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  72.70531117916107\n",
      "Epoch [16/500], Loss: 34.8608, Accuracy: 73.03 %\n",
      "Testing Accuracy: 46.21 %\n",
      "Test Loss:  71.1315279006958\n",
      "Epoch [17/500], Loss: 35.3445, Accuracy: 72.39 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  72.8405385017395\n",
      "Epoch [18/500], Loss: 34.1191, Accuracy: 73.68 %\n",
      "Testing Accuracy: 45.87 %\n",
      "Test Loss:  72.24882102012634\n",
      "Epoch [19/500], Loss: 33.4900, Accuracy: 74.52 %\n",
      "Testing Accuracy: 46.97 %\n",
      "Test Loss:  70.3236916065216\n",
      "Epoch [20/500], Loss: 33.9030, Accuracy: 73.72 %\n",
      "Testing Accuracy: 46.07 %\n",
      "Test Loss:  72.68739187717438\n",
      "Epoch [21/500], Loss: 33.6355, Accuracy: 73.95 %\n",
      "Testing Accuracy: 45.85 %\n",
      "Test Loss:  73.14952075481415\n",
      "Epoch [22/500], Loss: 34.1618, Accuracy: 73.45 %\n",
      "Testing Accuracy: 46.72 %\n",
      "Test Loss:  70.6597706079483\n",
      "Epoch [23/500], Loss: 33.6543, Accuracy: 73.97 %\n",
      "Testing Accuracy: 46.12 %\n",
      "Test Loss:  70.56452786922455\n",
      "Testing Accuracy: 46.12 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        980992   \n",
      "Net/Dropout[dropout]/onnx::Relu   958      \n",
      "Net/Linear[fc2]/onnx::Gemm        5269     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "987,219 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737]\n",
      "pruning hidden size:  479\n",
      "with hidden layer:  479\n",
      "removing:  (341, 283, 69)\n",
      "--- 68.73071670532227 seconds ---\n",
      "Epoch [1/500], Loss: 33.4816, Accuracy: 74.18 %\n",
      "Testing Accuracy: 46.69 %\n",
      "Test Loss:  70.31127798557281\n",
      "Epoch [2/500], Loss: 33.0178, Accuracy: 74.80 %\n",
      "Testing Accuracy: 45.82 %\n",
      "Test Loss:  71.93580508232117\n",
      "Epoch [3/500], Loss: 33.6549, Accuracy: 74.13 %\n",
      "Testing Accuracy: 46.57 %\n",
      "Test Loss:  70.48378622531891\n",
      "Epoch [4/500], Loss: 33.8213, Accuracy: 73.77 %\n",
      "Testing Accuracy: 42.48 %\n",
      "Test Loss:  78.33617448806763\n",
      "Epoch [5/500], Loss: 34.8213, Accuracy: 72.67 %\n",
      "Testing Accuracy: 45.00 %\n",
      "Test Loss:  76.71685802936554\n",
      "Epoch [6/500], Loss: 34.2144, Accuracy: 73.59 %\n",
      "Testing Accuracy: 46.96 %\n",
      "Test Loss:  71.24574494361877\n",
      "Epoch [7/500], Loss: 33.9599, Accuracy: 73.59 %\n",
      "Testing Accuracy: 46.38 %\n",
      "Test Loss:  71.807443857193\n",
      "Epoch [8/500], Loss: 33.6432, Accuracy: 73.92 %\n",
      "Testing Accuracy: 46.65 %\n",
      "Test Loss:  71.11688375473022\n",
      "Epoch [9/500], Loss: 32.9877, Accuracy: 74.67 %\n",
      "Testing Accuracy: 45.99 %\n",
      "Test Loss:  73.27100551128387\n",
      "Epoch [10/500], Loss: 33.6184, Accuracy: 73.77 %\n",
      "Testing Accuracy: 46.85 %\n",
      "Test Loss:  71.16583406925201\n",
      "Epoch [11/500], Loss: 33.0176, Accuracy: 74.53 %\n",
      "Testing Accuracy: 46.24 %\n",
      "Test Loss:  72.71436738967896\n",
      "Epoch [12/500], Loss: 32.6510, Accuracy: 75.04 %\n",
      "Testing Accuracy: 46.98 %\n",
      "Test Loss:  71.41335940361023\n",
      "Epoch [13/500], Loss: 33.2996, Accuracy: 74.30 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  73.53414559364319\n",
      "Epoch [14/500], Loss: 33.9458, Accuracy: 73.80 %\n",
      "Testing Accuracy: 45.56 %\n",
      "Test Loss:  72.13676261901855\n",
      "Epoch [15/500], Loss: 33.8375, Accuracy: 73.78 %\n",
      "Testing Accuracy: 46.10 %\n",
      "Test Loss:  71.57414746284485\n",
      "Epoch [16/500], Loss: 32.8292, Accuracy: 74.43 %\n",
      "Testing Accuracy: 45.39 %\n",
      "Test Loss:  75.13619792461395\n",
      "Epoch [17/500], Loss: 32.9411, Accuracy: 74.62 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  74.37510180473328\n",
      "Epoch [18/500], Loss: 32.7944, Accuracy: 74.59 %\n",
      "Testing Accuracy: 46.57 %\n",
      "Test Loss:  71.50451731681824\n",
      "Epoch [19/500], Loss: 32.9315, Accuracy: 74.55 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  74.32631957530975\n",
      "Epoch [20/500], Loss: 33.3019, Accuracy: 73.97 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  73.7409131526947\n",
      "Epoch [21/500], Loss: 32.9848, Accuracy: 74.71 %\n",
      "Testing Accuracy: 46.10 %\n",
      "Test Loss:  72.49494409561157\n",
      "Testing Accuracy: 46.10 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        974848   \n",
      "Net/Dropout[dropout]/onnx::Relu   952      \n",
      "Net/Linear[fc2]/onnx::Gemm        5236     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "981,036 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655]\n",
      "pruning hidden size:  476\n",
      "with hidden layer:  476\n",
      "removing:  (458, 262, 405)\n",
      "--- 67.42638611793518 seconds ---\n",
      "Epoch [1/500], Loss: 32.6638, Accuracy: 74.91 %\n",
      "Testing Accuracy: 46.41 %\n",
      "Test Loss:  72.2653648853302\n",
      "Epoch [2/500], Loss: 33.1204, Accuracy: 74.06 %\n",
      "Testing Accuracy: 44.33 %\n",
      "Test Loss:  75.2609885931015\n",
      "Epoch [3/500], Loss: 32.2380, Accuracy: 75.16 %\n",
      "Testing Accuracy: 46.08 %\n",
      "Test Loss:  72.54341447353363\n",
      "Epoch [4/500], Loss: 32.2706, Accuracy: 75.23 %\n",
      "Testing Accuracy: 46.35 %\n",
      "Test Loss:  71.3888111114502\n",
      "Epoch [5/500], Loss: 32.1500, Accuracy: 75.22 %\n",
      "Testing Accuracy: 46.05 %\n",
      "Test Loss:  72.51486611366272\n",
      "Epoch [6/500], Loss: 32.3258, Accuracy: 75.22 %\n",
      "Testing Accuracy: 47.00 %\n",
      "Test Loss:  71.38875389099121\n",
      "Epoch [7/500], Loss: 32.3332, Accuracy: 74.79 %\n",
      "Testing Accuracy: 46.14 %\n",
      "Test Loss:  71.05551159381866\n",
      "Epoch [8/500], Loss: 32.1160, Accuracy: 75.25 %\n",
      "Testing Accuracy: 45.42 %\n",
      "Test Loss:  75.05238318443298\n",
      "Epoch [9/500], Loss: 33.5063, Accuracy: 73.72 %\n",
      "Testing Accuracy: 46.22 %\n",
      "Test Loss:  72.8735579252243\n",
      "Epoch [10/500], Loss: 32.1876, Accuracy: 75.16 %\n",
      "Testing Accuracy: 46.68 %\n",
      "Test Loss:  71.78389799594879\n",
      "Epoch [11/500], Loss: 32.1491, Accuracy: 75.24 %\n",
      "Testing Accuracy: 45.41 %\n",
      "Test Loss:  74.1110805273056\n",
      "Epoch [12/500], Loss: 32.5946, Accuracy: 74.25 %\n",
      "Testing Accuracy: 46.30 %\n",
      "Test Loss:  72.39622986316681\n",
      "Epoch [13/500], Loss: 31.6989, Accuracy: 75.47 %\n",
      "Testing Accuracy: 45.48 %\n",
      "Test Loss:  76.80436313152313\n",
      "Epoch [14/500], Loss: 31.8873, Accuracy: 75.39 %\n",
      "Testing Accuracy: 45.58 %\n",
      "Test Loss:  73.43187582492828\n",
      "Epoch [15/500], Loss: 31.5964, Accuracy: 75.82 %\n",
      "Testing Accuracy: 46.20 %\n",
      "Test Loss:  73.2322643995285\n",
      "Epoch [16/500], Loss: 31.4541, Accuracy: 75.79 %\n",
      "Testing Accuracy: 45.38 %\n",
      "Test Loss:  74.37431037425995\n",
      "Epoch [17/500], Loss: 31.7952, Accuracy: 75.40 %\n",
      "Testing Accuracy: 46.33 %\n",
      "Test Loss:  73.12236428260803\n",
      "Epoch [18/500], Loss: 31.4616, Accuracy: 75.84 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  76.68687629699707\n",
      "Epoch [19/500], Loss: 31.4955, Accuracy: 75.99 %\n",
      "Testing Accuracy: 46.27 %\n",
      "Test Loss:  73.91053676605225\n",
      "Epoch [20/500], Loss: 30.4025, Accuracy: 76.81 %\n",
      "Testing Accuracy: 46.41 %\n",
      "Test Loss:  72.60296094417572\n",
      "Epoch [21/500], Loss: 30.9573, Accuracy: 76.10 %\n",
      "Testing Accuracy: 46.61 %\n",
      "Test Loss:  71.65392744541168\n",
      "Epoch [22/500], Loss: 30.8587, Accuracy: 76.15 %\n",
      "Testing Accuracy: 46.49 %\n",
      "Test Loss:  74.1564062833786\n",
      "Epoch [23/500], Loss: 30.2235, Accuracy: 76.68 %\n",
      "Testing Accuracy: 46.89 %\n",
      "Test Loss:  73.57324039936066\n",
      "Epoch [24/500], Loss: 30.6115, Accuracy: 76.46 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  74.64990663528442\n",
      "Epoch [25/500], Loss: 30.6970, Accuracy: 76.18 %\n",
      "Testing Accuracy: 46.55 %\n",
      "Test Loss:  74.01380753517151\n",
      "Epoch [26/500], Loss: 30.2653, Accuracy: 76.88 %\n",
      "Testing Accuracy: 46.88 %\n",
      "Test Loss:  72.07769393920898\n",
      "Epoch [27/500], Loss: 29.6462, Accuracy: 77.38 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  72.95809829235077\n",
      "Testing Accuracy: 45.90 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        968704   \n",
      "Net/Dropout[dropout]/onnx::Relu   946      \n",
      "Net/Linear[fc2]/onnx::Gemm        5203     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "974,853 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199]\n",
      "pruning hidden size:  473\n",
      "with hidden layer:  473\n",
      "removing:  (377, 268, 404)\n",
      "--- 66.28223013877869 seconds ---\n",
      "Epoch [1/500], Loss: 31.3875, Accuracy: 75.66 %\n",
      "Testing Accuracy: 46.34 %\n",
      "Test Loss:  74.28895425796509\n",
      "Epoch [2/500], Loss: 30.6999, Accuracy: 76.34 %\n",
      "Testing Accuracy: 46.39 %\n",
      "Test Loss:  73.31185841560364\n",
      "Epoch [3/500], Loss: 30.5337, Accuracy: 76.59 %\n",
      "Testing Accuracy: 45.48 %\n",
      "Test Loss:  75.48768746852875\n",
      "Epoch [4/500], Loss: 30.6701, Accuracy: 76.39 %\n",
      "Testing Accuracy: 45.38 %\n",
      "Test Loss:  75.32375228404999\n",
      "Epoch [5/500], Loss: 30.6839, Accuracy: 76.55 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  77.9442583322525\n",
      "Epoch [6/500], Loss: 30.8861, Accuracy: 76.24 %\n",
      "Testing Accuracy: 45.54 %\n",
      "Test Loss:  75.40690112113953\n",
      "Epoch [7/500], Loss: 29.8681, Accuracy: 77.02 %\n",
      "Testing Accuracy: 45.67 %\n",
      "Test Loss:  73.91173660755157\n",
      "Epoch [8/500], Loss: 30.9771, Accuracy: 76.09 %\n",
      "Testing Accuracy: 46.12 %\n",
      "Test Loss:  74.3630940914154\n",
      "Epoch [9/500], Loss: 30.6777, Accuracy: 76.40 %\n",
      "Testing Accuracy: 46.43 %\n",
      "Test Loss:  72.92248499393463\n",
      "Epoch [10/500], Loss: 30.3524, Accuracy: 76.84 %\n",
      "Testing Accuracy: 46.80 %\n",
      "Test Loss:  73.53598093986511\n",
      "Epoch [11/500], Loss: 29.7335, Accuracy: 77.24 %\n",
      "Testing Accuracy: 46.20 %\n",
      "Test Loss:  73.6143947839737\n",
      "Epoch [12/500], Loss: 30.0105, Accuracy: 76.83 %\n",
      "Testing Accuracy: 46.27 %\n",
      "Test Loss:  73.74938213825226\n",
      "Epoch [13/500], Loss: 30.1600, Accuracy: 76.72 %\n",
      "Testing Accuracy: 45.77 %\n",
      "Test Loss:  74.3401347398758\n",
      "Epoch [14/500], Loss: 30.0769, Accuracy: 76.84 %\n",
      "Testing Accuracy: 46.45 %\n",
      "Test Loss:  73.07910454273224\n",
      "Epoch [15/500], Loss: 29.6554, Accuracy: 77.33 %\n",
      "Testing Accuracy: 45.29 %\n",
      "Test Loss:  76.95545518398285\n",
      "Epoch [16/500], Loss: 30.5063, Accuracy: 76.26 %\n",
      "Testing Accuracy: 46.24 %\n",
      "Test Loss:  75.0191216468811\n",
      "Epoch [17/500], Loss: 29.7939, Accuracy: 77.11 %\n",
      "Testing Accuracy: 45.53 %\n",
      "Test Loss:  76.07944250106812\n",
      "Epoch [18/500], Loss: 29.3871, Accuracy: 77.60 %\n",
      "Testing Accuracy: 46.34 %\n",
      "Test Loss:  73.91306340694427\n",
      "Epoch [19/500], Loss: 29.6639, Accuracy: 77.20 %\n",
      "Testing Accuracy: 45.87 %\n",
      "Test Loss:  74.24569952487946\n",
      "Epoch [20/500], Loss: 29.6772, Accuracy: 77.40 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  75.05612397193909\n",
      "Epoch [21/500], Loss: 30.6130, Accuracy: 76.09 %\n",
      "Testing Accuracy: 45.22 %\n",
      "Test Loss:  75.97106087207794\n",
      "Epoch [22/500], Loss: 30.2528, Accuracy: 76.61 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  81.52888512611389\n",
      "Epoch [23/500], Loss: 30.4936, Accuracy: 76.34 %\n",
      "Testing Accuracy: 46.67 %\n",
      "Test Loss:  73.66239595413208\n",
      "Epoch [24/500], Loss: 29.8618, Accuracy: 76.86 %\n",
      "Testing Accuracy: 46.42 %\n",
      "Test Loss:  75.38892316818237\n",
      "Epoch [25/500], Loss: 28.8343, Accuracy: 77.93 %\n",
      "Testing Accuracy: 46.49 %\n",
      "Test Loss:  73.33560633659363\n",
      "Epoch [26/500], Loss: 28.9886, Accuracy: 77.77 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  75.30344581604004\n",
      "Epoch [27/500], Loss: 29.6478, Accuracy: 77.11 %\n",
      "Testing Accuracy: 46.33 %\n",
      "Test Loss:  73.90409731864929\n",
      "Epoch [28/500], Loss: 29.0278, Accuracy: 77.80 %\n",
      "Testing Accuracy: 45.52 %\n",
      "Test Loss:  73.46347296237946\n",
      "Epoch [29/500], Loss: 28.6269, Accuracy: 77.94 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  79.44721698760986\n",
      "Testing Accuracy: 44.74 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        962560   \n",
      "Net/Dropout[dropout]/onnx::Relu   940      \n",
      "Net/Linear[fc2]/onnx::Gemm        5170     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "968,670 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222]\n",
      "pruning hidden size:  470\n",
      "with hidden layer:  470\n",
      "removing:  (458, 262, 402)\n",
      "--- 65.28826594352722 seconds ---\n",
      "Epoch [1/500], Loss: 29.9253, Accuracy: 77.06 %\n",
      "Testing Accuracy: 45.62 %\n",
      "Test Loss:  77.4229588508606\n",
      "Epoch [2/500], Loss: 30.2439, Accuracy: 76.78 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  75.16120791435242\n",
      "Epoch [3/500], Loss: 28.8306, Accuracy: 78.09 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  75.00603747367859\n",
      "Epoch [4/500], Loss: 29.0057, Accuracy: 77.79 %\n",
      "Testing Accuracy: 45.56 %\n",
      "Test Loss:  75.49899566173553\n",
      "Epoch [5/500], Loss: 29.4964, Accuracy: 77.56 %\n",
      "Testing Accuracy: 45.75 %\n",
      "Test Loss:  75.22118389606476\n",
      "Epoch [6/500], Loss: 29.3374, Accuracy: 77.53 %\n",
      "Testing Accuracy: 45.61 %\n",
      "Test Loss:  75.07102739810944\n",
      "Epoch [7/500], Loss: 29.3952, Accuracy: 77.76 %\n",
      "Testing Accuracy: 45.93 %\n",
      "Test Loss:  75.2383908033371\n",
      "Epoch [8/500], Loss: 29.2511, Accuracy: 77.71 %\n",
      "Testing Accuracy: 45.45 %\n",
      "Test Loss:  75.54273927211761\n",
      "Epoch [9/500], Loss: 29.8247, Accuracy: 76.78 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  78.43179416656494\n",
      "Epoch [10/500], Loss: 29.5602, Accuracy: 77.25 %\n",
      "Testing Accuracy: 45.41 %\n",
      "Test Loss:  76.37293839454651\n",
      "Epoch [11/500], Loss: 28.3608, Accuracy: 78.59 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  76.94502425193787\n",
      "Epoch [12/500], Loss: 28.5090, Accuracy: 78.05 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  77.87138450145721\n",
      "Epoch [13/500], Loss: 27.8964, Accuracy: 78.86 %\n",
      "Testing Accuracy: 45.86 %\n",
      "Test Loss:  76.54954898357391\n",
      "Epoch [14/500], Loss: 27.9330, Accuracy: 78.92 %\n",
      "Testing Accuracy: 45.98 %\n",
      "Test Loss:  76.76971364021301\n",
      "Epoch [15/500], Loss: 28.6938, Accuracy: 77.88 %\n",
      "Testing Accuracy: 45.62 %\n",
      "Test Loss:  75.89779579639435\n",
      "Epoch [16/500], Loss: 29.0140, Accuracy: 77.43 %\n",
      "Testing Accuracy: 46.14 %\n",
      "Test Loss:  75.99581015110016\n",
      "Epoch [17/500], Loss: 28.3129, Accuracy: 78.12 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  77.47725999355316\n",
      "Epoch [18/500], Loss: 27.9972, Accuracy: 78.85 %\n",
      "Testing Accuracy: 46.22 %\n",
      "Test Loss:  74.75030028820038\n",
      "Epoch [19/500], Loss: 28.7233, Accuracy: 77.83 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  78.02258133888245\n",
      "Epoch [20/500], Loss: 28.3472, Accuracy: 78.10 %\n",
      "Testing Accuracy: 46.21 %\n",
      "Test Loss:  76.00841653347015\n",
      "Epoch [21/500], Loss: 28.0063, Accuracy: 78.54 %\n",
      "Testing Accuracy: 46.03 %\n",
      "Test Loss:  74.58693516254425\n",
      "Epoch [22/500], Loss: 28.0601, Accuracy: 78.58 %\n",
      "Testing Accuracy: 46.59 %\n",
      "Test Loss:  74.87378311157227\n",
      "Epoch [23/500], Loss: 27.7802, Accuracy: 78.93 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  78.49517977237701\n",
      "Epoch [24/500], Loss: 27.9136, Accuracy: 78.78 %\n",
      "Testing Accuracy: 45.95 %\n",
      "Test Loss:  75.7268807888031\n",
      "Epoch [25/500], Loss: 27.0244, Accuracy: 79.39 %\n",
      "Testing Accuracy: 46.02 %\n",
      "Test Loss:  75.79227912425995\n",
      "Epoch [26/500], Loss: 28.2242, Accuracy: 78.44 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  77.07538962364197\n",
      "Epoch [27/500], Loss: 28.6067, Accuracy: 77.71 %\n",
      "Testing Accuracy: 46.31 %\n",
      "Test Loss:  74.44006597995758\n",
      "Epoch [28/500], Loss: 27.3957, Accuracy: 79.28 %\n",
      "Testing Accuracy: 46.11 %\n",
      "Test Loss:  77.15182113647461\n",
      "Epoch [29/500], Loss: 26.6624, Accuracy: 79.79 %\n",
      "Testing Accuracy: 46.24 %\n",
      "Test Loss:  76.53411161899567\n",
      "Epoch [30/500], Loss: 27.3501, Accuracy: 78.99 %\n",
      "Testing Accuracy: 45.77 %\n",
      "Test Loss:  77.44622075557709\n",
      "Epoch [31/500], Loss: 28.0125, Accuracy: 78.51 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  76.17224836349487\n",
      "Epoch [32/500], Loss: 27.8624, Accuracy: 78.43 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  76.53238677978516\n",
      "Epoch [33/500], Loss: 27.1207, Accuracy: 79.57 %\n",
      "Testing Accuracy: 46.62 %\n",
      "Test Loss:  74.9549800157547\n",
      "Epoch [34/500], Loss: 27.0311, Accuracy: 79.57 %\n",
      "Testing Accuracy: 46.03 %\n",
      "Test Loss:  76.05652320384979\n",
      "Epoch [35/500], Loss: 27.4397, Accuracy: 78.94 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  77.16799449920654\n",
      "Epoch [36/500], Loss: 27.2209, Accuracy: 79.30 %\n",
      "Testing Accuracy: 46.45 %\n",
      "Test Loss:  76.27213704586029\n",
      "Epoch [37/500], Loss: 26.6952, Accuracy: 79.51 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  76.81771445274353\n",
      "Epoch [38/500], Loss: 27.4680, Accuracy: 78.97 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  77.3708245754242\n",
      "Epoch [39/500], Loss: 28.0901, Accuracy: 78.19 %\n",
      "Testing Accuracy: 45.97 %\n",
      "Test Loss:  76.27440941333771\n",
      "Epoch [40/500], Loss: 28.0679, Accuracy: 78.55 %\n",
      "Testing Accuracy: 46.16 %\n",
      "Test Loss:  77.72224795818329\n",
      "Epoch [41/500], Loss: 28.2338, Accuracy: 78.09 %\n",
      "Testing Accuracy: 45.45 %\n",
      "Test Loss:  76.26847994327545\n",
      "Epoch [42/500], Loss: 27.9815, Accuracy: 78.52 %\n",
      "Testing Accuracy: 45.76 %\n",
      "Test Loss:  75.89509427547455\n",
      "Epoch [43/500], Loss: 27.1293, Accuracy: 78.96 %\n",
      "Testing Accuracy: 45.75 %\n",
      "Test Loss:  76.4940322637558\n",
      "Epoch [44/500], Loss: 26.7985, Accuracy: 79.72 %\n",
      "Testing Accuracy: 45.49 %\n",
      "Test Loss:  77.74243462085724\n",
      "Epoch [45/500], Loss: 26.4887, Accuracy: 79.68 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  78.73663687705994\n",
      "Epoch [46/500], Loss: 27.4309, Accuracy: 79.00 %\n",
      "Testing Accuracy: 44.93 %\n",
      "Test Loss:  82.08044564723969\n",
      "Epoch [47/500], Loss: 27.8872, Accuracy: 78.20 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  77.06168603897095\n",
      "Testing Accuracy: 45.17 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        956416   \n",
      "Net/Dropout[dropout]/onnx::Relu   934      \n",
      "Net/Linear[fc2]/onnx::Gemm        5137     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "962,487 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471]\n",
      "pruning hidden size:  467\n",
      "with hidden layer:  467\n",
      "removing:  (456, 401, 64)\n",
      "--- 64.36741614341736 seconds ---\n",
      "Epoch [1/500], Loss: 27.7292, Accuracy: 78.76 %\n",
      "Testing Accuracy: 45.68 %\n",
      "Test Loss:  76.11509156227112\n",
      "Epoch [2/500], Loss: 27.8191, Accuracy: 78.46 %\n",
      "Testing Accuracy: 45.53 %\n",
      "Test Loss:  75.94068717956543\n",
      "Epoch [3/500], Loss: 27.0998, Accuracy: 79.31 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  76.50948464870453\n",
      "Epoch [4/500], Loss: 26.7406, Accuracy: 79.63 %\n",
      "Testing Accuracy: 45.40 %\n",
      "Test Loss:  77.8140252828598\n",
      "Epoch [5/500], Loss: 27.7560, Accuracy: 78.51 %\n",
      "Testing Accuracy: 46.04 %\n",
      "Test Loss:  75.51679730415344\n",
      "Epoch [6/500], Loss: 27.0415, Accuracy: 79.42 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  77.63689184188843\n",
      "Epoch [7/500], Loss: 27.0145, Accuracy: 79.45 %\n",
      "Testing Accuracy: 45.86 %\n",
      "Test Loss:  77.44279217720032\n",
      "Epoch [8/500], Loss: 26.2782, Accuracy: 80.11 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  79.94376146793365\n",
      "Epoch [9/500], Loss: 26.5424, Accuracy: 79.59 %\n",
      "Testing Accuracy: 45.67 %\n",
      "Test Loss:  76.74320805072784\n",
      "Epoch [10/500], Loss: 26.8439, Accuracy: 79.33 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  77.2433488368988\n",
      "Epoch [11/500], Loss: 28.6226, Accuracy: 77.69 %\n",
      "Testing Accuracy: 45.87 %\n",
      "Test Loss:  76.1540687084198\n",
      "Epoch [12/500], Loss: 27.5743, Accuracy: 78.90 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  77.6596326828003\n",
      "Epoch [13/500], Loss: 27.2309, Accuracy: 79.09 %\n",
      "Testing Accuracy: 45.41 %\n",
      "Test Loss:  76.45183145999908\n",
      "Epoch [14/500], Loss: 26.3306, Accuracy: 80.10 %\n",
      "Testing Accuracy: 45.70 %\n",
      "Test Loss:  75.97227787971497\n",
      "Epoch [15/500], Loss: 26.5670, Accuracy: 79.53 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  79.15271294116974\n",
      "Epoch [16/500], Loss: 25.7171, Accuracy: 80.50 %\n",
      "Testing Accuracy: 45.16 %\n",
      "Test Loss:  78.57361721992493\n",
      "Epoch [17/500], Loss: 26.5845, Accuracy: 79.57 %\n",
      "Testing Accuracy: 45.18 %\n",
      "Test Loss:  78.2901223897934\n",
      "Epoch [18/500], Loss: 26.2670, Accuracy: 80.02 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  77.35721325874329\n",
      "Epoch [19/500], Loss: 25.8741, Accuracy: 80.21 %\n",
      "Testing Accuracy: 45.84 %\n",
      "Test Loss:  75.48220360279083\n",
      "Epoch [20/500], Loss: 25.8724, Accuracy: 80.30 %\n",
      "Testing Accuracy: 44.92 %\n",
      "Test Loss:  78.11662459373474\n",
      "Epoch [21/500], Loss: 26.9881, Accuracy: 79.44 %\n",
      "Testing Accuracy: 46.01 %\n",
      "Test Loss:  76.81544172763824\n",
      "Epoch [22/500], Loss: 26.5738, Accuracy: 79.71 %\n",
      "Testing Accuracy: 45.33 %\n",
      "Test Loss:  75.786789894104\n",
      "Epoch [23/500], Loss: 25.8203, Accuracy: 80.35 %\n",
      "Testing Accuracy: 45.84 %\n",
      "Test Loss:  77.19365358352661\n",
      "Epoch [24/500], Loss: 25.3128, Accuracy: 80.84 %\n",
      "Testing Accuracy: 45.14 %\n",
      "Test Loss:  80.67324721813202\n",
      "Epoch [25/500], Loss: 25.3214, Accuracy: 80.86 %\n",
      "Testing Accuracy: 46.16 %\n",
      "Test Loss:  76.88742792606354\n",
      "Epoch [26/500], Loss: 25.5185, Accuracy: 80.78 %\n",
      "Testing Accuracy: 45.95 %\n",
      "Test Loss:  77.2452152967453\n",
      "Epoch [27/500], Loss: 25.2377, Accuracy: 80.86 %\n",
      "Testing Accuracy: 46.24 %\n",
      "Test Loss:  78.52226972579956\n",
      "Epoch [28/500], Loss: 25.9090, Accuracy: 80.17 %\n",
      "Testing Accuracy: 45.57 %\n",
      "Test Loss:  78.17001450061798\n",
      "Epoch [29/500], Loss: 25.7434, Accuracy: 80.32 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  76.57555174827576\n",
      "Epoch [30/500], Loss: 25.9557, Accuracy: 80.14 %\n",
      "Testing Accuracy: 45.83 %\n",
      "Test Loss:  78.40754556655884\n",
      "Epoch [31/500], Loss: 25.7580, Accuracy: 80.30 %\n",
      "Testing Accuracy: 45.21 %\n",
      "Test Loss:  79.27469837665558\n",
      "Epoch [32/500], Loss: 25.2822, Accuracy: 80.79 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  77.73215317726135\n",
      "Epoch [33/500], Loss: 25.2866, Accuracy: 80.74 %\n",
      "Testing Accuracy: 45.65 %\n",
      "Test Loss:  76.2370433807373\n",
      "Epoch [34/500], Loss: 24.6823, Accuracy: 81.54 %\n",
      "Testing Accuracy: 45.51 %\n",
      "Test Loss:  77.40772104263306\n",
      "Epoch [35/500], Loss: 24.5156, Accuracy: 81.55 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  78.4644023180008\n",
      "Epoch [36/500], Loss: 25.1061, Accuracy: 80.80 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  80.48628902435303\n",
      "Epoch [37/500], Loss: 24.9115, Accuracy: 81.13 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  77.4860919713974\n",
      "Epoch [38/500], Loss: 24.6872, Accuracy: 81.14 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  77.1768046617508\n",
      "Epoch [39/500], Loss: 24.1288, Accuracy: 81.97 %\n",
      "Testing Accuracy: 45.42 %\n",
      "Test Loss:  80.19176745414734\n",
      "Testing Accuracy: 45.42 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        950272   \n",
      "Net/Dropout[dropout]/onnx::Relu   928      \n",
      "Net/Linear[fc2]/onnx::Gemm        5104     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "956,304 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294]\n",
      "pruning hidden size:  464\n",
      "with hidden layer:  464\n",
      "removing:  (251, 157, 7)\n",
      "--- 63.509101152420044 seconds ---\n",
      "Epoch [1/500], Loss: 24.1509, Accuracy: 81.91 %\n",
      "Testing Accuracy: 45.80 %\n",
      "Test Loss:  78.18067502975464\n",
      "Epoch [2/500], Loss: 24.3634, Accuracy: 81.64 %\n",
      "Testing Accuracy: 45.52 %\n",
      "Test Loss:  78.69488275051117\n",
      "Epoch [3/500], Loss: 25.1019, Accuracy: 80.99 %\n",
      "Testing Accuracy: 45.59 %\n",
      "Test Loss:  78.3473664522171\n",
      "Epoch [4/500], Loss: 25.7008, Accuracy: 80.21 %\n",
      "Testing Accuracy: 45.52 %\n",
      "Test Loss:  78.44246649742126\n",
      "Epoch [5/500], Loss: 24.9577, Accuracy: 80.91 %\n",
      "Testing Accuracy: 45.85 %\n",
      "Test Loss:  77.95500075817108\n",
      "Epoch [6/500], Loss: 25.4530, Accuracy: 80.66 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  82.05271661281586\n",
      "Epoch [7/500], Loss: 27.1869, Accuracy: 78.78 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  80.77942156791687\n",
      "Epoch [8/500], Loss: 24.3534, Accuracy: 81.67 %\n",
      "Testing Accuracy: 45.58 %\n",
      "Test Loss:  79.38462710380554\n",
      "Epoch [9/500], Loss: 24.5514, Accuracy: 81.27 %\n",
      "Testing Accuracy: 45.96 %\n",
      "Test Loss:  77.83428955078125\n",
      "Epoch [10/500], Loss: 24.8663, Accuracy: 81.07 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  77.28971028327942\n",
      "Epoch [11/500], Loss: 24.0642, Accuracy: 81.92 %\n",
      "Testing Accuracy: 45.40 %\n",
      "Test Loss:  79.12506866455078\n",
      "Epoch [12/500], Loss: 24.3350, Accuracy: 81.53 %\n",
      "Testing Accuracy: 46.66 %\n",
      "Test Loss:  77.37059569358826\n",
      "Epoch [13/500], Loss: 24.1103, Accuracy: 81.83 %\n",
      "Testing Accuracy: 45.22 %\n",
      "Test Loss:  79.04666805267334\n",
      "Epoch [14/500], Loss: 25.0479, Accuracy: 80.90 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  79.6085547208786\n",
      "Epoch [15/500], Loss: 24.7993, Accuracy: 81.06 %\n",
      "Testing Accuracy: 44.63 %\n",
      "Test Loss:  80.78296136856079\n",
      "Epoch [16/500], Loss: 24.4067, Accuracy: 81.40 %\n",
      "Testing Accuracy: 45.70 %\n",
      "Test Loss:  79.22298681735992\n",
      "Epoch [17/500], Loss: 23.7979, Accuracy: 81.99 %\n",
      "Testing Accuracy: 46.21 %\n",
      "Test Loss:  77.88251996040344\n",
      "Epoch [18/500], Loss: 24.5885, Accuracy: 81.24 %\n",
      "Testing Accuracy: 45.93 %\n",
      "Test Loss:  77.96333241462708\n",
      "Epoch [19/500], Loss: 23.8666, Accuracy: 81.87 %\n",
      "Testing Accuracy: 45.77 %\n",
      "Test Loss:  80.0727903842926\n",
      "Epoch [20/500], Loss: 24.0593, Accuracy: 81.63 %\n",
      "Testing Accuracy: 44.70 %\n",
      "Test Loss:  81.68324661254883\n",
      "Epoch [21/500], Loss: 23.9886, Accuracy: 81.75 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  78.71990883350372\n",
      "Epoch [22/500], Loss: 24.2650, Accuracy: 81.57 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  79.75026047229767\n",
      "Epoch [23/500], Loss: 25.0205, Accuracy: 80.59 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  78.80928862094879\n",
      "Epoch [24/500], Loss: 24.0267, Accuracy: 81.87 %\n",
      "Testing Accuracy: 45.24 %\n",
      "Test Loss:  78.42898607254028\n",
      "Epoch [25/500], Loss: 23.4059, Accuracy: 82.55 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  81.09507501125336\n",
      "Epoch [26/500], Loss: 24.0439, Accuracy: 81.75 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  81.01328122615814\n",
      "Epoch [27/500], Loss: 23.7391, Accuracy: 81.85 %\n",
      "Testing Accuracy: 45.59 %\n",
      "Test Loss:  80.88242304325104\n",
      "Epoch [28/500], Loss: 23.9797, Accuracy: 81.68 %\n",
      "Testing Accuracy: 45.73 %\n",
      "Test Loss:  79.1588888168335\n",
      "Epoch [29/500], Loss: 24.3471, Accuracy: 81.41 %\n",
      "Testing Accuracy: 45.94 %\n",
      "Test Loss:  78.97197461128235\n",
      "Epoch [30/500], Loss: 24.2785, Accuracy: 81.35 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  80.54357099533081\n",
      "Testing Accuracy: 45.30 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        944128   \n",
      "Net/Dropout[dropout]/onnx::Relu   922      \n",
      "Net/Linear[fc2]/onnx::Gemm        5071     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "950,121 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068]\n",
      "pruning hidden size:  461\n",
      "with hidden layer:  461\n",
      "removing:  (241, 439, 296)\n",
      "--- 64.29516792297363 seconds ---\n",
      "Epoch [1/500], Loss: 24.2375, Accuracy: 81.57 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  79.59173476696014\n",
      "Epoch [2/500], Loss: 23.5329, Accuracy: 82.28 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  78.99910962581635\n",
      "Epoch [3/500], Loss: 23.5109, Accuracy: 82.07 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  80.90085804462433\n",
      "Epoch [4/500], Loss: 23.9707, Accuracy: 81.81 %\n",
      "Testing Accuracy: 44.37 %\n",
      "Test Loss:  82.39482128620148\n",
      "Epoch [5/500], Loss: 24.3261, Accuracy: 81.69 %\n",
      "Testing Accuracy: 45.45 %\n",
      "Test Loss:  82.3465164899826\n",
      "Epoch [6/500], Loss: 24.3098, Accuracy: 81.46 %\n",
      "Testing Accuracy: 46.20 %\n",
      "Test Loss:  79.65338551998138\n",
      "Epoch [7/500], Loss: 24.7409, Accuracy: 81.02 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  86.85403311252594\n",
      "Epoch [8/500], Loss: 24.4111, Accuracy: 81.30 %\n",
      "Testing Accuracy: 45.25 %\n",
      "Test Loss:  77.75150620937347\n",
      "Epoch [9/500], Loss: 23.2411, Accuracy: 82.41 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  79.0674501657486\n",
      "Epoch [10/500], Loss: 23.7712, Accuracy: 82.17 %\n",
      "Testing Accuracy: 45.08 %\n",
      "Test Loss:  79.62866485118866\n",
      "Epoch [11/500], Loss: 23.1665, Accuracy: 82.64 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  79.18204700946808\n",
      "Epoch [12/500], Loss: 23.4135, Accuracy: 82.41 %\n",
      "Testing Accuracy: 45.41 %\n",
      "Test Loss:  80.60146272182465\n",
      "Epoch [13/500], Loss: 25.3914, Accuracy: 80.43 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  79.70568287372589\n",
      "Epoch [14/500], Loss: 24.0780, Accuracy: 81.76 %\n",
      "Testing Accuracy: 43.65 %\n",
      "Test Loss:  84.16810512542725\n",
      "Epoch [15/500], Loss: 24.4980, Accuracy: 81.39 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  84.9446142911911\n",
      "Epoch [16/500], Loss: 24.0366, Accuracy: 81.61 %\n",
      "Testing Accuracy: 44.99 %\n",
      "Test Loss:  79.2095113992691\n",
      "Epoch [17/500], Loss: 22.8706, Accuracy: 82.56 %\n",
      "Testing Accuracy: 45.53 %\n",
      "Test Loss:  79.72957670688629\n",
      "Epoch [18/500], Loss: 22.1501, Accuracy: 83.54 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  79.2019819021225\n",
      "Epoch [19/500], Loss: 22.9332, Accuracy: 82.62 %\n",
      "Testing Accuracy: 46.31 %\n",
      "Test Loss:  80.08618998527527\n",
      "Epoch [20/500], Loss: 22.6025, Accuracy: 83.04 %\n",
      "Testing Accuracy: 45.83 %\n",
      "Test Loss:  80.98159039020538\n",
      "Epoch [21/500], Loss: 22.5284, Accuracy: 83.07 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  81.24080300331116\n",
      "Epoch [22/500], Loss: 22.2153, Accuracy: 83.39 %\n",
      "Testing Accuracy: 46.18 %\n",
      "Test Loss:  79.99653196334839\n",
      "Epoch [23/500], Loss: 22.0828, Accuracy: 83.54 %\n",
      "Testing Accuracy: 44.23 %\n",
      "Test Loss:  80.95830309391022\n",
      "Epoch [24/500], Loss: 22.4599, Accuracy: 82.93 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  81.47922027111053\n",
      "Epoch [25/500], Loss: 22.3493, Accuracy: 83.04 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  81.31972825527191\n",
      "Epoch [26/500], Loss: 22.2916, Accuracy: 83.22 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  81.45496559143066\n",
      "Epoch [27/500], Loss: 22.2145, Accuracy: 83.10 %\n",
      "Testing Accuracy: 45.98 %\n",
      "Test Loss:  81.06819570064545\n",
      "Epoch [28/500], Loss: 21.9528, Accuracy: 83.48 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  81.76361882686615\n",
      "Testing Accuracy: 45.27 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        937984   \n",
      "Net/Dropout[dropout]/onnx::Relu   916      \n",
      "Net/Linear[fc2]/onnx::Gemm        5038     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "943,938 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835]\n",
      "pruning hidden size:  458\n",
      "with hidden layer:  458\n",
      "removing:  (422, 437, 295)\n",
      "--- 62.27610182762146 seconds ---\n",
      "Epoch [1/500], Loss: 21.5936, Accuracy: 84.07 %\n",
      "Testing Accuracy: 45.28 %\n",
      "Test Loss:  80.44570934772491\n",
      "Epoch [2/500], Loss: 22.7601, Accuracy: 82.86 %\n",
      "Testing Accuracy: 45.28 %\n",
      "Test Loss:  79.95042479038239\n",
      "Epoch [3/500], Loss: 22.2250, Accuracy: 83.48 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  79.65056025981903\n",
      "Epoch [4/500], Loss: 22.0117, Accuracy: 83.47 %\n",
      "Testing Accuracy: 45.47 %\n",
      "Test Loss:  80.72804045677185\n",
      "Epoch [5/500], Loss: 22.0086, Accuracy: 83.58 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  79.56457006931305\n",
      "Epoch [6/500], Loss: 22.7724, Accuracy: 82.95 %\n",
      "Testing Accuracy: 45.54 %\n",
      "Test Loss:  80.76715457439423\n",
      "Epoch [7/500], Loss: 22.3481, Accuracy: 82.96 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  82.99399495124817\n",
      "Epoch [8/500], Loss: 22.0243, Accuracy: 83.43 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  82.39171528816223\n",
      "Epoch [9/500], Loss: 21.6429, Accuracy: 83.84 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  82.58412659168243\n",
      "Epoch [10/500], Loss: 21.9819, Accuracy: 83.54 %\n",
      "Testing Accuracy: 44.40 %\n",
      "Test Loss:  82.36003303527832\n",
      "Epoch [11/500], Loss: 21.7621, Accuracy: 83.84 %\n",
      "Testing Accuracy: 44.42 %\n",
      "Test Loss:  82.88141298294067\n",
      "Epoch [12/500], Loss: 22.3339, Accuracy: 82.94 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  81.892138838768\n",
      "Epoch [13/500], Loss: 21.8668, Accuracy: 83.58 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  80.30575489997864\n",
      "Epoch [14/500], Loss: 21.7668, Accuracy: 83.61 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  79.46915817260742\n",
      "Epoch [15/500], Loss: 21.2614, Accuracy: 84.36 %\n",
      "Testing Accuracy: 45.91 %\n",
      "Test Loss:  80.49221885204315\n",
      "Epoch [16/500], Loss: 21.8131, Accuracy: 83.75 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  83.81536030769348\n",
      "Epoch [17/500], Loss: 22.4240, Accuracy: 83.10 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  81.8652606010437\n",
      "Epoch [18/500], Loss: 21.4424, Accuracy: 83.92 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  83.99917995929718\n",
      "Epoch [19/500], Loss: 20.7009, Accuracy: 84.63 %\n",
      "Testing Accuracy: 45.56 %\n",
      "Test Loss:  81.25609123706818\n",
      "Epoch [20/500], Loss: 21.9048, Accuracy: 83.64 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  83.30185854434967\n",
      "Epoch [21/500], Loss: 22.1151, Accuracy: 83.24 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  82.08598172664642\n",
      "Epoch [22/500], Loss: 21.0955, Accuracy: 84.24 %\n",
      "Testing Accuracy: 45.87 %\n",
      "Test Loss:  81.01091873645782\n",
      "Epoch [23/500], Loss: 21.3021, Accuracy: 84.05 %\n",
      "Testing Accuracy: 45.94 %\n",
      "Test Loss:  79.75152683258057\n",
      "Epoch [24/500], Loss: 20.9831, Accuracy: 84.31 %\n",
      "Testing Accuracy: 45.64 %\n",
      "Test Loss:  81.00808453559875\n",
      "Epoch [25/500], Loss: 20.8080, Accuracy: 84.55 %\n",
      "Testing Accuracy: 45.97 %\n",
      "Test Loss:  80.81261909008026\n",
      "Epoch [26/500], Loss: 20.7949, Accuracy: 84.53 %\n",
      "Testing Accuracy: 46.00 %\n",
      "Test Loss:  80.76665246486664\n",
      "Epoch [27/500], Loss: 20.6771, Accuracy: 84.50 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  81.17272746562958\n",
      "Epoch [28/500], Loss: 20.7648, Accuracy: 84.47 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  82.20761132240295\n",
      "Epoch [29/500], Loss: 20.2295, Accuracy: 85.07 %\n",
      "Testing Accuracy: 45.75 %\n",
      "Test Loss:  80.68634283542633\n",
      "Epoch [30/500], Loss: 21.7173, Accuracy: 83.65 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  80.63673877716064\n",
      "Epoch [31/500], Loss: 21.7098, Accuracy: 83.88 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  84.49999988079071\n",
      "Epoch [32/500], Loss: 21.3656, Accuracy: 83.87 %\n",
      "Testing Accuracy: 44.20 %\n",
      "Test Loss:  87.73028361797333\n",
      "Epoch [33/500], Loss: 21.2298, Accuracy: 84.13 %\n",
      "Testing Accuracy: 45.43 %\n",
      "Test Loss:  83.48161447048187\n",
      "Epoch [34/500], Loss: 20.8299, Accuracy: 84.65 %\n",
      "Testing Accuracy: 46.04 %\n",
      "Test Loss:  82.38508701324463\n",
      "Testing Accuracy: 46.04 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        931840   \n",
      "Net/Dropout[dropout]/onnx::Relu   910      \n",
      "Net/Linear[fc2]/onnx::Gemm        5005     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "937,755 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253]\n",
      "pruning hidden size:  455\n",
      "with hidden layer:  455\n",
      "removing:  (249, 225, 161)\n",
      "--- 60.88614749908447 seconds ---\n",
      "Epoch [1/500], Loss: 21.0024, Accuracy: 84.43 %\n",
      "Testing Accuracy: 45.56 %\n",
      "Test Loss:  81.40045773983002\n",
      "Epoch [2/500], Loss: 21.4621, Accuracy: 83.85 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  81.85149335861206\n",
      "Epoch [3/500], Loss: 21.6852, Accuracy: 83.64 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  82.69104266166687\n",
      "Epoch [4/500], Loss: 21.6019, Accuracy: 83.81 %\n",
      "Testing Accuracy: 46.01 %\n",
      "Test Loss:  82.82887065410614\n",
      "Epoch [5/500], Loss: 22.1899, Accuracy: 83.25 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  84.00675237178802\n",
      "Epoch [6/500], Loss: 22.2807, Accuracy: 83.30 %\n",
      "Testing Accuracy: 45.75 %\n",
      "Test Loss:  82.61316883563995\n",
      "Epoch [7/500], Loss: 21.7732, Accuracy: 83.50 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  81.46780264377594\n",
      "Epoch [8/500], Loss: 21.8892, Accuracy: 83.48 %\n",
      "Testing Accuracy: 45.28 %\n",
      "Test Loss:  81.25765943527222\n",
      "Epoch [9/500], Loss: 20.6789, Accuracy: 84.66 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  84.29941320419312\n",
      "Epoch [10/500], Loss: 21.2965, Accuracy: 84.12 %\n",
      "Testing Accuracy: 45.43 %\n",
      "Test Loss:  83.2413614988327\n",
      "Epoch [11/500], Loss: 21.9765, Accuracy: 83.38 %\n",
      "Testing Accuracy: 46.23 %\n",
      "Test Loss:  80.8557779788971\n",
      "Epoch [12/500], Loss: 20.9139, Accuracy: 84.43 %\n",
      "Testing Accuracy: 45.41 %\n",
      "Test Loss:  81.67519962787628\n",
      "Epoch [13/500], Loss: 21.2833, Accuracy: 83.77 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  81.30682790279388\n",
      "Epoch [14/500], Loss: 21.3427, Accuracy: 83.90 %\n",
      "Testing Accuracy: 45.65 %\n",
      "Test Loss:  81.49812698364258\n",
      "Epoch [15/500], Loss: 20.8649, Accuracy: 84.55 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  81.7598032951355\n",
      "Epoch [16/500], Loss: 20.7043, Accuracy: 84.47 %\n",
      "Testing Accuracy: 45.64 %\n",
      "Test Loss:  81.92119717597961\n",
      "Epoch [17/500], Loss: 20.6212, Accuracy: 84.72 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  83.43650543689728\n",
      "Epoch [18/500], Loss: 21.3110, Accuracy: 84.00 %\n",
      "Testing Accuracy: 45.58 %\n",
      "Test Loss:  83.02318918704987\n",
      "Epoch [19/500], Loss: 21.7687, Accuracy: 83.64 %\n",
      "Testing Accuracy: 45.84 %\n",
      "Test Loss:  82.09192836284637\n",
      "Epoch [20/500], Loss: 20.9126, Accuracy: 84.61 %\n",
      "Testing Accuracy: 46.04 %\n",
      "Test Loss:  82.22137033939362\n",
      "Epoch [21/500], Loss: 21.0025, Accuracy: 84.38 %\n",
      "Testing Accuracy: 45.37 %\n",
      "Test Loss:  83.12352454662323\n",
      "Epoch [22/500], Loss: 20.7280, Accuracy: 84.50 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  82.07531726360321\n",
      "Epoch [23/500], Loss: 20.6591, Accuracy: 84.57 %\n",
      "Testing Accuracy: 45.94 %\n",
      "Test Loss:  82.66293632984161\n",
      "Epoch [24/500], Loss: 20.3618, Accuracy: 84.97 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  81.87001383304596\n",
      "Epoch [25/500], Loss: 20.0687, Accuracy: 84.98 %\n",
      "Testing Accuracy: 45.77 %\n",
      "Test Loss:  82.95896100997925\n",
      "Epoch [26/500], Loss: 19.7213, Accuracy: 85.66 %\n",
      "Testing Accuracy: 45.81 %\n",
      "Test Loss:  82.6766459941864\n",
      "Epoch [27/500], Loss: 20.9155, Accuracy: 84.41 %\n",
      "Testing Accuracy: 44.67 %\n",
      "Test Loss:  83.64825904369354\n",
      "Epoch [28/500], Loss: 20.8858, Accuracy: 84.24 %\n",
      "Testing Accuracy: 45.88 %\n",
      "Test Loss:  83.29988408088684\n",
      "Epoch [29/500], Loss: 20.0551, Accuracy: 85.02 %\n",
      "Testing Accuracy: 44.29 %\n",
      "Test Loss:  84.22108483314514\n",
      "Epoch [30/500], Loss: 19.9836, Accuracy: 85.02 %\n",
      "Testing Accuracy: 45.97 %\n",
      "Test Loss:  81.94359183311462\n",
      "Epoch [31/500], Loss: 20.1254, Accuracy: 85.06 %\n",
      "Testing Accuracy: 45.42 %\n",
      "Test Loss:  83.34396171569824\n",
      "Testing Accuracy: 45.42 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        925696   \n",
      "Net/Dropout[dropout]/onnx::Relu   904      \n",
      "Net/Linear[fc2]/onnx::Gemm        4972     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "931,572 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294]\n",
      "pruning hidden size:  452\n",
      "with hidden layer:  452\n",
      "removing:  (386, 321, 227)\n",
      "--- 61.05142092704773 seconds ---\n",
      "Epoch [1/500], Loss: 20.2288, Accuracy: 85.04 %\n",
      "Testing Accuracy: 45.52 %\n",
      "Test Loss:  82.55792534351349\n",
      "Epoch [2/500], Loss: 20.4181, Accuracy: 84.81 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  81.8089828491211\n",
      "Epoch [3/500], Loss: 20.7136, Accuracy: 84.51 %\n",
      "Testing Accuracy: 45.84 %\n",
      "Test Loss:  82.89107966423035\n",
      "Epoch [4/500], Loss: 20.0508, Accuracy: 84.91 %\n",
      "Testing Accuracy: 45.24 %\n",
      "Test Loss:  84.52357935905457\n",
      "Epoch [5/500], Loss: 20.0594, Accuracy: 85.00 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  84.55335438251495\n",
      "Epoch [6/500], Loss: 20.1816, Accuracy: 85.03 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  83.23488128185272\n",
      "Epoch [7/500], Loss: 20.1331, Accuracy: 84.92 %\n",
      "Testing Accuracy: 45.94 %\n",
      "Test Loss:  83.29231989383698\n",
      "Epoch [8/500], Loss: 20.7874, Accuracy: 84.41 %\n",
      "Testing Accuracy: 45.91 %\n",
      "Test Loss:  82.75912606716156\n",
      "Epoch [9/500], Loss: 20.2647, Accuracy: 84.82 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  82.95054423809052\n",
      "Epoch [10/500], Loss: 20.4173, Accuracy: 84.75 %\n",
      "Testing Accuracy: 45.63 %\n",
      "Test Loss:  82.89569127559662\n",
      "Epoch [11/500], Loss: 20.2593, Accuracy: 85.13 %\n",
      "Testing Accuracy: 45.52 %\n",
      "Test Loss:  84.58348524570465\n",
      "Epoch [12/500], Loss: 19.1604, Accuracy: 85.88 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  83.07533669471741\n",
      "Epoch [13/500], Loss: 20.0762, Accuracy: 85.02 %\n",
      "Testing Accuracy: 44.59 %\n",
      "Test Loss:  84.79873859882355\n",
      "Epoch [14/500], Loss: 19.5605, Accuracy: 85.58 %\n",
      "Testing Accuracy: 45.89 %\n",
      "Test Loss:  83.36145579814911\n",
      "Epoch [15/500], Loss: 19.6117, Accuracy: 85.41 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  82.78297805786133\n",
      "Epoch [16/500], Loss: 19.9259, Accuracy: 85.31 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  83.59107434749603\n",
      "Epoch [17/500], Loss: 20.0826, Accuracy: 85.01 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  83.6509598493576\n",
      "Epoch [18/500], Loss: 20.0063, Accuracy: 85.10 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  83.53522109985352\n",
      "Epoch [19/500], Loss: 19.9797, Accuracy: 85.01 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  83.7473258972168\n",
      "Epoch [20/500], Loss: 18.8994, Accuracy: 86.24 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  82.40223562717438\n",
      "Epoch [21/500], Loss: 19.8940, Accuracy: 84.96 %\n",
      "Testing Accuracy: 45.91 %\n",
      "Test Loss:  82.6790109872818\n",
      "Epoch [22/500], Loss: 19.1870, Accuracy: 85.89 %\n",
      "Testing Accuracy: 45.94 %\n",
      "Test Loss:  82.62717926502228\n",
      "Testing Accuracy: 45.94 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        919552   \n",
      "Net/Dropout[dropout]/onnx::Relu   898      \n",
      "Net/Linear[fc2]/onnx::Gemm        4939     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "925,389 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941]\n",
      "pruning hidden size:  449\n",
      "with hidden layer:  449\n",
      "removing:  (353, 283, 168)\n",
      "--- 59.985373735427856 seconds ---\n",
      "Epoch [1/500], Loss: 18.4466, Accuracy: 86.63 %\n",
      "Testing Accuracy: 45.25 %\n",
      "Test Loss:  83.10355067253113\n",
      "Epoch [2/500], Loss: 19.5169, Accuracy: 85.67 %\n",
      "Testing Accuracy: 45.68 %\n",
      "Test Loss:  83.91746413707733\n",
      "Epoch [3/500], Loss: 19.3533, Accuracy: 85.71 %\n",
      "Testing Accuracy: 45.58 %\n",
      "Test Loss:  84.49693894386292\n",
      "Epoch [4/500], Loss: 19.8651, Accuracy: 85.20 %\n",
      "Testing Accuracy: 44.85 %\n",
      "Test Loss:  83.05720567703247\n",
      "Epoch [5/500], Loss: 20.0617, Accuracy: 84.98 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  84.95081663131714\n",
      "Epoch [6/500], Loss: 20.1157, Accuracy: 85.02 %\n",
      "Testing Accuracy: 46.04 %\n",
      "Test Loss:  83.09404850006104\n",
      "Epoch [7/500], Loss: 19.3530, Accuracy: 85.53 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  84.64062666893005\n",
      "Epoch [8/500], Loss: 18.7711, Accuracy: 86.39 %\n",
      "Testing Accuracy: 45.48 %\n",
      "Test Loss:  83.47990930080414\n",
      "Epoch [9/500], Loss: 19.8464, Accuracy: 85.33 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  83.45838725566864\n",
      "Epoch [10/500], Loss: 19.3848, Accuracy: 85.72 %\n",
      "Testing Accuracy: 45.00 %\n",
      "Test Loss:  84.72946858406067\n",
      "Epoch [11/500], Loss: 19.5536, Accuracy: 85.38 %\n",
      "Testing Accuracy: 44.70 %\n",
      "Test Loss:  86.21633112430573\n",
      "Epoch [12/500], Loss: 19.4397, Accuracy: 85.58 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  83.60150420665741\n",
      "Epoch [13/500], Loss: 19.3209, Accuracy: 85.73 %\n",
      "Testing Accuracy: 45.57 %\n",
      "Test Loss:  83.40961575508118\n",
      "Epoch [14/500], Loss: 19.6016, Accuracy: 85.37 %\n",
      "Testing Accuracy: 44.30 %\n",
      "Test Loss:  85.05307817459106\n",
      "Epoch [15/500], Loss: 19.4907, Accuracy: 85.60 %\n",
      "Testing Accuracy: 45.22 %\n",
      "Test Loss:  83.41923201084137\n",
      "Epoch [16/500], Loss: 18.9145, Accuracy: 85.80 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  83.8087956905365\n",
      "Epoch [17/500], Loss: 19.5697, Accuracy: 85.45 %\n",
      "Testing Accuracy: 44.04 %\n",
      "Test Loss:  87.33048462867737\n",
      "Epoch [18/500], Loss: 19.6107, Accuracy: 85.26 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  84.44936764240265\n",
      "Epoch [19/500], Loss: 19.8449, Accuracy: 85.09 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  86.28066837787628\n",
      "Epoch [20/500], Loss: 18.4123, Accuracy: 86.44 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  84.5484094619751\n",
      "Epoch [21/500], Loss: 19.4908, Accuracy: 85.57 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  84.21210837364197\n",
      "Epoch [22/500], Loss: 18.8429, Accuracy: 86.12 %\n",
      "Testing Accuracy: 45.70 %\n",
      "Test Loss:  82.39017927646637\n",
      "Epoch [23/500], Loss: 18.2451, Accuracy: 86.58 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  85.85729134082794\n",
      "Epoch [24/500], Loss: 18.4611, Accuracy: 86.61 %\n",
      "Testing Accuracy: 45.85 %\n",
      "Test Loss:  85.79101037979126\n",
      "Epoch [25/500], Loss: 18.6681, Accuracy: 86.36 %\n",
      "Testing Accuracy: 45.18 %\n",
      "Test Loss:  85.99053525924683\n",
      "Epoch [26/500], Loss: 18.1711, Accuracy: 86.71 %\n",
      "Testing Accuracy: 45.54 %\n",
      "Test Loss:  84.3984557390213\n",
      "Epoch [27/500], Loss: 18.5136, Accuracy: 86.42 %\n",
      "Testing Accuracy: 45.58 %\n",
      "Test Loss:  85.2597895860672\n",
      "Epoch [28/500], Loss: 18.8575, Accuracy: 86.00 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  84.42680668830872\n",
      "Epoch [29/500], Loss: 18.7825, Accuracy: 86.09 %\n",
      "Testing Accuracy: 45.31 %\n",
      "Test Loss:  83.23083400726318\n",
      "Epoch [30/500], Loss: 18.4523, Accuracy: 86.68 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  83.43112134933472\n",
      "Epoch [31/500], Loss: 18.5488, Accuracy: 86.42 %\n",
      "Testing Accuracy: 45.33 %\n",
      "Test Loss:  84.40017914772034\n",
      "Epoch [32/500], Loss: 18.3091, Accuracy: 86.44 %\n",
      "Testing Accuracy: 44.97 %\n",
      "Test Loss:  84.95615029335022\n",
      "Epoch [33/500], Loss: 18.4031, Accuracy: 86.52 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  84.88858938217163\n",
      "Epoch [34/500], Loss: 17.8622, Accuracy: 87.02 %\n",
      "Testing Accuracy: 45.51 %\n",
      "Test Loss:  84.47766101360321\n",
      "Epoch [35/500], Loss: 18.3361, Accuracy: 86.70 %\n",
      "Testing Accuracy: 44.17 %\n",
      "Test Loss:  90.04260671138763\n",
      "Epoch [36/500], Loss: 19.0586, Accuracy: 85.57 %\n",
      "Testing Accuracy: 46.00 %\n",
      "Test Loss:  84.7238974571228\n",
      "Epoch [37/500], Loss: 17.7898, Accuracy: 86.99 %\n",
      "Testing Accuracy: 45.26 %\n",
      "Test Loss:  85.85939490795135\n",
      "Epoch [38/500], Loss: 19.0462, Accuracy: 85.70 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  84.7020753622055\n",
      "Epoch [39/500], Loss: 18.8595, Accuracy: 85.82 %\n",
      "Testing Accuracy: 44.51 %\n",
      "Test Loss:  84.70812785625458\n",
      "Epoch [40/500], Loss: 17.8429, Accuracy: 87.11 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  84.31478583812714\n",
      "Epoch [41/500], Loss: 17.6809, Accuracy: 87.01 %\n",
      "Testing Accuracy: 45.55 %\n",
      "Test Loss:  85.5301319360733\n",
      "Epoch [42/500], Loss: 18.7390, Accuracy: 86.06 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  85.85757350921631\n",
      "Testing Accuracy: 45.11 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        913408   \n",
      "Net/Dropout[dropout]/onnx::Relu   892      \n",
      "Net/Linear[fc2]/onnx::Gemm        4906     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "919,206 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815]\n",
      "pruning hidden size:  446\n",
      "with hidden layer:  446\n",
      "removing:  (344, 317, 121)\n",
      "--- 59.11492395401001 seconds ---\n",
      "Epoch [1/500], Loss: 17.8297, Accuracy: 86.92 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  85.3398048877716\n",
      "Epoch [2/500], Loss: 17.4714, Accuracy: 87.31 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  87.72363710403442\n",
      "Epoch [3/500], Loss: 19.0114, Accuracy: 85.66 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  90.1449111700058\n",
      "Epoch [4/500], Loss: 19.5198, Accuracy: 85.22 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  86.02313661575317\n",
      "Epoch [5/500], Loss: 18.8879, Accuracy: 85.95 %\n",
      "Testing Accuracy: 45.51 %\n",
      "Test Loss:  85.38679790496826\n",
      "Epoch [6/500], Loss: 18.4351, Accuracy: 86.21 %\n",
      "Testing Accuracy: 45.24 %\n",
      "Test Loss:  85.9705274105072\n",
      "Epoch [7/500], Loss: 18.1176, Accuracy: 86.52 %\n",
      "Testing Accuracy: 44.99 %\n",
      "Test Loss:  84.35854589939117\n",
      "Epoch [8/500], Loss: 17.5684, Accuracy: 87.27 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  87.57227218151093\n",
      "Epoch [9/500], Loss: 17.7632, Accuracy: 86.77 %\n",
      "Testing Accuracy: 45.39 %\n",
      "Test Loss:  86.91049182415009\n",
      "Epoch [10/500], Loss: 18.3250, Accuracy: 86.46 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  89.56601822376251\n",
      "Epoch [11/500], Loss: 18.1804, Accuracy: 86.55 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  89.29574632644653\n",
      "Epoch [12/500], Loss: 19.2462, Accuracy: 85.70 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  84.3687973022461\n",
      "Epoch [13/500], Loss: 17.9015, Accuracy: 86.97 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  85.25675225257874\n",
      "Epoch [14/500], Loss: 18.1504, Accuracy: 86.79 %\n",
      "Testing Accuracy: 43.62 %\n",
      "Test Loss:  89.64012801647186\n",
      "Epoch [15/500], Loss: 18.1731, Accuracy: 86.46 %\n",
      "Testing Accuracy: 45.62 %\n",
      "Test Loss:  85.43349516391754\n",
      "Epoch [16/500], Loss: 17.9100, Accuracy: 86.90 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  85.40308392047882\n",
      "Epoch [17/500], Loss: 17.0554, Accuracy: 87.58 %\n",
      "Testing Accuracy: 45.54 %\n",
      "Test Loss:  85.28524482250214\n",
      "Epoch [18/500], Loss: 17.3502, Accuracy: 87.31 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  86.54743838310242\n",
      "Epoch [19/500], Loss: 17.7125, Accuracy: 86.90 %\n",
      "Testing Accuracy: 45.41 %\n",
      "Test Loss:  88.65924417972565\n",
      "Epoch [20/500], Loss: 17.8328, Accuracy: 86.91 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  86.16222989559174\n",
      "Epoch [21/500], Loss: 18.0466, Accuracy: 86.66 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  86.15548717975616\n",
      "Epoch [22/500], Loss: 18.4924, Accuracy: 86.38 %\n",
      "Testing Accuracy: 44.50 %\n",
      "Test Loss:  86.10423624515533\n",
      "Epoch [23/500], Loss: 17.7075, Accuracy: 87.00 %\n",
      "Testing Accuracy: 44.87 %\n",
      "Test Loss:  86.29441058635712\n",
      "Epoch [24/500], Loss: 17.6681, Accuracy: 87.01 %\n",
      "Testing Accuracy: 44.67 %\n",
      "Test Loss:  86.70689141750336\n",
      "Epoch [25/500], Loss: 18.2044, Accuracy: 86.47 %\n",
      "Testing Accuracy: 44.28 %\n",
      "Test Loss:  88.04406559467316\n",
      "Epoch [26/500], Loss: 18.3305, Accuracy: 86.39 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  86.0204690694809\n",
      "Epoch [27/500], Loss: 17.4605, Accuracy: 87.35 %\n",
      "Testing Accuracy: 44.87 %\n",
      "Test Loss:  85.19766569137573\n",
      "Testing Accuracy: 44.87 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        907264   \n",
      "Net/Dropout[dropout]/onnx::Relu   886      \n",
      "Net/Linear[fc2]/onnx::Gemm        4873     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "913,023 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906]\n",
      "pruning hidden size:  443\n",
      "with hidden layer:  443\n",
      "removing:  (184, 227, 31)\n",
      "--- 58.438151836395264 seconds ---\n",
      "Epoch [1/500], Loss: 16.8936, Accuracy: 87.74 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  87.05474066734314\n",
      "Epoch [2/500], Loss: 17.9808, Accuracy: 86.62 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  88.37275278568268\n",
      "Epoch [3/500], Loss: 17.1863, Accuracy: 87.67 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  86.84818303585052\n",
      "Epoch [4/500], Loss: 17.2756, Accuracy: 87.27 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  88.05426442623138\n",
      "Epoch [5/500], Loss: 18.5969, Accuracy: 86.16 %\n",
      "Testing Accuracy: 45.23 %\n",
      "Test Loss:  84.46228659152985\n",
      "Epoch [6/500], Loss: 17.0862, Accuracy: 87.49 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  89.53124785423279\n",
      "Epoch [7/500], Loss: 17.5405, Accuracy: 87.02 %\n",
      "Testing Accuracy: 45.71 %\n",
      "Test Loss:  89.45701396465302\n",
      "Epoch [8/500], Loss: 18.6968, Accuracy: 85.90 %\n",
      "Testing Accuracy: 45.42 %\n",
      "Test Loss:  85.79870009422302\n",
      "Epoch [9/500], Loss: 18.1484, Accuracy: 86.54 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  87.55555880069733\n",
      "Epoch [10/500], Loss: 17.5300, Accuracy: 87.16 %\n",
      "Testing Accuracy: 46.00 %\n",
      "Test Loss:  85.69608235359192\n",
      "Epoch [11/500], Loss: 17.0220, Accuracy: 87.59 %\n",
      "Testing Accuracy: 44.28 %\n",
      "Test Loss:  87.76506292819977\n",
      "Epoch [12/500], Loss: 16.9338, Accuracy: 87.72 %\n",
      "Testing Accuracy: 45.26 %\n",
      "Test Loss:  90.09684526920319\n",
      "Epoch [13/500], Loss: 17.7600, Accuracy: 86.97 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  86.70657527446747\n",
      "Epoch [14/500], Loss: 17.1016, Accuracy: 87.48 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  87.35133755207062\n",
      "Epoch [15/500], Loss: 17.5484, Accuracy: 87.06 %\n",
      "Testing Accuracy: 44.44 %\n",
      "Test Loss:  91.38000190258026\n",
      "Epoch [16/500], Loss: 17.3438, Accuracy: 87.39 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  89.00019156932831\n",
      "Epoch [17/500], Loss: 18.1934, Accuracy: 86.55 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  86.2529776096344\n",
      "Epoch [18/500], Loss: 18.1372, Accuracy: 86.43 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  85.96258735656738\n",
      "Epoch [19/500], Loss: 16.0690, Accuracy: 88.50 %\n",
      "Testing Accuracy: 45.25 %\n",
      "Test Loss:  88.43302595615387\n",
      "Epoch [20/500], Loss: 17.1259, Accuracy: 87.40 %\n",
      "Testing Accuracy: 45.64 %\n",
      "Test Loss:  87.48773074150085\n",
      "Epoch [21/500], Loss: 16.5635, Accuracy: 87.99 %\n",
      "Testing Accuracy: 45.61 %\n",
      "Test Loss:  85.88345682621002\n",
      "Epoch [22/500], Loss: 16.3454, Accuracy: 88.17 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  87.51485002040863\n",
      "Epoch [23/500], Loss: 16.3215, Accuracy: 88.21 %\n",
      "Testing Accuracy: 44.97 %\n",
      "Test Loss:  85.53706276416779\n",
      "Epoch [24/500], Loss: 15.7993, Accuracy: 88.76 %\n",
      "Testing Accuracy: 45.28 %\n",
      "Test Loss:  85.74522149562836\n",
      "Epoch [25/500], Loss: 16.2046, Accuracy: 88.35 %\n",
      "Testing Accuracy: 45.57 %\n",
      "Test Loss:  86.55497324466705\n",
      "Testing Accuracy: 45.57 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        901120   \n",
      "Net/Dropout[dropout]/onnx::Relu   880      \n",
      "Net/Linear[fc2]/onnx::Gemm        4840     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "906,840 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348]\n",
      "pruning hidden size:  440\n",
      "with hidden layer:  440\n",
      "removing:  (409, 402, 357)\n",
      "--- 57.7730438709259 seconds ---\n",
      "Epoch [1/500], Loss: 15.9720, Accuracy: 88.74 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  87.29618990421295\n",
      "Epoch [2/500], Loss: 16.8108, Accuracy: 87.81 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  86.36530816555023\n",
      "Epoch [3/500], Loss: 17.1695, Accuracy: 87.42 %\n",
      "Testing Accuracy: 44.84 %\n",
      "Test Loss:  89.08182442188263\n",
      "Epoch [4/500], Loss: 16.9456, Accuracy: 87.62 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  85.92183351516724\n",
      "Epoch [5/500], Loss: 16.7383, Accuracy: 87.85 %\n",
      "Testing Accuracy: 44.75 %\n",
      "Test Loss:  89.01217567920685\n",
      "Epoch [6/500], Loss: 17.0596, Accuracy: 87.43 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  89.22043132781982\n",
      "Epoch [7/500], Loss: 16.9977, Accuracy: 87.53 %\n",
      "Testing Accuracy: 45.18 %\n",
      "Test Loss:  89.14143025875092\n",
      "Epoch [8/500], Loss: 16.8079, Accuracy: 87.69 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  90.17054271697998\n",
      "Epoch [9/500], Loss: 17.2399, Accuracy: 87.28 %\n",
      "Testing Accuracy: 45.70 %\n",
      "Test Loss:  87.59472155570984\n",
      "Epoch [10/500], Loss: 16.4455, Accuracy: 88.08 %\n",
      "Testing Accuracy: 44.14 %\n",
      "Test Loss:  89.01811838150024\n",
      "Epoch [11/500], Loss: 16.1263, Accuracy: 88.31 %\n",
      "Testing Accuracy: 44.43 %\n",
      "Test Loss:  87.46821320056915\n",
      "Epoch [12/500], Loss: 16.9186, Accuracy: 87.69 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  86.28797543048859\n",
      "Epoch [13/500], Loss: 17.8729, Accuracy: 86.57 %\n",
      "Testing Accuracy: 44.22 %\n",
      "Test Loss:  91.5299152135849\n",
      "Epoch [14/500], Loss: 18.2237, Accuracy: 86.34 %\n",
      "Testing Accuracy: 44.14 %\n",
      "Test Loss:  90.19009923934937\n",
      "Epoch [15/500], Loss: 17.9583, Accuracy: 86.70 %\n",
      "Testing Accuracy: 45.25 %\n",
      "Test Loss:  86.75968194007874\n",
      "Epoch [16/500], Loss: 16.8527, Accuracy: 87.75 %\n",
      "Testing Accuracy: 44.97 %\n",
      "Test Loss:  88.5949866771698\n",
      "Epoch [17/500], Loss: 17.0981, Accuracy: 87.49 %\n",
      "Testing Accuracy: 45.75 %\n",
      "Test Loss:  87.48718535900116\n",
      "Epoch [18/500], Loss: 17.0957, Accuracy: 87.48 %\n",
      "Testing Accuracy: 44.81 %\n",
      "Test Loss:  87.29996109008789\n",
      "Epoch [19/500], Loss: 17.3898, Accuracy: 87.24 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  87.04715144634247\n",
      "Epoch [20/500], Loss: 17.2584, Accuracy: 87.39 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  87.83746469020844\n",
      "Epoch [21/500], Loss: 17.1975, Accuracy: 87.39 %\n",
      "Testing Accuracy: 45.90 %\n",
      "Test Loss:  88.4578868150711\n",
      "Epoch [22/500], Loss: 17.6493, Accuracy: 86.79 %\n",
      "Testing Accuracy: 44.69 %\n",
      "Test Loss:  87.31532907485962\n",
      "Epoch [23/500], Loss: 17.0110, Accuracy: 87.45 %\n",
      "Testing Accuracy: 45.89 %\n",
      "Test Loss:  89.14139127731323\n",
      "Epoch [24/500], Loss: 16.8850, Accuracy: 87.82 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  89.05608308315277\n",
      "Testing Accuracy: 44.88 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        894976   \n",
      "Net/Dropout[dropout]/onnx::Relu   874      \n",
      "Net/Linear[fc2]/onnx::Gemm        4807     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "900,657 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363]\n",
      "pruning hidden size:  437\n",
      "with hidden layer:  437\n",
      "removing:  (409, 405, 225)\n",
      "--- 57.11010670661926 seconds ---\n",
      "Epoch [1/500], Loss: 17.7257, Accuracy: 86.75 %\n",
      "Testing Accuracy: 45.23 %\n",
      "Test Loss:  87.54875242710114\n",
      "Epoch [2/500], Loss: 17.2737, Accuracy: 87.30 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  87.37603533267975\n",
      "Epoch [3/500], Loss: 16.4331, Accuracy: 88.08 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  88.1793841123581\n",
      "Epoch [4/500], Loss: 17.6202, Accuracy: 86.93 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  87.85346031188965\n",
      "Epoch [5/500], Loss: 18.4756, Accuracy: 86.12 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  89.32125949859619\n",
      "Epoch [6/500], Loss: 17.2852, Accuracy: 87.27 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  88.94685709476471\n",
      "Epoch [7/500], Loss: 16.6084, Accuracy: 87.98 %\n",
      "Testing Accuracy: 45.54 %\n",
      "Test Loss:  88.51335847377777\n",
      "Epoch [8/500], Loss: 16.7213, Accuracy: 87.86 %\n",
      "Testing Accuracy: 44.93 %\n",
      "Test Loss:  89.11236083507538\n",
      "Epoch [9/500], Loss: 17.7600, Accuracy: 86.83 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  87.36393964290619\n",
      "Epoch [10/500], Loss: 16.7982, Accuracy: 87.77 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  86.74383687973022\n",
      "Epoch [11/500], Loss: 16.5233, Accuracy: 88.14 %\n",
      "Testing Accuracy: 45.39 %\n",
      "Test Loss:  90.02100205421448\n",
      "Epoch [12/500], Loss: 16.4915, Accuracy: 88.12 %\n",
      "Testing Accuracy: 45.60 %\n",
      "Test Loss:  87.1071013212204\n",
      "Epoch [13/500], Loss: 16.5637, Accuracy: 87.83 %\n",
      "Testing Accuracy: 44.59 %\n",
      "Test Loss:  88.859858751297\n",
      "Epoch [14/500], Loss: 16.6266, Accuracy: 87.90 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  87.86778795719147\n",
      "Epoch [15/500], Loss: 16.6366, Accuracy: 87.88 %\n",
      "Testing Accuracy: 45.47 %\n",
      "Test Loss:  88.21599805355072\n",
      "Epoch [16/500], Loss: 16.2744, Accuracy: 88.25 %\n",
      "Testing Accuracy: 45.83 %\n",
      "Test Loss:  87.0426653623581\n",
      "Epoch [17/500], Loss: 16.1865, Accuracy: 88.48 %\n",
      "Testing Accuracy: 44.45 %\n",
      "Test Loss:  89.25070917606354\n",
      "Epoch [18/500], Loss: 16.1121, Accuracy: 88.47 %\n",
      "Testing Accuracy: 45.24 %\n",
      "Test Loss:  88.92824149131775\n",
      "Epoch [19/500], Loss: 16.0558, Accuracy: 88.51 %\n",
      "Testing Accuracy: 45.14 %\n",
      "Test Loss:  88.46473038196564\n",
      "Epoch [20/500], Loss: 16.3406, Accuracy: 88.19 %\n",
      "Testing Accuracy: 44.69 %\n",
      "Test Loss:  87.162766456604\n",
      "Epoch [21/500], Loss: 16.4772, Accuracy: 88.23 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  89.52586734294891\n",
      "Epoch [22/500], Loss: 15.9360, Accuracy: 88.48 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  88.94215393066406\n",
      "Epoch [23/500], Loss: 16.4703, Accuracy: 87.89 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  89.18252563476562\n",
      "Epoch [24/500], Loss: 16.2649, Accuracy: 88.21 %\n",
      "Testing Accuracy: 45.73 %\n",
      "Test Loss:  88.75299310684204\n",
      "Epoch [25/500], Loss: 16.1071, Accuracy: 88.22 %\n",
      "Testing Accuracy: 45.21 %\n",
      "Test Loss:  88.85803020000458\n",
      "Epoch [26/500], Loss: 16.3034, Accuracy: 88.06 %\n",
      "Testing Accuracy: 45.31 %\n",
      "Test Loss:  89.64084362983704\n",
      "Epoch [27/500], Loss: 16.5405, Accuracy: 87.76 %\n",
      "Testing Accuracy: 44.75 %\n",
      "Test Loss:  87.99573612213135\n",
      "Epoch [28/500], Loss: 15.5535, Accuracy: 88.79 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  91.14487373828888\n",
      "Epoch [29/500], Loss: 15.9512, Accuracy: 88.51 %\n",
      "Testing Accuracy: 45.08 %\n",
      "Test Loss:  91.39189517498016\n",
      "Epoch [30/500], Loss: 15.5941, Accuracy: 88.91 %\n",
      "Testing Accuracy: 45.22 %\n",
      "Test Loss:  88.39876735210419\n",
      "Testing Accuracy: 45.22 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        888832   \n",
      "Net/Dropout[dropout]/onnx::Relu   868      \n",
      "Net/Linear[fc2]/onnx::Gemm        4774     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "894,474 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127]\n",
      "pruning hidden size:  434\n",
      "with hidden layer:  434\n",
      "removing:  (300, 424, 85)\n",
      "--- 56.93152403831482 seconds ---\n",
      "Epoch [1/500], Loss: 15.5458, Accuracy: 88.83 %\n",
      "Testing Accuracy: 45.35 %\n",
      "Test Loss:  89.37997734546661\n",
      "Epoch [2/500], Loss: 15.9426, Accuracy: 88.53 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  90.47823929786682\n",
      "Epoch [3/500], Loss: 16.6399, Accuracy: 87.68 %\n",
      "Testing Accuracy: 44.52 %\n",
      "Test Loss:  89.56658899784088\n",
      "Epoch [4/500], Loss: 15.8418, Accuracy: 88.67 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  89.63911557197571\n",
      "Epoch [5/500], Loss: 15.9814, Accuracy: 88.45 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  87.67199313640594\n",
      "Epoch [6/500], Loss: 16.6850, Accuracy: 87.71 %\n",
      "Testing Accuracy: 45.26 %\n",
      "Test Loss:  89.1163101196289\n",
      "Epoch [7/500], Loss: 16.8004, Accuracy: 87.58 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  88.4568464756012\n",
      "Epoch [8/500], Loss: 15.9802, Accuracy: 88.46 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  88.44528436660767\n",
      "Epoch [9/500], Loss: 15.8284, Accuracy: 88.54 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  89.8109142780304\n",
      "Epoch [10/500], Loss: 15.1988, Accuracy: 89.19 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  87.58158230781555\n",
      "Epoch [11/500], Loss: 15.4047, Accuracy: 89.01 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  87.64392864704132\n",
      "Epoch [12/500], Loss: 15.6075, Accuracy: 88.90 %\n",
      "Testing Accuracy: 45.22 %\n",
      "Test Loss:  89.28023636341095\n",
      "Epoch [13/500], Loss: 15.9628, Accuracy: 88.47 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  90.18223178386688\n",
      "Epoch [14/500], Loss: 17.0560, Accuracy: 87.30 %\n",
      "Testing Accuracy: 45.65 %\n",
      "Test Loss:  87.44972670078278\n",
      "Epoch [15/500], Loss: 15.1832, Accuracy: 89.16 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  88.14581644535065\n",
      "Epoch [16/500], Loss: 14.9555, Accuracy: 89.31 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  87.63626754283905\n",
      "Epoch [17/500], Loss: 15.3375, Accuracy: 88.87 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  88.01997828483582\n",
      "Epoch [18/500], Loss: 15.6965, Accuracy: 88.47 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  91.30422520637512\n",
      "Epoch [19/500], Loss: 15.5425, Accuracy: 88.74 %\n",
      "Testing Accuracy: 45.65 %\n",
      "Test Loss:  88.09717547893524\n",
      "Epoch [20/500], Loss: 15.1916, Accuracy: 89.13 %\n",
      "Testing Accuracy: 45.33 %\n",
      "Test Loss:  88.30535781383514\n",
      "Epoch [21/500], Loss: 15.3848, Accuracy: 88.93 %\n",
      "Testing Accuracy: 45.35 %\n",
      "Test Loss:  88.71422481536865\n",
      "Epoch [22/500], Loss: 16.2054, Accuracy: 88.13 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  89.51928126811981\n",
      "Epoch [23/500], Loss: 15.3360, Accuracy: 89.12 %\n",
      "Testing Accuracy: 45.82 %\n",
      "Test Loss:  89.15188157558441\n",
      "Epoch [24/500], Loss: 15.3533, Accuracy: 88.96 %\n",
      "Testing Accuracy: 45.21 %\n",
      "Test Loss:  88.86719119548798\n",
      "Epoch [25/500], Loss: 15.7330, Accuracy: 88.54 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  90.73258924484253\n",
      "Epoch [26/500], Loss: 15.2346, Accuracy: 89.15 %\n",
      "Testing Accuracy: 44.68 %\n",
      "Test Loss:  90.74470067024231\n",
      "Epoch [27/500], Loss: 15.3986, Accuracy: 88.98 %\n",
      "Testing Accuracy: 45.43 %\n",
      "Test Loss:  89.30074524879456\n",
      "Epoch [28/500], Loss: 15.3481, Accuracy: 88.93 %\n",
      "Testing Accuracy: 44.93 %\n",
      "Test Loss:  88.44955587387085\n",
      "Epoch [29/500], Loss: 15.7025, Accuracy: 88.76 %\n",
      "Testing Accuracy: 45.21 %\n",
      "Test Loss:  88.72413682937622\n",
      "Epoch [30/500], Loss: 15.8846, Accuracy: 88.44 %\n",
      "Testing Accuracy: 44.14 %\n",
      "Test Loss:  90.68368971347809\n",
      "Epoch [31/500], Loss: 15.9649, Accuracy: 88.37 %\n",
      "Testing Accuracy: 45.21 %\n",
      "Test Loss:  89.86087274551392\n",
      "Epoch [32/500], Loss: 16.0274, Accuracy: 88.28 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  88.64344382286072\n",
      "Epoch [33/500], Loss: 15.8815, Accuracy: 88.43 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  88.73175251483917\n",
      "Epoch [34/500], Loss: 15.2570, Accuracy: 89.02 %\n",
      "Testing Accuracy: 45.40 %\n",
      "Test Loss:  89.62948215007782\n",
      "Testing Accuracy: 45.40 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        882688   \n",
      "Net/Dropout[dropout]/onnx::Relu   862      \n",
      "Net/Linear[fc2]/onnx::Gemm        4741     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "888,291 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466]\n",
      "pruning hidden size:  431\n",
      "with hidden layer:  431\n",
      "removing:  (274, 181, 368)\n",
      "--- 54.61352825164795 seconds ---\n",
      "Epoch [1/500], Loss: 15.1305, Accuracy: 89.18 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  89.50256502628326\n",
      "Epoch [2/500], Loss: 15.5707, Accuracy: 88.73 %\n",
      "Testing Accuracy: 43.98 %\n",
      "Test Loss:  92.02045047283173\n",
      "Epoch [3/500], Loss: 16.6080, Accuracy: 87.66 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  89.43762123584747\n",
      "Epoch [4/500], Loss: 16.0160, Accuracy: 88.42 %\n",
      "Testing Accuracy: 44.08 %\n",
      "Test Loss:  90.81351578235626\n",
      "Epoch [5/500], Loss: 15.8326, Accuracy: 88.45 %\n",
      "Testing Accuracy: 44.69 %\n",
      "Test Loss:  89.68945240974426\n",
      "Epoch [6/500], Loss: 15.7582, Accuracy: 88.85 %\n",
      "Testing Accuracy: 44.00 %\n",
      "Test Loss:  92.81389546394348\n",
      "Epoch [7/500], Loss: 16.5179, Accuracy: 87.85 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  91.08267652988434\n",
      "Epoch [8/500], Loss: 16.2488, Accuracy: 88.15 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  89.75116181373596\n",
      "Epoch [9/500], Loss: 15.3394, Accuracy: 88.94 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  90.17606270313263\n",
      "Epoch [10/500], Loss: 15.4933, Accuracy: 88.80 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  89.16074824333191\n",
      "Epoch [11/500], Loss: 15.8692, Accuracy: 88.56 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  89.88567817211151\n",
      "Epoch [12/500], Loss: 16.4448, Accuracy: 87.62 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  87.43253302574158\n",
      "Epoch [13/500], Loss: 15.9236, Accuracy: 88.39 %\n",
      "Testing Accuracy: 45.36 %\n",
      "Test Loss:  91.41287040710449\n",
      "Epoch [14/500], Loss: 16.4019, Accuracy: 87.88 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  91.98395776748657\n",
      "Epoch [15/500], Loss: 16.8321, Accuracy: 87.40 %\n",
      "Testing Accuracy: 45.08 %\n",
      "Test Loss:  90.45949101448059\n",
      "Epoch [16/500], Loss: 16.4745, Accuracy: 87.95 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  88.1150232553482\n",
      "Epoch [17/500], Loss: 15.5533, Accuracy: 88.87 %\n",
      "Testing Accuracy: 44.13 %\n",
      "Test Loss:  88.99983215332031\n",
      "Epoch [18/500], Loss: 17.2021, Accuracy: 87.14 %\n",
      "Testing Accuracy: 45.69 %\n",
      "Test Loss:  89.5248191356659\n",
      "Epoch [19/500], Loss: 15.4648, Accuracy: 88.87 %\n",
      "Testing Accuracy: 45.23 %\n",
      "Test Loss:  90.37313175201416\n",
      "Epoch [20/500], Loss: 15.8333, Accuracy: 88.51 %\n",
      "Testing Accuracy: 45.46 %\n",
      "Test Loss:  88.94635140895844\n",
      "Epoch [21/500], Loss: 16.3775, Accuracy: 87.98 %\n",
      "Testing Accuracy: 44.39 %\n",
      "Test Loss:  88.66165566444397\n",
      "Epoch [22/500], Loss: 15.8204, Accuracy: 88.53 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  90.12284660339355\n",
      "Epoch [23/500], Loss: 16.7658, Accuracy: 87.60 %\n",
      "Testing Accuracy: 45.28 %\n",
      "Test Loss:  90.16251754760742\n",
      "Epoch [24/500], Loss: 15.2554, Accuracy: 88.90 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  93.74904191493988\n",
      "Epoch [25/500], Loss: 15.6450, Accuracy: 88.60 %\n",
      "Testing Accuracy: 44.01 %\n",
      "Test Loss:  96.54811334609985\n",
      "Epoch [26/500], Loss: 15.9336, Accuracy: 88.39 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  89.56245517730713\n",
      "Epoch [27/500], Loss: 15.5058, Accuracy: 88.79 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  88.98283874988556\n",
      "Epoch [28/500], Loss: 15.6598, Accuracy: 88.53 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  92.53443217277527\n",
      "Epoch [29/500], Loss: 16.0045, Accuracy: 88.30 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  90.95894730091095\n",
      "Epoch [30/500], Loss: 15.7013, Accuracy: 88.54 %\n",
      "Testing Accuracy: 45.08 %\n",
      "Test Loss:  90.43770468235016\n",
      "Epoch [31/500], Loss: 15.7303, Accuracy: 88.57 %\n",
      "Testing Accuracy: 42.41 %\n",
      "Test Loss:  94.02622497081757\n",
      "Epoch [32/500], Loss: 16.2178, Accuracy: 88.07 %\n",
      "Testing Accuracy: 45.49 %\n",
      "Test Loss:  89.89475405216217\n",
      "Testing Accuracy: 45.49 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        876544   \n",
      "Net/Dropout[dropout]/onnx::Relu   856      \n",
      "Net/Linear[fc2]/onnx::Gemm        4708     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "882,108 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864]\n",
      "pruning hidden size:  428\n",
      "with hidden layer:  428\n",
      "removing:  (297, 262, 94)\n",
      "--- 54.78119683265686 seconds ---\n",
      "Epoch [1/500], Loss: 14.5058, Accuracy: 89.88 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  90.8975397348404\n",
      "Epoch [2/500], Loss: 15.9946, Accuracy: 88.24 %\n",
      "Testing Accuracy: 44.06 %\n",
      "Test Loss:  89.47514605522156\n",
      "Epoch [3/500], Loss: 15.6654, Accuracy: 88.41 %\n",
      "Testing Accuracy: 45.23 %\n",
      "Test Loss:  89.79830145835876\n",
      "Epoch [4/500], Loss: 14.7510, Accuracy: 89.38 %\n",
      "Testing Accuracy: 45.23 %\n",
      "Test Loss:  90.55914509296417\n",
      "Epoch [5/500], Loss: 15.3999, Accuracy: 88.94 %\n",
      "Testing Accuracy: 44.75 %\n",
      "Test Loss:  91.02065706253052\n",
      "Epoch [6/500], Loss: 15.7172, Accuracy: 88.51 %\n",
      "Testing Accuracy: 44.19 %\n",
      "Test Loss:  92.55670738220215\n",
      "Epoch [7/500], Loss: 14.9841, Accuracy: 89.19 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  89.67669117450714\n",
      "Epoch [8/500], Loss: 15.2893, Accuracy: 88.82 %\n",
      "Testing Accuracy: 45.26 %\n",
      "Test Loss:  89.88910782337189\n",
      "Epoch [9/500], Loss: 15.2034, Accuracy: 89.04 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  91.47338783740997\n",
      "Epoch [10/500], Loss: 15.4133, Accuracy: 88.75 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  92.29615902900696\n",
      "Epoch [11/500], Loss: 15.9235, Accuracy: 88.23 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  90.18021976947784\n",
      "Epoch [12/500], Loss: 16.2101, Accuracy: 87.83 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  90.11442983150482\n",
      "Epoch [13/500], Loss: 15.2156, Accuracy: 89.05 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  89.11479556560516\n",
      "Epoch [14/500], Loss: 15.3088, Accuracy: 88.96 %\n",
      "Testing Accuracy: 45.20 %\n",
      "Test Loss:  93.13093113899231\n",
      "Epoch [15/500], Loss: 14.4782, Accuracy: 89.65 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  91.47050750255585\n",
      "Epoch [16/500], Loss: 15.2685, Accuracy: 89.22 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  91.14032685756683\n",
      "Epoch [17/500], Loss: 15.1336, Accuracy: 89.12 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  94.49429941177368\n",
      "Epoch [18/500], Loss: 15.4430, Accuracy: 88.82 %\n",
      "Testing Accuracy: 45.47 %\n",
      "Test Loss:  89.24224984645844\n",
      "Epoch [19/500], Loss: 15.6481, Accuracy: 88.74 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  91.88021528720856\n",
      "Epoch [20/500], Loss: 15.8472, Accuracy: 88.29 %\n",
      "Testing Accuracy: 44.20 %\n",
      "Test Loss:  91.47815215587616\n",
      "Epoch [21/500], Loss: 15.1719, Accuracy: 89.06 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  89.88411283493042\n",
      "Epoch [22/500], Loss: 14.9414, Accuracy: 89.32 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  90.54112780094147\n",
      "Epoch [23/500], Loss: 14.4673, Accuracy: 89.78 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  91.23993265628815\n",
      "Epoch [24/500], Loss: 14.6835, Accuracy: 89.61 %\n",
      "Testing Accuracy: 44.95 %\n",
      "Test Loss:  90.75370252132416\n",
      "Epoch [25/500], Loss: 14.7865, Accuracy: 89.24 %\n",
      "Testing Accuracy: 46.12 %\n",
      "Test Loss:  91.59047341346741\n",
      "Epoch [26/500], Loss: 14.4036, Accuracy: 89.71 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  92.47552585601807\n",
      "Epoch [27/500], Loss: 14.1816, Accuracy: 89.92 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  89.81737232208252\n",
      "Epoch [28/500], Loss: 14.0170, Accuracy: 90.04 %\n",
      "Testing Accuracy: 45.37 %\n",
      "Test Loss:  90.31903338432312\n",
      "Epoch [29/500], Loss: 14.3207, Accuracy: 89.79 %\n",
      "Testing Accuracy: 45.40 %\n",
      "Test Loss:  89.72648191452026\n",
      "Epoch [30/500], Loss: 14.2537, Accuracy: 89.78 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  91.05966925621033\n",
      "Epoch [31/500], Loss: 15.2018, Accuracy: 88.91 %\n",
      "Testing Accuracy: 44.93 %\n",
      "Test Loss:  90.94020509719849\n",
      "Epoch [32/500], Loss: 14.3541, Accuracy: 89.66 %\n",
      "Testing Accuracy: 45.10 %\n",
      "Test Loss:  89.82396984100342\n",
      "Epoch [33/500], Loss: 14.2498, Accuracy: 89.83 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  89.6790429353714\n",
      "Testing Accuracy: 45.01 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        870400   \n",
      "Net/Dropout[dropout]/onnx::Relu   850      \n",
      "Net/Linear[fc2]/onnx::Gemm        4675     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "875,925 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025]\n",
      "pruning hidden size:  425\n",
      "with hidden layer:  425\n",
      "removing:  (406, 342, 214)\n",
      "--- 54.196927547454834 seconds ---\n",
      "Epoch [1/500], Loss: 14.3673, Accuracy: 89.87 %\n",
      "Testing Accuracy: 44.55 %\n",
      "Test Loss:  91.11673414707184\n",
      "Epoch [2/500], Loss: 14.8409, Accuracy: 89.41 %\n",
      "Testing Accuracy: 44.26 %\n",
      "Test Loss:  91.62174367904663\n",
      "Epoch [3/500], Loss: 15.5372, Accuracy: 88.65 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  90.61301493644714\n",
      "Epoch [4/500], Loss: 15.9306, Accuracy: 88.51 %\n",
      "Testing Accuracy: 45.42 %\n",
      "Test Loss:  92.96666979789734\n",
      "Epoch [5/500], Loss: 14.5809, Accuracy: 89.53 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  91.02267444133759\n",
      "Epoch [6/500], Loss: 14.9400, Accuracy: 89.35 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  96.7754602432251\n",
      "Epoch [7/500], Loss: 16.0352, Accuracy: 88.18 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  93.44771218299866\n",
      "Epoch [8/500], Loss: 15.2116, Accuracy: 88.91 %\n",
      "Testing Accuracy: 43.92 %\n",
      "Test Loss:  92.54643380641937\n",
      "Epoch [9/500], Loss: 15.6106, Accuracy: 88.41 %\n",
      "Testing Accuracy: 44.95 %\n",
      "Test Loss:  91.89155924320221\n",
      "Epoch [10/500], Loss: 14.7528, Accuracy: 89.37 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  96.93399453163147\n",
      "Epoch [11/500], Loss: 16.0089, Accuracy: 88.20 %\n",
      "Testing Accuracy: 45.27 %\n",
      "Test Loss:  91.93791604042053\n",
      "Epoch [12/500], Loss: 14.7579, Accuracy: 89.55 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  90.99949622154236\n",
      "Epoch [13/500], Loss: 14.3467, Accuracy: 89.80 %\n",
      "Testing Accuracy: 44.75 %\n",
      "Test Loss:  91.86392140388489\n",
      "Epoch [14/500], Loss: 14.2284, Accuracy: 89.84 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  92.31046092510223\n",
      "Epoch [15/500], Loss: 14.6140, Accuracy: 89.65 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  91.65952670574188\n",
      "Epoch [16/500], Loss: 15.0465, Accuracy: 89.10 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  93.1876026391983\n",
      "Epoch [17/500], Loss: 14.9298, Accuracy: 89.44 %\n",
      "Testing Accuracy: 45.38 %\n",
      "Test Loss:  91.90358662605286\n",
      "Epoch [18/500], Loss: 14.1830, Accuracy: 89.86 %\n",
      "Testing Accuracy: 45.40 %\n",
      "Test Loss:  91.57778906822205\n",
      "Epoch [19/500], Loss: 14.6580, Accuracy: 89.43 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  91.1987282037735\n",
      "Epoch [20/500], Loss: 14.9874, Accuracy: 89.19 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  91.59728157520294\n",
      "Epoch [21/500], Loss: 15.5954, Accuracy: 88.37 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  91.37692058086395\n",
      "Epoch [22/500], Loss: 14.8074, Accuracy: 89.22 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  91.30592930316925\n",
      "Epoch [23/500], Loss: 14.1431, Accuracy: 90.25 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  91.5883709192276\n",
      "Testing Accuracy: 44.89 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        864256   \n",
      "Net/Dropout[dropout]/onnx::Relu   844      \n",
      "Net/Linear[fc2]/onnx::Gemm        4642     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "869,742 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336]\n",
      "pruning hidden size:  422\n",
      "with hidden layer:  422\n",
      "removing:  (187, 166, 40)\n",
      "--- 54.37392711639404 seconds ---\n",
      "Epoch [1/500], Loss: 15.0921, Accuracy: 88.98 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  95.89526796340942\n",
      "Epoch [2/500], Loss: 14.9652, Accuracy: 89.18 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  91.77649104595184\n",
      "Epoch [3/500], Loss: 14.8364, Accuracy: 89.35 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  91.91877615451813\n",
      "Epoch [4/500], Loss: 14.1789, Accuracy: 89.79 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  91.43678641319275\n",
      "Epoch [5/500], Loss: 15.1571, Accuracy: 88.92 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  93.40550708770752\n",
      "Epoch [6/500], Loss: 14.6719, Accuracy: 89.20 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  91.54951643943787\n",
      "Epoch [7/500], Loss: 14.1979, Accuracy: 89.90 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  92.62879657745361\n",
      "Epoch [8/500], Loss: 14.2283, Accuracy: 89.75 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  91.56518948078156\n",
      "Epoch [9/500], Loss: 15.0206, Accuracy: 89.09 %\n",
      "Testing Accuracy: 44.69 %\n",
      "Test Loss:  91.7244461774826\n",
      "Epoch [10/500], Loss: 14.7180, Accuracy: 89.43 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  93.03211462497711\n",
      "Epoch [11/500], Loss: 14.2292, Accuracy: 89.91 %\n",
      "Testing Accuracy: 45.70 %\n",
      "Test Loss:  91.28562462329865\n",
      "Epoch [12/500], Loss: 14.3986, Accuracy: 89.75 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  91.18819606304169\n",
      "Epoch [13/500], Loss: 14.4572, Accuracy: 89.79 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  90.95873522758484\n",
      "Epoch [14/500], Loss: 14.9802, Accuracy: 89.10 %\n",
      "Testing Accuracy: 45.42 %\n",
      "Test Loss:  91.82923185825348\n",
      "Epoch [15/500], Loss: 14.4883, Accuracy: 89.70 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  90.90221035480499\n",
      "Epoch [16/500], Loss: 14.5787, Accuracy: 89.48 %\n",
      "Testing Accuracy: 44.63 %\n",
      "Test Loss:  93.99824452400208\n",
      "Epoch [17/500], Loss: 15.3263, Accuracy: 88.79 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  92.02518582344055\n",
      "Epoch [18/500], Loss: 15.2016, Accuracy: 89.04 %\n",
      "Testing Accuracy: 44.55 %\n",
      "Test Loss:  92.6161869764328\n",
      "Epoch [19/500], Loss: 14.4810, Accuracy: 89.73 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  92.99038052558899\n",
      "Epoch [20/500], Loss: 14.4745, Accuracy: 89.63 %\n",
      "Testing Accuracy: 44.63 %\n",
      "Test Loss:  92.49386322498322\n",
      "Epoch [21/500], Loss: 14.3123, Accuracy: 89.86 %\n",
      "Testing Accuracy: 44.95 %\n",
      "Test Loss:  93.39961338043213\n",
      "Epoch [22/500], Loss: 14.5821, Accuracy: 89.42 %\n",
      "Testing Accuracy: 45.50 %\n",
      "Test Loss:  91.66927063465118\n",
      "Epoch [23/500], Loss: 14.0578, Accuracy: 89.96 %\n",
      "Testing Accuracy: 44.70 %\n",
      "Test Loss:  91.25842273235321\n",
      "Epoch [24/500], Loss: 14.0605, Accuracy: 90.00 %\n",
      "Testing Accuracy: 44.76 %\n",
      "Test Loss:  91.17034828662872\n",
      "Epoch [25/500], Loss: 14.7818, Accuracy: 89.26 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  91.75347304344177\n",
      "Epoch [26/500], Loss: 14.2854, Accuracy: 89.86 %\n",
      "Testing Accuracy: 45.12 %\n",
      "Test Loss:  90.96070861816406\n",
      "Epoch [27/500], Loss: 14.2744, Accuracy: 89.71 %\n",
      "Testing Accuracy: 43.56 %\n",
      "Test Loss:  95.51307964324951\n",
      "Epoch [28/500], Loss: 15.3767, Accuracy: 88.72 %\n",
      "Testing Accuracy: 44.99 %\n",
      "Test Loss:  91.20861518383026\n",
      "Epoch [29/500], Loss: 15.4796, Accuracy: 88.73 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  90.35553324222565\n",
      "Epoch [30/500], Loss: 15.3972, Accuracy: 88.68 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  93.92546331882477\n",
      "Epoch [31/500], Loss: 14.9369, Accuracy: 89.00 %\n",
      "Testing Accuracy: 44.64 %\n",
      "Test Loss:  93.38197946548462\n",
      "Epoch [32/500], Loss: 15.3456, Accuracy: 88.85 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  92.64372289180756\n",
      "Epoch [33/500], Loss: 14.7321, Accuracy: 89.40 %\n",
      "Testing Accuracy: 45.00 %\n",
      "Test Loss:  91.94114291667938\n",
      "Epoch [34/500], Loss: 13.8308, Accuracy: 90.13 %\n",
      "Testing Accuracy: 45.48 %\n",
      "Test Loss:  92.05230188369751\n",
      "Epoch [35/500], Loss: 13.4959, Accuracy: 90.38 %\n",
      "Testing Accuracy: 44.48 %\n",
      "Test Loss:  92.05495369434357\n",
      "Epoch [36/500], Loss: 13.5307, Accuracy: 90.46 %\n",
      "Testing Accuracy: 44.75 %\n",
      "Test Loss:  90.08292925357819\n",
      "Epoch [37/500], Loss: 13.8361, Accuracy: 90.14 %\n",
      "Testing Accuracy: 45.79 %\n",
      "Test Loss:  93.14983785152435\n",
      "Epoch [38/500], Loss: 14.2227, Accuracy: 89.86 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  91.87037765979767\n",
      "Epoch [39/500], Loss: 14.4446, Accuracy: 89.78 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  90.50148689746857\n",
      "Epoch [40/500], Loss: 13.8093, Accuracy: 90.13 %\n",
      "Testing Accuracy: 45.24 %\n",
      "Test Loss:  92.58002471923828\n",
      "Epoch [41/500], Loss: 14.0415, Accuracy: 89.87 %\n",
      "Testing Accuracy: 43.70 %\n",
      "Test Loss:  97.68160772323608\n",
      "Epoch [42/500], Loss: 14.7040, Accuracy: 89.31 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  92.92409384250641\n",
      "Epoch [43/500], Loss: 14.9108, Accuracy: 89.16 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  90.8116865158081\n",
      "Epoch [44/500], Loss: 14.4689, Accuracy: 89.43 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  96.16598832607269\n",
      "Epoch [45/500], Loss: 14.4110, Accuracy: 89.62 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  92.69112503528595\n",
      "Epoch [46/500], Loss: 13.9447, Accuracy: 89.99 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  91.77806282043457\n",
      "Epoch [47/500], Loss: 13.8604, Accuracy: 90.14 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  93.53311479091644\n",
      "Epoch [48/500], Loss: 14.3980, Accuracy: 89.63 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  93.14347350597382\n",
      "Epoch [49/500], Loss: 14.5067, Accuracy: 89.32 %\n",
      "Testing Accuracy: 44.25 %\n",
      "Test Loss:  91.87280738353729\n",
      "Epoch [50/500], Loss: 14.1693, Accuracy: 89.72 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  92.1760368347168\n",
      "Epoch [51/500], Loss: 14.2610, Accuracy: 89.74 %\n",
      "Testing Accuracy: 44.94 %\n",
      "Test Loss:  93.1622782945633\n",
      "Epoch [52/500], Loss: 14.0374, Accuracy: 89.83 %\n",
      "Testing Accuracy: 44.97 %\n",
      "Test Loss:  92.3248336315155\n",
      "Epoch [53/500], Loss: 14.3456, Accuracy: 89.80 %\n",
      "Testing Accuracy: 44.24 %\n",
      "Test Loss:  94.38385391235352\n",
      "Epoch [54/500], Loss: 14.1897, Accuracy: 89.70 %\n",
      "Testing Accuracy: 44.76 %\n",
      "Test Loss:  93.86174976825714\n",
      "Epoch [55/500], Loss: 14.0541, Accuracy: 89.94 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  91.95018446445465\n",
      "Epoch [56/500], Loss: 14.1541, Accuracy: 89.97 %\n",
      "Testing Accuracy: 44.68 %\n",
      "Test Loss:  92.83435785770416\n",
      "Testing Accuracy: 44.68 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        858112   \n",
      "Net/Dropout[dropout]/onnx::Relu   838      \n",
      "Net/Linear[fc2]/onnx::Gemm        4609     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "863,559 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211]\n",
      "pruning hidden size:  419\n",
      "with hidden layer:  419\n",
      "removing:  (383, 239, 168)\n",
      "--- 51.969979763031006 seconds ---\n",
      "Epoch [1/500], Loss: 13.5195, Accuracy: 90.57 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  94.02362287044525\n",
      "Epoch [2/500], Loss: 14.0484, Accuracy: 89.90 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  93.11409401893616\n",
      "Epoch [3/500], Loss: 15.1909, Accuracy: 88.73 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  93.64603006839752\n",
      "Epoch [4/500], Loss: 14.4255, Accuracy: 89.48 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  94.10382413864136\n",
      "Epoch [5/500], Loss: 14.4503, Accuracy: 89.55 %\n",
      "Testing Accuracy: 44.33 %\n",
      "Test Loss:  94.87804198265076\n",
      "Epoch [6/500], Loss: 14.5138, Accuracy: 89.30 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  91.5919439792633\n",
      "Epoch [7/500], Loss: 15.1811, Accuracy: 88.93 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  91.75573790073395\n",
      "Epoch [8/500], Loss: 15.0488, Accuracy: 89.14 %\n",
      "Testing Accuracy: 45.43 %\n",
      "Test Loss:  92.57451617717743\n",
      "Epoch [9/500], Loss: 14.7493, Accuracy: 89.25 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  91.573686003685\n",
      "Epoch [10/500], Loss: 14.3469, Accuracy: 89.55 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  93.24790334701538\n",
      "Epoch [11/500], Loss: 13.6952, Accuracy: 90.08 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  94.2088565826416\n",
      "Epoch [12/500], Loss: 14.0755, Accuracy: 89.64 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  94.14431369304657\n",
      "Epoch [13/500], Loss: 14.2128, Accuracy: 89.64 %\n",
      "Testing Accuracy: 45.20 %\n",
      "Test Loss:  92.6221536397934\n",
      "Epoch [14/500], Loss: 14.4066, Accuracy: 89.56 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  95.16127395629883\n",
      "Epoch [15/500], Loss: 13.5662, Accuracy: 90.29 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  94.12120270729065\n",
      "Epoch [16/500], Loss: 14.1261, Accuracy: 90.01 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  96.52487826347351\n",
      "Epoch [17/500], Loss: 15.0218, Accuracy: 88.99 %\n",
      "Testing Accuracy: 43.86 %\n",
      "Test Loss:  95.89880847930908\n",
      "Epoch [18/500], Loss: 13.4400, Accuracy: 90.49 %\n",
      "Testing Accuracy: 44.45 %\n",
      "Test Loss:  95.14720487594604\n",
      "Epoch [19/500], Loss: 13.9246, Accuracy: 89.89 %\n",
      "Testing Accuracy: 45.35 %\n",
      "Test Loss:  92.77607274055481\n",
      "Epoch [20/500], Loss: 13.5762, Accuracy: 90.27 %\n",
      "Testing Accuracy: 45.12 %\n",
      "Test Loss:  94.09295797348022\n",
      "Epoch [21/500], Loss: 14.1228, Accuracy: 89.87 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  91.05073964595795\n",
      "Epoch [22/500], Loss: 13.4354, Accuracy: 90.63 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  91.94672226905823\n",
      "Epoch [23/500], Loss: 13.3572, Accuracy: 90.33 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  94.95497989654541\n",
      "Epoch [24/500], Loss: 13.3544, Accuracy: 90.45 %\n",
      "Testing Accuracy: 45.54 %\n",
      "Test Loss:  91.43677318096161\n",
      "Epoch [25/500], Loss: 13.1828, Accuracy: 90.58 %\n",
      "Testing Accuracy: 44.76 %\n",
      "Test Loss:  94.32773101329803\n",
      "Epoch [26/500], Loss: 13.4568, Accuracy: 90.59 %\n",
      "Testing Accuracy: 45.07 %\n",
      "Test Loss:  91.70030236244202\n",
      "Epoch [27/500], Loss: 13.5746, Accuracy: 90.35 %\n",
      "Testing Accuracy: 44.68 %\n",
      "Test Loss:  93.14873671531677\n",
      "Epoch [28/500], Loss: 14.2864, Accuracy: 89.66 %\n",
      "Testing Accuracy: 44.13 %\n",
      "Test Loss:  94.64641642570496\n",
      "Epoch [29/500], Loss: 14.4537, Accuracy: 89.68 %\n",
      "Testing Accuracy: 45.09 %\n",
      "Test Loss:  93.53231906890869\n",
      "Epoch [30/500], Loss: 13.4469, Accuracy: 90.49 %\n",
      "Testing Accuracy: 44.22 %\n",
      "Test Loss:  96.99411356449127\n",
      "Epoch [31/500], Loss: 14.2711, Accuracy: 89.70 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  94.98630881309509\n",
      "Epoch [32/500], Loss: 14.4339, Accuracy: 89.47 %\n",
      "Testing Accuracy: 45.32 %\n",
      "Test Loss:  91.92277657985687\n",
      "Epoch [33/500], Loss: 13.1537, Accuracy: 90.81 %\n",
      "Testing Accuracy: 45.16 %\n",
      "Test Loss:  92.31153845787048\n",
      "Epoch [34/500], Loss: 13.5480, Accuracy: 90.17 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  94.37121820449829\n",
      "Epoch [35/500], Loss: 14.0663, Accuracy: 89.77 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  93.6265218257904\n",
      "Epoch [36/500], Loss: 13.8796, Accuracy: 90.05 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  93.49065434932709\n",
      "Epoch [37/500], Loss: 14.2956, Accuracy: 89.54 %\n",
      "Testing Accuracy: 45.59 %\n",
      "Test Loss:  94.23044550418854\n",
      "Epoch [38/500], Loss: 12.9589, Accuracy: 90.82 %\n",
      "Testing Accuracy: 45.40 %\n",
      "Test Loss:  95.71305048465729\n",
      "Epoch [39/500], Loss: 12.5908, Accuracy: 91.24 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  96.33975434303284\n",
      "Epoch [40/500], Loss: 13.2271, Accuracy: 90.59 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  93.23988568782806\n",
      "Epoch [41/500], Loss: 13.2641, Accuracy: 90.54 %\n",
      "Testing Accuracy: 45.65 %\n",
      "Test Loss:  94.6481773853302\n",
      "Testing Accuracy: 45.65 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        851968   \n",
      "Net/Dropout[dropout]/onnx::Relu   832      \n",
      "Net/Linear[fc2]/onnx::Gemm        4576     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "857,376 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289]\n",
      "pruning hidden size:  416\n",
      "with hidden layer:  416\n",
      "removing:  (127, 118, 109)\n",
      "--- 53.11803364753723 seconds ---\n",
      "Epoch [1/500], Loss: 13.0461, Accuracy: 90.84 %\n",
      "Testing Accuracy: 45.56 %\n",
      "Test Loss:  94.54775619506836\n",
      "Epoch [2/500], Loss: 14.3850, Accuracy: 89.50 %\n",
      "Testing Accuracy: 43.99 %\n",
      "Test Loss:  97.05084013938904\n",
      "Epoch [3/500], Loss: 14.4317, Accuracy: 89.36 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  95.3582843542099\n",
      "Epoch [4/500], Loss: 13.5860, Accuracy: 90.20 %\n",
      "Testing Accuracy: 45.24 %\n",
      "Test Loss:  95.30976819992065\n",
      "Epoch [5/500], Loss: 13.2317, Accuracy: 90.55 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  94.7557647228241\n",
      "Epoch [6/500], Loss: 13.0536, Accuracy: 90.77 %\n",
      "Testing Accuracy: 44.76 %\n",
      "Test Loss:  94.90074348449707\n",
      "Epoch [7/500], Loss: 13.1594, Accuracy: 90.66 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  94.21656239032745\n",
      "Epoch [8/500], Loss: 13.6236, Accuracy: 90.23 %\n",
      "Testing Accuracy: 44.90 %\n",
      "Test Loss:  93.79047203063965\n",
      "Epoch [9/500], Loss: 13.0857, Accuracy: 90.70 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  96.19357895851135\n",
      "Epoch [10/500], Loss: 13.2577, Accuracy: 90.58 %\n",
      "Testing Accuracy: 44.51 %\n",
      "Test Loss:  94.44259142875671\n",
      "Epoch [11/500], Loss: 13.3106, Accuracy: 90.58 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  93.06450712680817\n",
      "Epoch [12/500], Loss: 12.7462, Accuracy: 91.06 %\n",
      "Testing Accuracy: 44.87 %\n",
      "Test Loss:  93.20638525485992\n",
      "Epoch [13/500], Loss: 13.2302, Accuracy: 90.56 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  93.1356554031372\n",
      "Epoch [14/500], Loss: 12.7756, Accuracy: 91.05 %\n",
      "Testing Accuracy: 45.34 %\n",
      "Test Loss:  94.44369149208069\n",
      "Epoch [15/500], Loss: 12.9481, Accuracy: 90.89 %\n",
      "Testing Accuracy: 44.71 %\n",
      "Test Loss:  94.87961268424988\n",
      "Epoch [16/500], Loss: 13.4420, Accuracy: 90.36 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  92.65398824214935\n",
      "Epoch [17/500], Loss: 12.6173, Accuracy: 91.11 %\n",
      "Testing Accuracy: 45.33 %\n",
      "Test Loss:  94.71751892566681\n",
      "Epoch [18/500], Loss: 13.8781, Accuracy: 89.89 %\n",
      "Testing Accuracy: 45.44 %\n",
      "Test Loss:  95.14348697662354\n",
      "Epoch [19/500], Loss: 13.1767, Accuracy: 90.56 %\n",
      "Testing Accuracy: 44.51 %\n",
      "Test Loss:  97.11208462715149\n",
      "Epoch [20/500], Loss: 13.2220, Accuracy: 90.70 %\n",
      "Testing Accuracy: 44.84 %\n",
      "Test Loss:  95.54749202728271\n",
      "Epoch [21/500], Loss: 12.8077, Accuracy: 90.89 %\n",
      "Testing Accuracy: 44.35 %\n",
      "Test Loss:  94.55651664733887\n",
      "Epoch [22/500], Loss: 12.9507, Accuracy: 90.80 %\n",
      "Testing Accuracy: 44.63 %\n",
      "Test Loss:  96.86716318130493\n",
      "Epoch [23/500], Loss: 12.5728, Accuracy: 91.28 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  96.65501689910889\n",
      "Epoch [24/500], Loss: 13.3416, Accuracy: 90.33 %\n",
      "Testing Accuracy: 44.46 %\n",
      "Test Loss:  93.73548138141632\n",
      "Epoch [25/500], Loss: 12.3345, Accuracy: 91.46 %\n",
      "Testing Accuracy: 44.31 %\n",
      "Test Loss:  94.02150785923004\n",
      "Epoch [26/500], Loss: 13.0287, Accuracy: 90.60 %\n",
      "Testing Accuracy: 45.18 %\n",
      "Test Loss:  95.33708167076111\n",
      "Epoch [27/500], Loss: 12.2597, Accuracy: 91.53 %\n",
      "Testing Accuracy: 44.79 %\n",
      "Test Loss:  95.12772703170776\n",
      "Epoch [28/500], Loss: 12.6823, Accuracy: 91.08 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  94.24460363388062\n",
      "Epoch [29/500], Loss: 12.5326, Accuracy: 91.14 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  94.00847017765045\n",
      "Epoch [30/500], Loss: 12.9449, Accuracy: 90.83 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  93.88535070419312\n",
      "Epoch [31/500], Loss: 12.9966, Accuracy: 90.67 %\n",
      "Testing Accuracy: 44.81 %\n",
      "Test Loss:  95.99395895004272\n",
      "Epoch [32/500], Loss: 12.8347, Accuracy: 90.91 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  95.51750385761261\n",
      "Epoch [33/500], Loss: 12.7064, Accuracy: 91.11 %\n",
      "Testing Accuracy: 44.65 %\n",
      "Test Loss:  95.79212522506714\n",
      "Epoch [34/500], Loss: 13.5288, Accuracy: 90.30 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  97.69004452228546\n",
      "Epoch [35/500], Loss: 13.3961, Accuracy: 90.25 %\n",
      "Testing Accuracy: 44.92 %\n",
      "Test Loss:  95.00353288650513\n",
      "Epoch [36/500], Loss: 12.7827, Accuracy: 90.98 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  96.23712587356567\n",
      "Testing Accuracy: 44.72 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        845824   \n",
      "Net/Dropout[dropout]/onnx::Relu   826      \n",
      "Net/Linear[fc2]/onnx::Gemm        4543     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "851,193 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852]\n",
      "pruning hidden size:  413\n",
      "with hidden layer:  413\n",
      "removing:  (358, 255, 17)\n",
      "--- 50.3310432434082 seconds ---\n",
      "Epoch [1/500], Loss: 12.1184, Accuracy: 91.58 %\n",
      "Testing Accuracy: 44.08 %\n",
      "Test Loss:  97.59742856025696\n",
      "Epoch [2/500], Loss: 13.8737, Accuracy: 90.05 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  97.82078003883362\n",
      "Epoch [3/500], Loss: 13.2292, Accuracy: 90.47 %\n",
      "Testing Accuracy: 44.95 %\n",
      "Test Loss:  97.7238199710846\n",
      "Epoch [4/500], Loss: 13.3938, Accuracy: 90.47 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  97.09179997444153\n",
      "Epoch [5/500], Loss: 13.0976, Accuracy: 90.62 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  95.55931448936462\n",
      "Epoch [6/500], Loss: 12.4295, Accuracy: 91.24 %\n",
      "Testing Accuracy: 44.52 %\n",
      "Test Loss:  95.06004810333252\n",
      "Epoch [7/500], Loss: 12.4709, Accuracy: 91.39 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  95.95316934585571\n",
      "Epoch [8/500], Loss: 12.8608, Accuracy: 90.76 %\n",
      "Testing Accuracy: 44.64 %\n",
      "Test Loss:  95.51949262619019\n",
      "Epoch [9/500], Loss: 12.4378, Accuracy: 91.31 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  94.83870506286621\n",
      "Epoch [10/500], Loss: 12.4571, Accuracy: 91.29 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  95.11962032318115\n",
      "Epoch [11/500], Loss: 12.7242, Accuracy: 90.97 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  95.88767409324646\n",
      "Epoch [12/500], Loss: 12.7868, Accuracy: 90.87 %\n",
      "Testing Accuracy: 44.75 %\n",
      "Test Loss:  94.45023357868195\n",
      "Epoch [13/500], Loss: 12.4557, Accuracy: 91.31 %\n",
      "Testing Accuracy: 44.92 %\n",
      "Test Loss:  95.60854411125183\n",
      "Epoch [14/500], Loss: 12.4092, Accuracy: 91.16 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  95.91960096359253\n",
      "Epoch [15/500], Loss: 12.6500, Accuracy: 90.98 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  94.62673795223236\n",
      "Epoch [16/500], Loss: 12.9104, Accuracy: 90.81 %\n",
      "Testing Accuracy: 44.76 %\n",
      "Test Loss:  97.49438881874084\n",
      "Epoch [17/500], Loss: 12.7544, Accuracy: 90.94 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  98.25816786289215\n",
      "Epoch [18/500], Loss: 12.3695, Accuracy: 91.22 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  97.39499723911285\n",
      "Epoch [19/500], Loss: 12.7167, Accuracy: 90.98 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  97.67286229133606\n",
      "Epoch [20/500], Loss: 12.4503, Accuracy: 91.15 %\n",
      "Testing Accuracy: 44.65 %\n",
      "Test Loss:  98.35099124908447\n",
      "Epoch [21/500], Loss: 12.0131, Accuracy: 91.64 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  95.75086283683777\n",
      "Epoch [22/500], Loss: 11.8513, Accuracy: 91.81 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  96.78485190868378\n",
      "Epoch [23/500], Loss: 12.1708, Accuracy: 91.49 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  96.95711374282837\n",
      "Epoch [24/500], Loss: 12.4749, Accuracy: 91.15 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  96.97036528587341\n",
      "Epoch [25/500], Loss: 12.0025, Accuracy: 91.68 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  95.30826544761658\n",
      "Epoch [26/500], Loss: 12.0271, Accuracy: 91.56 %\n",
      "Testing Accuracy: 44.67 %\n",
      "Test Loss:  96.61965107917786\n",
      "Epoch [27/500], Loss: 13.1264, Accuracy: 90.58 %\n",
      "Testing Accuracy: 45.17 %\n",
      "Test Loss:  96.3455708026886\n",
      "Epoch [28/500], Loss: 11.9586, Accuracy: 91.59 %\n",
      "Testing Accuracy: 44.57 %\n",
      "Test Loss:  96.33472537994385\n",
      "Epoch [29/500], Loss: 12.1909, Accuracy: 91.40 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  94.80467748641968\n",
      "Epoch [30/500], Loss: 12.1098, Accuracy: 91.59 %\n",
      "Testing Accuracy: 44.77 %\n",
      "Test Loss:  98.60569739341736\n",
      "Epoch [31/500], Loss: 12.0200, Accuracy: 91.57 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  96.17252612113953\n",
      "Epoch [32/500], Loss: 12.4802, Accuracy: 91.11 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  94.40259110927582\n",
      "Epoch [33/500], Loss: 12.4056, Accuracy: 91.17 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  97.87202596664429\n",
      "Epoch [34/500], Loss: 13.4726, Accuracy: 90.17 %\n",
      "Testing Accuracy: 44.73 %\n",
      "Test Loss:  96.13530492782593\n",
      "Epoch [35/500], Loss: 11.8658, Accuracy: 91.61 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  97.76142525672913\n",
      "Epoch [36/500], Loss: 12.1159, Accuracy: 91.62 %\n",
      "Testing Accuracy: 44.89 %\n",
      "Test Loss:  96.11751472949982\n",
      "Epoch [37/500], Loss: 12.2800, Accuracy: 91.25 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  95.87551474571228\n",
      "Epoch [38/500], Loss: 11.4284, Accuracy: 92.27 %\n",
      "Testing Accuracy: 44.22 %\n",
      "Test Loss:  98.01470589637756\n",
      "Epoch [39/500], Loss: 11.7641, Accuracy: 91.79 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  95.37922024726868\n",
      "Epoch [40/500], Loss: 11.8106, Accuracy: 91.60 %\n",
      "Testing Accuracy: 44.44 %\n",
      "Test Loss:  94.36576759815216\n",
      "Epoch [41/500], Loss: 11.3047, Accuracy: 92.20 %\n",
      "Testing Accuracy: 44.55 %\n",
      "Test Loss:  95.0547970533371\n",
      "Epoch [42/500], Loss: 11.6905, Accuracy: 92.00 %\n",
      "Testing Accuracy: 44.73 %\n",
      "Test Loss:  95.33567190170288\n",
      "Epoch [43/500], Loss: 11.7970, Accuracy: 91.77 %\n",
      "Testing Accuracy: 44.42 %\n",
      "Test Loss:  95.5461562871933\n",
      "Epoch [44/500], Loss: 11.6225, Accuracy: 91.98 %\n",
      "Testing Accuracy: 44.97 %\n",
      "Test Loss:  95.03150403499603\n",
      "Epoch [45/500], Loss: 11.3869, Accuracy: 92.18 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  97.20060181617737\n",
      "Epoch [46/500], Loss: 11.3703, Accuracy: 92.08 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  98.02769207954407\n",
      "Epoch [47/500], Loss: 11.8642, Accuracy: 91.69 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  98.75783944129944\n",
      "Epoch [48/500], Loss: 12.2192, Accuracy: 91.28 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  97.51564764976501\n",
      "Epoch [49/500], Loss: 12.3535, Accuracy: 91.20 %\n",
      "Testing Accuracy: 44.95 %\n",
      "Test Loss:  97.2605459690094\n",
      "Epoch [50/500], Loss: 11.6493, Accuracy: 91.74 %\n",
      "Testing Accuracy: 44.65 %\n",
      "Test Loss:  95.48592567443848\n",
      "Epoch [51/500], Loss: 12.1949, Accuracy: 91.40 %\n",
      "Testing Accuracy: 44.20 %\n",
      "Test Loss:  96.28211867809296\n",
      "Epoch [52/500], Loss: 11.8499, Accuracy: 91.62 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  99.7633945941925\n",
      "Epoch [53/500], Loss: 12.0847, Accuracy: 91.37 %\n",
      "Testing Accuracy: 44.50 %\n",
      "Test Loss:  96.619473695755\n",
      "Epoch [54/500], Loss: 12.1037, Accuracy: 91.41 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  97.22065901756287\n",
      "Epoch [55/500], Loss: 12.3846, Accuracy: 91.20 %\n",
      "Testing Accuracy: 44.39 %\n",
      "Test Loss:  99.72966241836548\n",
      "Epoch [56/500], Loss: 11.9619, Accuracy: 91.55 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  95.64949679374695\n",
      "Epoch [57/500], Loss: 11.7117, Accuracy: 91.85 %\n",
      "Testing Accuracy: 44.84 %\n",
      "Test Loss:  94.70704758167267\n",
      "Epoch [58/500], Loss: 11.9775, Accuracy: 91.68 %\n",
      "Testing Accuracy: 44.20 %\n",
      "Test Loss:  95.63708424568176\n",
      "Epoch [59/500], Loss: 11.5669, Accuracy: 92.00 %\n",
      "Testing Accuracy: 44.52 %\n",
      "Test Loss:  97.49684000015259\n",
      "Epoch [60/500], Loss: 11.6381, Accuracy: 91.91 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  99.21749567985535\n",
      "Testing Accuracy: 44.47 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        839680   \n",
      "Net/Dropout[dropout]/onnx::Relu   820      \n",
      "Net/Linear[fc2]/onnx::Gemm        4510     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "845,010 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485]\n",
      "pruning hidden size:  410\n",
      "with hidden layer:  410\n",
      "removing:  (251, 159, 102)\n",
      "--- 49.574756145477295 seconds ---\n",
      "Epoch [1/500], Loss: 12.4727, Accuracy: 90.96 %\n",
      "Testing Accuracy: 44.38 %\n",
      "Test Loss:  99.7003984451294\n",
      "Epoch [2/500], Loss: 12.5557, Accuracy: 90.90 %\n",
      "Testing Accuracy: 44.61 %\n",
      "Test Loss:  99.62462639808655\n",
      "Epoch [3/500], Loss: 12.2321, Accuracy: 91.25 %\n",
      "Testing Accuracy: 44.03 %\n",
      "Test Loss:  97.69900560379028\n",
      "Epoch [4/500], Loss: 12.6777, Accuracy: 91.04 %\n",
      "Testing Accuracy: 44.39 %\n",
      "Test Loss:  95.84723925590515\n",
      "Epoch [5/500], Loss: 12.1652, Accuracy: 91.31 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  97.7971522808075\n",
      "Epoch [6/500], Loss: 11.9394, Accuracy: 91.58 %\n",
      "Testing Accuracy: 44.73 %\n",
      "Test Loss:  96.32080793380737\n",
      "Epoch [7/500], Loss: 12.4793, Accuracy: 91.14 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  96.51957964897156\n",
      "Epoch [8/500], Loss: 11.7475, Accuracy: 91.72 %\n",
      "Testing Accuracy: 44.93 %\n",
      "Test Loss:  97.51991844177246\n",
      "Epoch [9/500], Loss: 11.7667, Accuracy: 91.76 %\n",
      "Testing Accuracy: 44.79 %\n",
      "Test Loss:  95.13046216964722\n",
      "Epoch [10/500], Loss: 11.4411, Accuracy: 92.09 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  97.66173791885376\n",
      "Epoch [11/500], Loss: 11.7710, Accuracy: 91.86 %\n",
      "Testing Accuracy: 45.51 %\n",
      "Test Loss:  95.6517927646637\n",
      "Epoch [12/500], Loss: 11.5001, Accuracy: 91.98 %\n",
      "Testing Accuracy: 44.23 %\n",
      "Test Loss:  98.1327338218689\n",
      "Epoch [13/500], Loss: 11.8747, Accuracy: 91.54 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  100.50539124011993\n",
      "Epoch [14/500], Loss: 11.0785, Accuracy: 92.28 %\n",
      "Testing Accuracy: 44.32 %\n",
      "Test Loss:  98.37091398239136\n",
      "Epoch [15/500], Loss: 11.2225, Accuracy: 92.15 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  95.76261305809021\n",
      "Epoch [16/500], Loss: 11.3315, Accuracy: 92.04 %\n",
      "Testing Accuracy: 44.63 %\n",
      "Test Loss:  97.52934336662292\n",
      "Epoch [17/500], Loss: 11.4154, Accuracy: 91.96 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  99.04412198066711\n",
      "Epoch [18/500], Loss: 11.8685, Accuracy: 91.52 %\n",
      "Testing Accuracy: 44.19 %\n",
      "Test Loss:  96.73232853412628\n",
      "Epoch [19/500], Loss: 11.2004, Accuracy: 92.25 %\n",
      "Testing Accuracy: 44.29 %\n",
      "Test Loss:  96.95579266548157\n",
      "Epoch [20/500], Loss: 11.1477, Accuracy: 92.35 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  97.23129987716675\n",
      "Epoch [21/500], Loss: 11.5631, Accuracy: 91.70 %\n",
      "Testing Accuracy: 44.50 %\n",
      "Test Loss:  97.33616781234741\n",
      "Epoch [22/500], Loss: 11.1197, Accuracy: 92.41 %\n",
      "Testing Accuracy: 45.20 %\n",
      "Test Loss:  96.1724739074707\n",
      "Epoch [23/500], Loss: 11.0071, Accuracy: 92.33 %\n",
      "Testing Accuracy: 44.37 %\n",
      "Test Loss:  98.39959859848022\n",
      "Epoch [24/500], Loss: 11.2951, Accuracy: 92.13 %\n",
      "Testing Accuracy: 44.55 %\n",
      "Test Loss:  97.09139227867126\n",
      "Epoch [25/500], Loss: 11.2775, Accuracy: 92.12 %\n",
      "Testing Accuracy: 44.69 %\n",
      "Test Loss:  98.33460474014282\n",
      "Epoch [26/500], Loss: 11.6695, Accuracy: 91.72 %\n",
      "Testing Accuracy: 44.73 %\n",
      "Test Loss:  98.17533874511719\n",
      "Epoch [27/500], Loss: 11.1521, Accuracy: 92.22 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  97.80170345306396\n",
      "Epoch [28/500], Loss: 11.5209, Accuracy: 91.90 %\n",
      "Testing Accuracy: 44.53 %\n",
      "Test Loss:  97.29683983325958\n",
      "Epoch [29/500], Loss: 11.4057, Accuracy: 92.04 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  97.85414099693298\n",
      "Testing Accuracy: 45.15 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        833536   \n",
      "Net/Dropout[dropout]/onnx::Relu   814      \n",
      "Net/Linear[fc2]/onnx::Gemm        4477     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "838,827 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641]\n",
      "pruning hidden size:  407\n",
      "with hidden layer:  407\n",
      "removing:  (348, 391, 89)\n",
      "--- 49.025784492492676 seconds ---\n",
      "Epoch [1/500], Loss: 11.3180, Accuracy: 92.18 %\n",
      "Testing Accuracy: 44.92 %\n",
      "Test Loss:  99.52833151817322\n",
      "Epoch [2/500], Loss: 11.5576, Accuracy: 91.83 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  99.35913062095642\n",
      "Epoch [3/500], Loss: 11.2508, Accuracy: 92.11 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  99.14783763885498\n",
      "Epoch [4/500], Loss: 11.5608, Accuracy: 91.70 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  101.13283896446228\n",
      "Epoch [5/500], Loss: 11.8759, Accuracy: 91.42 %\n",
      "Testing Accuracy: 44.46 %\n",
      "Test Loss:  98.88979268074036\n",
      "Epoch [6/500], Loss: 11.1742, Accuracy: 92.19 %\n",
      "Testing Accuracy: 44.73 %\n",
      "Test Loss:  99.0159866809845\n",
      "Epoch [7/500], Loss: 11.3421, Accuracy: 92.15 %\n",
      "Testing Accuracy: 45.03 %\n",
      "Test Loss:  98.35508108139038\n",
      "Epoch [8/500], Loss: 11.3294, Accuracy: 92.18 %\n",
      "Testing Accuracy: 45.05 %\n",
      "Test Loss:  97.88204455375671\n",
      "Epoch [9/500], Loss: 11.3972, Accuracy: 92.06 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  96.44386339187622\n",
      "Epoch [10/500], Loss: 10.9842, Accuracy: 92.40 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  102.1701807975769\n",
      "Epoch [11/500], Loss: 11.6098, Accuracy: 91.78 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  97.69504237174988\n",
      "Epoch [12/500], Loss: 11.5610, Accuracy: 91.80 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  97.77670383453369\n",
      "Epoch [13/500], Loss: 11.1137, Accuracy: 92.27 %\n",
      "Testing Accuracy: 45.12 %\n",
      "Test Loss:  98.96067523956299\n",
      "Epoch [14/500], Loss: 11.6909, Accuracy: 91.77 %\n",
      "Testing Accuracy: 45.19 %\n",
      "Test Loss:  98.56909775733948\n",
      "Epoch [15/500], Loss: 11.1140, Accuracy: 92.21 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  98.51059484481812\n",
      "Epoch [16/500], Loss: 11.1729, Accuracy: 92.27 %\n",
      "Testing Accuracy: 44.40 %\n",
      "Test Loss:  95.4348692893982\n",
      "Epoch [17/500], Loss: 11.4904, Accuracy: 91.95 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  99.16744923591614\n",
      "Epoch [18/500], Loss: 11.0867, Accuracy: 92.35 %\n",
      "Testing Accuracy: 45.01 %\n",
      "Test Loss:  98.32620763778687\n",
      "Epoch [19/500], Loss: 11.3987, Accuracy: 91.98 %\n",
      "Testing Accuracy: 43.66 %\n",
      "Test Loss:  100.30774140357971\n",
      "Epoch [20/500], Loss: 11.2107, Accuracy: 92.07 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  99.79730916023254\n",
      "Epoch [21/500], Loss: 11.3229, Accuracy: 92.07 %\n",
      "Testing Accuracy: 44.20 %\n",
      "Test Loss:  98.90332531929016\n",
      "Epoch [22/500], Loss: 12.1546, Accuracy: 91.25 %\n",
      "Testing Accuracy: 43.51 %\n",
      "Test Loss:  101.38376998901367\n",
      "Epoch [23/500], Loss: 11.9380, Accuracy: 91.39 %\n",
      "Testing Accuracy: 45.30 %\n",
      "Test Loss:  99.44595909118652\n",
      "Epoch [24/500], Loss: 10.8307, Accuracy: 92.46 %\n",
      "Testing Accuracy: 44.84 %\n",
      "Test Loss:  99.08250904083252\n",
      "Epoch [25/500], Loss: 10.9202, Accuracy: 92.39 %\n",
      "Testing Accuracy: 45.02 %\n",
      "Test Loss:  99.72317433357239\n",
      "Epoch [26/500], Loss: 10.5591, Accuracy: 92.68 %\n",
      "Testing Accuracy: 44.68 %\n",
      "Test Loss:  99.69137263298035\n",
      "Epoch [27/500], Loss: 10.8235, Accuracy: 92.58 %\n",
      "Testing Accuracy: 44.82 %\n",
      "Test Loss:  96.72936201095581\n",
      "Epoch [28/500], Loss: 10.5522, Accuracy: 92.76 %\n",
      "Testing Accuracy: 44.99 %\n",
      "Test Loss:  98.18428158760071\n",
      "Epoch [29/500], Loss: 11.3786, Accuracy: 91.86 %\n",
      "Testing Accuracy: 44.91 %\n",
      "Test Loss:  97.51293969154358\n",
      "Epoch [30/500], Loss: 11.1531, Accuracy: 92.35 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  98.72081708908081\n",
      "Epoch [31/500], Loss: 10.8998, Accuracy: 92.35 %\n",
      "Testing Accuracy: 44.90 %\n",
      "Test Loss:  99.33674955368042\n",
      "Epoch [32/500], Loss: 11.1401, Accuracy: 92.13 %\n",
      "Testing Accuracy: 44.31 %\n",
      "Test Loss:  100.85799622535706\n",
      "Epoch [33/500], Loss: 11.0931, Accuracy: 92.41 %\n",
      "Testing Accuracy: 45.11 %\n",
      "Test Loss:  98.92569136619568\n",
      "Epoch [34/500], Loss: 11.3558, Accuracy: 92.01 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  100.78229212760925\n",
      "Epoch [35/500], Loss: 10.7969, Accuracy: 92.43 %\n",
      "Testing Accuracy: 44.51 %\n",
      "Test Loss:  99.13810014724731\n",
      "Epoch [36/500], Loss: 10.4723, Accuracy: 92.93 %\n",
      "Testing Accuracy: 44.88 %\n",
      "Test Loss:  99.35218834877014\n",
      "Testing Accuracy: 44.88 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        827392   \n",
      "Net/Dropout[dropout]/onnx::Relu   808      \n",
      "Net/Linear[fc2]/onnx::Gemm        4444     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "832,644 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582]\n",
      "pruning hidden size:  404\n",
      "with hidden layer:  404\n",
      "removing:  (231, 136, 48)\n",
      "--- 47.211130142211914 seconds ---\n",
      "Epoch [1/500], Loss: 11.1405, Accuracy: 92.32 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  96.30685865879059\n",
      "Epoch [2/500], Loss: 11.4251, Accuracy: 92.03 %\n",
      "Testing Accuracy: 44.76 %\n",
      "Test Loss:  97.26256012916565\n",
      "Epoch [3/500], Loss: 11.1891, Accuracy: 92.13 %\n",
      "Testing Accuracy: 44.80 %\n",
      "Test Loss:  99.27348732948303\n",
      "Epoch [4/500], Loss: 11.9686, Accuracy: 91.51 %\n",
      "Testing Accuracy: 44.28 %\n",
      "Test Loss:  96.63599681854248\n",
      "Epoch [5/500], Loss: 11.7927, Accuracy: 91.64 %\n",
      "Testing Accuracy: 44.25 %\n",
      "Test Loss:  97.1751537322998\n",
      "Epoch [6/500], Loss: 11.0825, Accuracy: 92.27 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  98.28427147865295\n",
      "Epoch [7/500], Loss: 11.4503, Accuracy: 91.99 %\n",
      "Testing Accuracy: 44.09 %\n",
      "Test Loss:  98.62254202365875\n",
      "Epoch [8/500], Loss: 11.2629, Accuracy: 92.20 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  103.00645136833191\n",
      "Epoch [9/500], Loss: 10.7587, Accuracy: 92.64 %\n",
      "Testing Accuracy: 44.38 %\n",
      "Test Loss:  98.97670412063599\n",
      "Epoch [10/500], Loss: 11.0509, Accuracy: 92.18 %\n",
      "Testing Accuracy: 44.52 %\n",
      "Test Loss:  98.70999526977539\n",
      "Epoch [11/500], Loss: 10.9993, Accuracy: 92.22 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  99.29848408699036\n",
      "Epoch [12/500], Loss: 11.4315, Accuracy: 91.95 %\n",
      "Testing Accuracy: 44.51 %\n",
      "Test Loss:  98.72933435440063\n",
      "Epoch [13/500], Loss: 11.2046, Accuracy: 92.13 %\n",
      "Testing Accuracy: 44.62 %\n",
      "Test Loss:  98.2146577835083\n",
      "Epoch [14/500], Loss: 10.8082, Accuracy: 92.48 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  100.31255006790161\n",
      "Epoch [15/500], Loss: 10.7158, Accuracy: 92.53 %\n",
      "Testing Accuracy: 44.70 %\n",
      "Test Loss:  97.75253677368164\n",
      "Epoch [16/500], Loss: 10.9717, Accuracy: 92.43 %\n",
      "Testing Accuracy: 44.57 %\n",
      "Test Loss:  100.13264966011047\n",
      "Epoch [17/500], Loss: 11.4878, Accuracy: 91.83 %\n",
      "Testing Accuracy: 44.87 %\n",
      "Test Loss:  97.73235368728638\n",
      "Epoch [18/500], Loss: 11.0482, Accuracy: 92.22 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  101.75467014312744\n",
      "Epoch [19/500], Loss: 10.9768, Accuracy: 92.27 %\n",
      "Testing Accuracy: 44.48 %\n",
      "Test Loss:  101.24689865112305\n",
      "Epoch [20/500], Loss: 10.9102, Accuracy: 92.40 %\n",
      "Testing Accuracy: 44.72 %\n",
      "Test Loss:  98.65841174125671\n",
      "Epoch [21/500], Loss: 10.8306, Accuracy: 92.56 %\n",
      "Testing Accuracy: 44.44 %\n",
      "Test Loss:  98.98950505256653\n",
      "Testing Accuracy: 44.44 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        821248   \n",
      "Net/Dropout[dropout]/onnx::Relu   802      \n",
      "Net/Linear[fc2]/onnx::Gemm        4411     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "826,461 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352]\n",
      "pruning hidden size:  401\n",
      "with hidden layer:  401\n",
      "removing:  (288, 246, 231)\n",
      "--- 48.71681785583496 seconds ---\n",
      "Epoch [1/500], Loss: 10.3539, Accuracy: 92.94 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  98.77496337890625\n",
      "Epoch [2/500], Loss: 10.7716, Accuracy: 92.64 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  103.00169491767883\n",
      "Epoch [3/500], Loss: 11.6972, Accuracy: 91.67 %\n",
      "Testing Accuracy: 44.33 %\n",
      "Test Loss:  100.29217100143433\n",
      "Epoch [4/500], Loss: 11.0525, Accuracy: 92.34 %\n",
      "Testing Accuracy: 44.60 %\n",
      "Test Loss:  98.19746518135071\n",
      "Epoch [5/500], Loss: 12.1064, Accuracy: 91.26 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  100.15140795707703\n",
      "Epoch [6/500], Loss: 12.2155, Accuracy: 91.17 %\n",
      "Testing Accuracy: 44.58 %\n",
      "Test Loss:  97.02307891845703\n",
      "Epoch [7/500], Loss: 11.4988, Accuracy: 92.03 %\n",
      "Testing Accuracy: 44.92 %\n",
      "Test Loss:  98.96223664283752\n",
      "Epoch [8/500], Loss: 11.3893, Accuracy: 91.98 %\n",
      "Testing Accuracy: 44.45 %\n",
      "Test Loss:  100.26750254631042\n",
      "Epoch [9/500], Loss: 11.4037, Accuracy: 91.85 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  98.617604970932\n",
      "Epoch [10/500], Loss: 13.2046, Accuracy: 90.22 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  100.4832866191864\n",
      "Epoch [11/500], Loss: 12.4504, Accuracy: 90.92 %\n",
      "Testing Accuracy: 44.48 %\n",
      "Test Loss:  98.25094628334045\n",
      "Epoch [12/500], Loss: 12.1077, Accuracy: 91.34 %\n",
      "Testing Accuracy: 44.45 %\n",
      "Test Loss:  99.56309938430786\n",
      "Epoch [13/500], Loss: 12.4836, Accuracy: 90.92 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  101.5323007106781\n",
      "Epoch [14/500], Loss: 13.5669, Accuracy: 89.77 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  98.77683806419373\n",
      "Epoch [15/500], Loss: 12.1620, Accuracy: 91.29 %\n",
      "Testing Accuracy: 44.38 %\n",
      "Test Loss:  98.34955859184265\n",
      "Epoch [16/500], Loss: 11.9463, Accuracy: 91.63 %\n",
      "Testing Accuracy: 44.37 %\n",
      "Test Loss:  99.20543360710144\n",
      "Epoch [17/500], Loss: 11.3668, Accuracy: 92.05 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  98.34284663200378\n",
      "Epoch [18/500], Loss: 11.3243, Accuracy: 92.00 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  96.68395400047302\n",
      "Epoch [19/500], Loss: 10.8755, Accuracy: 92.46 %\n",
      "Testing Accuracy: 44.45 %\n",
      "Test Loss:  98.73573088645935\n",
      "Epoch [20/500], Loss: 11.3494, Accuracy: 91.90 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  99.32524824142456\n",
      "Epoch [21/500], Loss: 11.3439, Accuracy: 91.93 %\n",
      "Testing Accuracy: 44.21 %\n",
      "Test Loss:  99.13557600975037\n",
      "Epoch [22/500], Loss: 11.2190, Accuracy: 92.16 %\n",
      "Testing Accuracy: 44.09 %\n",
      "Test Loss:  100.33307576179504\n",
      "Epoch [23/500], Loss: 10.5312, Accuracy: 92.71 %\n",
      "Testing Accuracy: 44.96 %\n",
      "Test Loss:  97.8936538696289\n",
      "Epoch [24/500], Loss: 10.2647, Accuracy: 92.91 %\n",
      "Testing Accuracy: 44.99 %\n",
      "Test Loss:  98.81097829341888\n",
      "Epoch [25/500], Loss: 10.6277, Accuracy: 92.68 %\n",
      "Testing Accuracy: 44.34 %\n",
      "Test Loss:  101.76649475097656\n",
      "Epoch [26/500], Loss: 11.1403, Accuracy: 92.15 %\n",
      "Testing Accuracy: 44.68 %\n",
      "Test Loss:  101.08908176422119\n",
      "Epoch [27/500], Loss: 10.3570, Accuracy: 92.84 %\n",
      "Testing Accuracy: 44.23 %\n",
      "Test Loss:  100.35389757156372\n",
      "Epoch [28/500], Loss: 11.2827, Accuracy: 91.98 %\n",
      "Testing Accuracy: 44.64 %\n",
      "Test Loss:  100.48133111000061\n",
      "Epoch [29/500], Loss: 11.1678, Accuracy: 92.16 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  99.70269727706909\n",
      "Epoch [30/500], Loss: 10.9933, Accuracy: 92.29 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  101.72154092788696\n",
      "Epoch [31/500], Loss: 11.1230, Accuracy: 92.25 %\n",
      "Testing Accuracy: 44.68 %\n",
      "Test Loss:  99.78131747245789\n",
      "Epoch [32/500], Loss: 11.1942, Accuracy: 92.18 %\n",
      "Testing Accuracy: 45.15 %\n",
      "Test Loss:  99.69406342506409\n",
      "Epoch [33/500], Loss: 10.8733, Accuracy: 92.34 %\n",
      "Testing Accuracy: 44.94 %\n",
      "Test Loss:  99.49070978164673\n",
      "Epoch [34/500], Loss: 10.7610, Accuracy: 92.44 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  99.28487992286682\n",
      "Epoch [35/500], Loss: 10.3963, Accuracy: 92.90 %\n",
      "Testing Accuracy: 44.98 %\n",
      "Test Loss:  102.31421542167664\n",
      "Epoch [36/500], Loss: 10.4937, Accuracy: 92.81 %\n",
      "Testing Accuracy: 44.79 %\n",
      "Test Loss:  98.9852843284607\n",
      "Epoch [37/500], Loss: 10.4749, Accuracy: 92.67 %\n",
      "Testing Accuracy: 44.45 %\n",
      "Test Loss:  100.16261458396912\n",
      "Epoch [38/500], Loss: 10.2090, Accuracy: 93.00 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  98.20525074005127\n",
      "Testing Accuracy: 44.27 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        815104   \n",
      "Net/Dropout[dropout]/onnx::Relu   796      \n",
      "Net/Linear[fc2]/onnx::Gemm        4378     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "820,278 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775]\n",
      "pruning hidden size:  398\n",
      "with hidden layer:  398\n",
      "removing:  (397, 377, 11)\n",
      "--- 46.270572662353516 seconds ---\n",
      "Epoch [1/500], Loss: 10.5454, Accuracy: 92.64 %\n",
      "Testing Accuracy: 45.13 %\n",
      "Test Loss:  97.57314562797546\n",
      "Epoch [2/500], Loss: 10.9334, Accuracy: 92.43 %\n",
      "Testing Accuracy: 44.06 %\n",
      "Test Loss:  100.9864776134491\n",
      "Epoch [3/500], Loss: 11.1377, Accuracy: 92.14 %\n",
      "Testing Accuracy: 44.78 %\n",
      "Test Loss:  100.56081175804138\n",
      "Epoch [4/500], Loss: 11.6617, Accuracy: 91.65 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  100.22981142997742\n",
      "Epoch [5/500], Loss: 12.3994, Accuracy: 90.99 %\n",
      "Testing Accuracy: 43.95 %\n",
      "Test Loss:  104.93113088607788\n",
      "Epoch [6/500], Loss: 12.1265, Accuracy: 91.22 %\n",
      "Testing Accuracy: 44.49 %\n",
      "Test Loss:  100.08397150039673\n",
      "Epoch [7/500], Loss: 11.4099, Accuracy: 91.98 %\n",
      "Testing Accuracy: 44.25 %\n",
      "Test Loss:  100.51828980445862\n",
      "Epoch [8/500], Loss: 11.1612, Accuracy: 92.18 %\n",
      "Testing Accuracy: 44.70 %\n",
      "Test Loss:  98.59309422969818\n",
      "Epoch [9/500], Loss: 10.9140, Accuracy: 92.29 %\n",
      "Testing Accuracy: 44.31 %\n",
      "Test Loss:  100.8826310634613\n",
      "Epoch [10/500], Loss: 11.4318, Accuracy: 91.84 %\n",
      "Testing Accuracy: 44.08 %\n",
      "Test Loss:  100.69441437721252\n",
      "Epoch [11/500], Loss: 11.4096, Accuracy: 91.93 %\n",
      "Testing Accuracy: 44.47 %\n",
      "Test Loss:  98.06548261642456\n",
      "Epoch [12/500], Loss: 10.6501, Accuracy: 92.54 %\n",
      "Testing Accuracy: 44.29 %\n",
      "Test Loss:  100.19133639335632\n",
      "Epoch [13/500], Loss: 11.3778, Accuracy: 91.92 %\n",
      "Testing Accuracy: 44.50 %\n",
      "Test Loss:  99.76248049736023\n",
      "Epoch [14/500], Loss: 11.4461, Accuracy: 91.92 %\n",
      "Testing Accuracy: 44.21 %\n",
      "Test Loss:  97.4749710559845\n",
      "Epoch [15/500], Loss: 11.4385, Accuracy: 91.78 %\n",
      "Testing Accuracy: 44.74 %\n",
      "Test Loss:  99.2651538848877\n",
      "Epoch [16/500], Loss: 11.4273, Accuracy: 91.86 %\n",
      "Testing Accuracy: 44.50 %\n",
      "Test Loss:  99.72335243225098\n",
      "Epoch [17/500], Loss: 11.2878, Accuracy: 92.10 %\n",
      "Testing Accuracy: 44.86 %\n",
      "Test Loss:  98.87243700027466\n",
      "Epoch [18/500], Loss: 10.9975, Accuracy: 92.26 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  99.58748960494995\n",
      "Epoch [19/500], Loss: 11.8713, Accuracy: 91.45 %\n",
      "Testing Accuracy: 43.91 %\n",
      "Test Loss:  103.4919683933258\n",
      "Epoch [20/500], Loss: 12.5095, Accuracy: 90.88 %\n",
      "Testing Accuracy: 43.63 %\n",
      "Test Loss:  101.22814607620239\n",
      "Epoch [21/500], Loss: 13.0306, Accuracy: 90.40 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  97.67881894111633\n",
      "Epoch [22/500], Loss: 12.3236, Accuracy: 91.13 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  99.1790177822113\n",
      "Epoch [23/500], Loss: 11.6964, Accuracy: 91.59 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  101.348073720932\n",
      "Epoch [24/500], Loss: 11.7573, Accuracy: 91.60 %\n",
      "Testing Accuracy: 43.89 %\n",
      "Test Loss:  98.82126259803772\n",
      "Epoch [25/500], Loss: 11.6897, Accuracy: 91.65 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  101.51233649253845\n",
      "Epoch [26/500], Loss: 11.6420, Accuracy: 91.61 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  97.92339992523193\n",
      "Epoch [27/500], Loss: 11.8827, Accuracy: 91.45 %\n",
      "Testing Accuracy: 43.94 %\n",
      "Test Loss:  99.63126826286316\n",
      "Epoch [28/500], Loss: 11.8179, Accuracy: 91.55 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  100.87209606170654\n",
      "Epoch [29/500], Loss: 11.5548, Accuracy: 91.74 %\n",
      "Testing Accuracy: 43.74 %\n",
      "Test Loss:  101.04522585868835\n",
      "Epoch [30/500], Loss: 11.8651, Accuracy: 91.54 %\n",
      "Testing Accuracy: 44.66 %\n",
      "Test Loss:  99.17503595352173\n",
      "Epoch [31/500], Loss: 11.6870, Accuracy: 91.69 %\n",
      "Testing Accuracy: 44.43 %\n",
      "Test Loss:  100.22062611579895\n",
      "Epoch [32/500], Loss: 11.9060, Accuracy: 91.29 %\n",
      "Testing Accuracy: 44.31 %\n",
      "Test Loss:  101.32456827163696\n",
      "Epoch [33/500], Loss: 11.7730, Accuracy: 91.56 %\n",
      "Testing Accuracy: 44.52 %\n",
      "Test Loss:  97.65713310241699\n",
      "Epoch [34/500], Loss: 11.0762, Accuracy: 92.36 %\n",
      "Testing Accuracy: 44.39 %\n",
      "Test Loss:  98.58509874343872\n",
      "Testing Accuracy: 44.39 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        808960   \n",
      "Net/Dropout[dropout]/onnx::Relu   790      \n",
      "Net/Linear[fc2]/onnx::Gemm        4345     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "814,095 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544]\n",
      "pruning hidden size:  395\n",
      "with hidden layer:  395\n",
      "removing:  (92, 56, 34)\n",
      "--- 45.30956697463989 seconds ---\n",
      "Epoch [1/500], Loss: 11.5306, Accuracy: 91.98 %\n",
      "Testing Accuracy: 44.12 %\n",
      "Test Loss:  97.73552870750427\n",
      "Epoch [2/500], Loss: 12.4830, Accuracy: 90.99 %\n",
      "Testing Accuracy: 44.04 %\n",
      "Test Loss:  100.0902099609375\n",
      "Epoch [3/500], Loss: 12.3537, Accuracy: 90.96 %\n",
      "Testing Accuracy: 44.29 %\n",
      "Test Loss:  102.12966871261597\n",
      "Epoch [4/500], Loss: 12.6195, Accuracy: 90.69 %\n",
      "Testing Accuracy: 43.84 %\n",
      "Test Loss:  99.16101837158203\n",
      "Epoch [5/500], Loss: 12.4540, Accuracy: 90.84 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  98.92254137992859\n",
      "Epoch [6/500], Loss: 12.8919, Accuracy: 90.50 %\n",
      "Testing Accuracy: 43.82 %\n",
      "Test Loss:  99.67816615104675\n",
      "Epoch [7/500], Loss: 12.4397, Accuracy: 90.99 %\n",
      "Testing Accuracy: 44.23 %\n",
      "Test Loss:  98.76304149627686\n",
      "Epoch [8/500], Loss: 12.2483, Accuracy: 91.25 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  102.77644228935242\n",
      "Epoch [9/500], Loss: 12.9211, Accuracy: 90.40 %\n",
      "Testing Accuracy: 44.31 %\n",
      "Test Loss:  98.55598139762878\n",
      "Epoch [10/500], Loss: 13.1569, Accuracy: 90.34 %\n",
      "Testing Accuracy: 44.12 %\n",
      "Test Loss:  98.70599842071533\n",
      "Epoch [11/500], Loss: 12.2562, Accuracy: 91.36 %\n",
      "Testing Accuracy: 44.85 %\n",
      "Test Loss:  100.75210547447205\n",
      "Epoch [12/500], Loss: 13.8181, Accuracy: 89.71 %\n",
      "Testing Accuracy: 43.87 %\n",
      "Test Loss:  96.31755995750427\n",
      "Epoch [13/500], Loss: 13.2200, Accuracy: 90.23 %\n",
      "Testing Accuracy: 44.04 %\n",
      "Test Loss:  97.19454538822174\n",
      "Epoch [14/500], Loss: 12.9796, Accuracy: 90.57 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  100.19542872905731\n",
      "Epoch [15/500], Loss: 13.9305, Accuracy: 89.50 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  101.44183373451233\n",
      "Epoch [16/500], Loss: 14.9025, Accuracy: 88.81 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  96.56056022644043\n",
      "Epoch [17/500], Loss: 13.4457, Accuracy: 90.20 %\n",
      "Testing Accuracy: 43.28 %\n",
      "Test Loss:  100.73705005645752\n",
      "Epoch [18/500], Loss: 13.7209, Accuracy: 89.96 %\n",
      "Testing Accuracy: 44.17 %\n",
      "Test Loss:  98.56000506877899\n",
      "Epoch [19/500], Loss: 13.7533, Accuracy: 89.94 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  99.81262564659119\n",
      "Epoch [20/500], Loss: 14.1948, Accuracy: 89.53 %\n",
      "Testing Accuracy: 43.36 %\n",
      "Test Loss:  100.68981528282166\n",
      "Epoch [21/500], Loss: 13.2178, Accuracy: 90.25 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  100.54895496368408\n",
      "Epoch [22/500], Loss: 13.4066, Accuracy: 90.02 %\n",
      "Testing Accuracy: 44.22 %\n",
      "Test Loss:  98.59602284431458\n",
      "Epoch [23/500], Loss: 12.5949, Accuracy: 90.77 %\n",
      "Testing Accuracy: 43.28 %\n",
      "Test Loss:  99.83765816688538\n",
      "Epoch [24/500], Loss: 12.7601, Accuracy: 90.67 %\n",
      "Testing Accuracy: 43.99 %\n",
      "Test Loss:  99.10551381111145\n",
      "Epoch [25/500], Loss: 13.5857, Accuracy: 89.95 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  101.68830370903015\n",
      "Epoch [26/500], Loss: 13.0342, Accuracy: 90.49 %\n",
      "Testing Accuracy: 43.08 %\n",
      "Test Loss:  100.91126132011414\n",
      "Epoch [27/500], Loss: 12.4090, Accuracy: 91.14 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  100.8303472995758\n",
      "Epoch [28/500], Loss: 12.5070, Accuracy: 90.92 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  99.37210607528687\n",
      "Epoch [29/500], Loss: 14.0311, Accuracy: 89.55 %\n",
      "Testing Accuracy: 44.37 %\n",
      "Test Loss:  99.32498145103455\n",
      "Epoch [30/500], Loss: 13.3313, Accuracy: 90.14 %\n",
      "Testing Accuracy: 43.85 %\n",
      "Test Loss:  98.87422728538513\n",
      "Epoch [31/500], Loss: 12.3833, Accuracy: 91.11 %\n",
      "Testing Accuracy: 43.62 %\n",
      "Test Loss:  101.35822749137878\n",
      "Epoch [32/500], Loss: 12.6180, Accuracy: 90.74 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  101.73708271980286\n",
      "Testing Accuracy: 43.73 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        802816   \n",
      "Net/Dropout[dropout]/onnx::Relu   784      \n",
      "Net/Linear[fc2]/onnx::Gemm        4312     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "807,912 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843]\n",
      "pruning hidden size:  392\n",
      "with hidden layer:  392\n",
      "removing:  (231, 328, 76)\n",
      "--- 45.41833019256592 seconds ---\n",
      "Epoch [1/500], Loss: 12.7271, Accuracy: 90.62 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  101.85511422157288\n",
      "Epoch [2/500], Loss: 12.9015, Accuracy: 90.76 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  101.15936708450317\n",
      "Epoch [3/500], Loss: 12.3418, Accuracy: 91.07 %\n",
      "Testing Accuracy: 44.09 %\n",
      "Test Loss:  99.23072504997253\n",
      "Epoch [4/500], Loss: 13.3716, Accuracy: 90.16 %\n",
      "Testing Accuracy: 44.01 %\n",
      "Test Loss:  97.91717863082886\n",
      "Epoch [5/500], Loss: 13.5392, Accuracy: 90.10 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  99.70293259620667\n",
      "Epoch [6/500], Loss: 15.6776, Accuracy: 88.06 %\n",
      "Testing Accuracy: 43.18 %\n",
      "Test Loss:  100.29746675491333\n",
      "Epoch [7/500], Loss: 14.1414, Accuracy: 89.61 %\n",
      "Testing Accuracy: 43.51 %\n",
      "Test Loss:  101.13498401641846\n",
      "Epoch [8/500], Loss: 13.2049, Accuracy: 90.39 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  100.44617056846619\n",
      "Epoch [9/500], Loss: 13.1449, Accuracy: 90.45 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  98.71779155731201\n",
      "Epoch [10/500], Loss: 13.0338, Accuracy: 90.48 %\n",
      "Testing Accuracy: 43.88 %\n",
      "Test Loss:  100.71532130241394\n",
      "Epoch [11/500], Loss: 12.6207, Accuracy: 90.97 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  101.85434532165527\n",
      "Epoch [12/500], Loss: 13.2627, Accuracy: 90.24 %\n",
      "Testing Accuracy: 44.40 %\n",
      "Test Loss:  99.87341022491455\n",
      "Epoch [13/500], Loss: 13.7167, Accuracy: 89.97 %\n",
      "Testing Accuracy: 43.32 %\n",
      "Test Loss:  100.39628791809082\n",
      "Epoch [14/500], Loss: 13.5551, Accuracy: 90.05 %\n",
      "Testing Accuracy: 44.03 %\n",
      "Test Loss:  100.1283073425293\n",
      "Epoch [15/500], Loss: 13.3593, Accuracy: 90.21 %\n",
      "Testing Accuracy: 43.37 %\n",
      "Test Loss:  98.99972748756409\n",
      "Epoch [16/500], Loss: 13.0415, Accuracy: 90.41 %\n",
      "Testing Accuracy: 43.28 %\n",
      "Test Loss:  98.11655688285828\n",
      "Epoch [17/500], Loss: 12.9065, Accuracy: 90.51 %\n",
      "Testing Accuracy: 44.15 %\n",
      "Test Loss:  100.4640634059906\n",
      "Epoch [18/500], Loss: 13.4084, Accuracy: 90.19 %\n",
      "Testing Accuracy: 44.00 %\n",
      "Test Loss:  99.18029737472534\n",
      "Epoch [19/500], Loss: 12.4174, Accuracy: 91.06 %\n",
      "Testing Accuracy: 43.63 %\n",
      "Test Loss:  100.59773540496826\n",
      "Epoch [20/500], Loss: 12.6359, Accuracy: 90.80 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  97.52434515953064\n",
      "Epoch [21/500], Loss: 12.6348, Accuracy: 91.03 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  101.24697875976562\n",
      "Epoch [22/500], Loss: 12.4532, Accuracy: 91.00 %\n",
      "Testing Accuracy: 43.67 %\n",
      "Test Loss:  99.51344776153564\n",
      "Epoch [23/500], Loss: 12.9988, Accuracy: 90.60 %\n",
      "Testing Accuracy: 44.12 %\n",
      "Test Loss:  100.1421947479248\n",
      "Epoch [24/500], Loss: 12.4498, Accuracy: 91.13 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  99.90465188026428\n",
      "Epoch [25/500], Loss: 13.0375, Accuracy: 90.53 %\n",
      "Testing Accuracy: 43.48 %\n",
      "Test Loss:  101.5623049736023\n",
      "Epoch [26/500], Loss: 12.6302, Accuracy: 90.78 %\n",
      "Testing Accuracy: 44.19 %\n",
      "Test Loss:  99.84815454483032\n",
      "Epoch [27/500], Loss: 12.8666, Accuracy: 90.72 %\n",
      "Testing Accuracy: 44.26 %\n",
      "Test Loss:  99.71711111068726\n",
      "Epoch [28/500], Loss: 12.1891, Accuracy: 91.44 %\n",
      "Testing Accuracy: 43.39 %\n",
      "Test Loss:  100.89114332199097\n",
      "Epoch [29/500], Loss: 12.7976, Accuracy: 90.69 %\n",
      "Testing Accuracy: 44.33 %\n",
      "Test Loss:  100.45134687423706\n",
      "Epoch [30/500], Loss: 13.0024, Accuracy: 90.46 %\n",
      "Testing Accuracy: 43.57 %\n",
      "Test Loss:  101.3023009300232\n",
      "Epoch [31/500], Loss: 12.7204, Accuracy: 90.86 %\n",
      "Testing Accuracy: 43.11 %\n",
      "Test Loss:  102.28228211402893\n",
      "Epoch [32/500], Loss: 13.0367, Accuracy: 90.52 %\n",
      "Testing Accuracy: 44.23 %\n",
      "Test Loss:  97.91461372375488\n",
      "Epoch [33/500], Loss: 12.5923, Accuracy: 90.94 %\n",
      "Testing Accuracy: 44.40 %\n",
      "Test Loss:  100.76339364051819\n",
      "Epoch [34/500], Loss: 12.7384, Accuracy: 90.75 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  99.3828341960907\n",
      "Epoch [35/500], Loss: 13.0165, Accuracy: 90.46 %\n",
      "Testing Accuracy: 43.37 %\n",
      "Test Loss:  99.35786819458008\n",
      "Epoch [36/500], Loss: 12.7181, Accuracy: 90.97 %\n",
      "Testing Accuracy: 44.25 %\n",
      "Test Loss:  102.77545523643494\n",
      "Epoch [37/500], Loss: 12.4092, Accuracy: 91.15 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  100.5431478023529\n",
      "Epoch [38/500], Loss: 12.5821, Accuracy: 90.92 %\n",
      "Testing Accuracy: 43.60 %\n",
      "Test Loss:  102.99304533004761\n",
      "Epoch [39/500], Loss: 12.3197, Accuracy: 91.09 %\n",
      "Testing Accuracy: 44.13 %\n",
      "Test Loss:  100.73024296760559\n",
      "Epoch [40/500], Loss: 12.2515, Accuracy: 91.18 %\n",
      "Testing Accuracy: 44.12 %\n",
      "Test Loss:  97.93460583686829\n",
      "Testing Accuracy: 44.12 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        796672   \n",
      "Net/Dropout[dropout]/onnx::Relu   778      \n",
      "Net/Linear[fc2]/onnx::Gemm        4279     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "801,729 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263]\n",
      "pruning hidden size:  389\n",
      "with hidden layer:  389\n",
      "removing:  (246, 147, 46)\n",
      "--- 45.40887451171875 seconds ---\n",
      "Epoch [1/500], Loss: 11.8193, Accuracy: 91.64 %\n",
      "Testing Accuracy: 43.65 %\n",
      "Test Loss:  100.67935538291931\n",
      "Epoch [2/500], Loss: 13.2984, Accuracy: 90.35 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  99.31791186332703\n",
      "Epoch [3/500], Loss: 13.4096, Accuracy: 90.21 %\n",
      "Testing Accuracy: 43.76 %\n",
      "Test Loss:  103.69243097305298\n",
      "Epoch [4/500], Loss: 13.5567, Accuracy: 90.22 %\n",
      "Testing Accuracy: 43.89 %\n",
      "Test Loss:  100.58859515190125\n",
      "Epoch [5/500], Loss: 13.4619, Accuracy: 90.14 %\n",
      "Testing Accuracy: 44.10 %\n",
      "Test Loss:  99.00019025802612\n",
      "Epoch [6/500], Loss: 13.4414, Accuracy: 90.16 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  100.73651361465454\n",
      "Epoch [7/500], Loss: 13.4528, Accuracy: 90.20 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  97.82568883895874\n",
      "Epoch [8/500], Loss: 13.2344, Accuracy: 90.25 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  101.73488116264343\n",
      "Epoch [9/500], Loss: 13.1567, Accuracy: 90.29 %\n",
      "Testing Accuracy: 44.56 %\n",
      "Test Loss:  101.82379817962646\n",
      "Epoch [10/500], Loss: 13.0456, Accuracy: 90.48 %\n",
      "Testing Accuracy: 44.23 %\n",
      "Test Loss:  101.19457626342773\n",
      "Epoch [11/500], Loss: 13.1901, Accuracy: 90.35 %\n",
      "Testing Accuracy: 43.49 %\n",
      "Test Loss:  99.59517788887024\n",
      "Epoch [12/500], Loss: 13.2827, Accuracy: 90.48 %\n",
      "Testing Accuracy: 43.57 %\n",
      "Test Loss:  97.24410080909729\n",
      "Epoch [13/500], Loss: 12.7247, Accuracy: 90.80 %\n",
      "Testing Accuracy: 44.37 %\n",
      "Test Loss:  99.61453628540039\n",
      "Epoch [14/500], Loss: 12.7595, Accuracy: 90.77 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  100.20458221435547\n",
      "Epoch [15/500], Loss: 13.1147, Accuracy: 90.41 %\n",
      "Testing Accuracy: 44.12 %\n",
      "Test Loss:  100.34943079948425\n",
      "Epoch [16/500], Loss: 13.5667, Accuracy: 90.15 %\n",
      "Testing Accuracy: 43.52 %\n",
      "Test Loss:  101.9918839931488\n",
      "Epoch [17/500], Loss: 13.2867, Accuracy: 90.22 %\n",
      "Testing Accuracy: 43.78 %\n",
      "Test Loss:  100.70221138000488\n",
      "Epoch [18/500], Loss: 12.7788, Accuracy: 90.85 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  99.20224213600159\n",
      "Epoch [19/500], Loss: 13.2356, Accuracy: 90.27 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  99.37102103233337\n",
      "Epoch [20/500], Loss: 14.0162, Accuracy: 89.41 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  98.86982917785645\n",
      "Epoch [21/500], Loss: 13.2246, Accuracy: 90.47 %\n",
      "Testing Accuracy: 43.50 %\n",
      "Test Loss:  100.53413701057434\n",
      "Epoch [22/500], Loss: 12.5446, Accuracy: 90.93 %\n",
      "Testing Accuracy: 44.00 %\n",
      "Test Loss:  99.52791166305542\n",
      "Epoch [23/500], Loss: 12.5389, Accuracy: 91.12 %\n",
      "Testing Accuracy: 43.49 %\n",
      "Test Loss:  99.91121435165405\n",
      "Epoch [24/500], Loss: 12.7457, Accuracy: 90.80 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  101.8047080039978\n",
      "Epoch [25/500], Loss: 13.2000, Accuracy: 90.53 %\n",
      "Testing Accuracy: 44.05 %\n",
      "Test Loss:  98.72552633285522\n",
      "Epoch [26/500], Loss: 12.9696, Accuracy: 90.66 %\n",
      "Testing Accuracy: 43.72 %\n",
      "Test Loss:  101.09746527671814\n",
      "Epoch [27/500], Loss: 13.2956, Accuracy: 90.25 %\n",
      "Testing Accuracy: 43.72 %\n",
      "Test Loss:  101.39214634895325\n",
      "Epoch [28/500], Loss: 13.9415, Accuracy: 89.63 %\n",
      "Testing Accuracy: 43.22 %\n",
      "Test Loss:  101.24821543693542\n",
      "Epoch [29/500], Loss: 14.0366, Accuracy: 89.54 %\n",
      "Testing Accuracy: 43.86 %\n",
      "Test Loss:  100.2195475101471\n",
      "Epoch [30/500], Loss: 13.4539, Accuracy: 90.15 %\n",
      "Testing Accuracy: 43.67 %\n",
      "Test Loss:  101.82719135284424\n",
      "Epoch [31/500], Loss: 13.1087, Accuracy: 90.35 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  101.85549759864807\n",
      "Epoch [32/500], Loss: 13.3706, Accuracy: 90.22 %\n",
      "Testing Accuracy: 44.19 %\n",
      "Test Loss:  99.82370138168335\n",
      "Testing Accuracy: 44.19 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        790528   \n",
      "Net/Dropout[dropout]/onnx::Relu   772      \n",
      "Net/Linear[fc2]/onnx::Gemm        4246     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "795,546 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291]\n",
      "pruning hidden size:  386\n",
      "with hidden layer:  386\n",
      "removing:  (367, 226, 214)\n",
      "--- 45.02285838127136 seconds ---\n",
      "Epoch [1/500], Loss: 14.2454, Accuracy: 89.66 %\n",
      "Testing Accuracy: 43.01 %\n",
      "Test Loss:  100.47618126869202\n",
      "Epoch [2/500], Loss: 13.9664, Accuracy: 89.82 %\n",
      "Testing Accuracy: 42.65 %\n",
      "Test Loss:  101.05215311050415\n",
      "Epoch [3/500], Loss: 15.7751, Accuracy: 88.09 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  97.86018347740173\n",
      "Epoch [4/500], Loss: 14.3982, Accuracy: 89.22 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  99.67403197288513\n",
      "Epoch [5/500], Loss: 14.2412, Accuracy: 89.47 %\n",
      "Testing Accuracy: 43.28 %\n",
      "Test Loss:  101.92588663101196\n",
      "Epoch [6/500], Loss: 15.7191, Accuracy: 88.24 %\n",
      "Testing Accuracy: 43.98 %\n",
      "Test Loss:  99.99080443382263\n",
      "Epoch [7/500], Loss: 14.6529, Accuracy: 89.05 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  103.4383499622345\n",
      "Epoch [8/500], Loss: 14.0614, Accuracy: 89.64 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  100.31533241271973\n",
      "Epoch [9/500], Loss: 14.1681, Accuracy: 89.61 %\n",
      "Testing Accuracy: 43.74 %\n",
      "Test Loss:  100.37349128723145\n",
      "Epoch [10/500], Loss: 14.5585, Accuracy: 89.18 %\n",
      "Testing Accuracy: 43.90 %\n",
      "Test Loss:  100.91651844978333\n",
      "Epoch [11/500], Loss: 13.5508, Accuracy: 90.15 %\n",
      "Testing Accuracy: 43.97 %\n",
      "Test Loss:  100.85583329200745\n",
      "Epoch [12/500], Loss: 13.8182, Accuracy: 89.87 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  100.13293147087097\n",
      "Epoch [13/500], Loss: 13.3279, Accuracy: 90.29 %\n",
      "Testing Accuracy: 44.12 %\n",
      "Test Loss:  99.93704795837402\n",
      "Epoch [14/500], Loss: 13.8721, Accuracy: 89.85 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  100.6648485660553\n",
      "Epoch [15/500], Loss: 14.1048, Accuracy: 89.68 %\n",
      "Testing Accuracy: 43.01 %\n",
      "Test Loss:  98.35078883171082\n",
      "Epoch [16/500], Loss: 14.5911, Accuracy: 89.08 %\n",
      "Testing Accuracy: 43.74 %\n",
      "Test Loss:  100.22715425491333\n",
      "Epoch [17/500], Loss: 13.7208, Accuracy: 89.95 %\n",
      "Testing Accuracy: 42.95 %\n",
      "Test Loss:  100.45513200759888\n",
      "Epoch [18/500], Loss: 14.8845, Accuracy: 88.84 %\n",
      "Testing Accuracy: 43.16 %\n",
      "Test Loss:  100.3284022808075\n",
      "Epoch [19/500], Loss: 14.5803, Accuracy: 89.25 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  97.7652518749237\n",
      "Epoch [20/500], Loss: 13.8131, Accuracy: 89.89 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  100.7880392074585\n",
      "Epoch [21/500], Loss: 13.8961, Accuracy: 89.84 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  99.21914076805115\n",
      "Epoch [22/500], Loss: 13.1874, Accuracy: 90.52 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  101.13114023208618\n",
      "Epoch [23/500], Loss: 13.7696, Accuracy: 89.92 %\n",
      "Testing Accuracy: 44.02 %\n",
      "Test Loss:  103.66884398460388\n",
      "Epoch [24/500], Loss: 13.6888, Accuracy: 89.88 %\n",
      "Testing Accuracy: 44.14 %\n",
      "Test Loss:  101.2678964138031\n",
      "Epoch [25/500], Loss: 13.4823, Accuracy: 90.15 %\n",
      "Testing Accuracy: 43.86 %\n",
      "Test Loss:  99.94502019882202\n",
      "Epoch [26/500], Loss: 13.7015, Accuracy: 90.03 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  100.02071499824524\n",
      "Epoch [27/500], Loss: 13.7263, Accuracy: 89.82 %\n",
      "Testing Accuracy: 43.78 %\n",
      "Test Loss:  100.72672748565674\n",
      "Epoch [28/500], Loss: 14.0646, Accuracy: 89.72 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  99.0635941028595\n",
      "Epoch [29/500], Loss: 13.1977, Accuracy: 90.42 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  101.51900911331177\n",
      "Epoch [30/500], Loss: 13.5796, Accuracy: 90.12 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  99.50096654891968\n",
      "Epoch [31/500], Loss: 12.9138, Accuracy: 90.80 %\n",
      "Testing Accuracy: 44.36 %\n",
      "Test Loss:  101.47196674346924\n",
      "Epoch [32/500], Loss: 13.2899, Accuracy: 90.23 %\n",
      "Testing Accuracy: 43.74 %\n",
      "Test Loss:  99.78057503700256\n",
      "Epoch [33/500], Loss: 13.0385, Accuracy: 90.63 %\n",
      "Testing Accuracy: 43.69 %\n",
      "Test Loss:  103.5717601776123\n",
      "Epoch [34/500], Loss: 13.4611, Accuracy: 90.12 %\n",
      "Testing Accuracy: 43.39 %\n",
      "Test Loss:  101.33144116401672\n",
      "Epoch [35/500], Loss: 14.2000, Accuracy: 89.55 %\n",
      "Testing Accuracy: 42.68 %\n",
      "Test Loss:  101.25944924354553\n",
      "Epoch [36/500], Loss: 13.3852, Accuracy: 90.40 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  100.1425895690918\n",
      "Epoch [37/500], Loss: 13.5560, Accuracy: 89.93 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  101.99439311027527\n",
      "Epoch [38/500], Loss: 13.5154, Accuracy: 90.10 %\n",
      "Testing Accuracy: 44.07 %\n",
      "Test Loss:  100.75681257247925\n",
      "Epoch [39/500], Loss: 13.7843, Accuracy: 89.92 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  100.77062559127808\n",
      "Testing Accuracy: 43.42 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        784384   \n",
      "Net/Dropout[dropout]/onnx::Relu   766      \n",
      "Net/Linear[fc2]/onnx::Gemm        4213     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "789,363 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215]\n",
      "pruning hidden size:  383\n",
      "with hidden layer:  383\n",
      "removing:  (140, 273, 24)\n",
      "--- 43.08110523223877 seconds ---\n",
      "Epoch [1/500], Loss: 12.5976, Accuracy: 90.94 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  101.06533026695251\n",
      "Epoch [2/500], Loss: 13.1691, Accuracy: 90.47 %\n",
      "Testing Accuracy: 43.63 %\n",
      "Test Loss:  102.24638056755066\n",
      "Epoch [3/500], Loss: 13.8091, Accuracy: 89.66 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  98.27493095397949\n",
      "Epoch [4/500], Loss: 13.3530, Accuracy: 90.25 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  100.62365341186523\n",
      "Epoch [5/500], Loss: 13.3660, Accuracy: 90.28 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  100.63100051879883\n",
      "Epoch [6/500], Loss: 13.9735, Accuracy: 89.73 %\n",
      "Testing Accuracy: 43.68 %\n",
      "Test Loss:  101.98109722137451\n",
      "Epoch [7/500], Loss: 13.2494, Accuracy: 90.43 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  102.69998002052307\n",
      "Epoch [8/500], Loss: 14.3326, Accuracy: 89.39 %\n",
      "Testing Accuracy: 43.03 %\n",
      "Test Loss:  101.4828851222992\n",
      "Epoch [9/500], Loss: 13.9105, Accuracy: 89.75 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  98.94260001182556\n",
      "Epoch [10/500], Loss: 14.4508, Accuracy: 89.35 %\n",
      "Testing Accuracy: 43.39 %\n",
      "Test Loss:  101.81630516052246\n",
      "Epoch [11/500], Loss: 14.7548, Accuracy: 88.96 %\n",
      "Testing Accuracy: 42.83 %\n",
      "Test Loss:  102.04150772094727\n",
      "Epoch [12/500], Loss: 14.4855, Accuracy: 89.25 %\n",
      "Testing Accuracy: 43.57 %\n",
      "Test Loss:  100.7357622385025\n",
      "Epoch [13/500], Loss: 13.7462, Accuracy: 89.83 %\n",
      "Testing Accuracy: 44.37 %\n",
      "Test Loss:  101.96266150474548\n",
      "Epoch [14/500], Loss: 13.4693, Accuracy: 90.23 %\n",
      "Testing Accuracy: 43.03 %\n",
      "Test Loss:  101.06203079223633\n",
      "Epoch [15/500], Loss: 13.9199, Accuracy: 89.81 %\n",
      "Testing Accuracy: 43.39 %\n",
      "Test Loss:  100.36078190803528\n",
      "Epoch [16/500], Loss: 14.4396, Accuracy: 89.39 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  97.89037013053894\n",
      "Epoch [17/500], Loss: 13.2561, Accuracy: 90.24 %\n",
      "Testing Accuracy: 43.88 %\n",
      "Test Loss:  103.86066913604736\n",
      "Epoch [18/500], Loss: 13.0695, Accuracy: 90.47 %\n",
      "Testing Accuracy: 44.16 %\n",
      "Test Loss:  101.211266040802\n",
      "Epoch [19/500], Loss: 12.8959, Accuracy: 90.62 %\n",
      "Testing Accuracy: 43.79 %\n",
      "Test Loss:  101.68046021461487\n",
      "Epoch [20/500], Loss: 13.0538, Accuracy: 90.56 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  102.1462390422821\n",
      "Epoch [21/500], Loss: 13.2762, Accuracy: 90.34 %\n",
      "Testing Accuracy: 43.72 %\n",
      "Test Loss:  99.66960525512695\n",
      "Epoch [22/500], Loss: 12.6385, Accuracy: 90.90 %\n",
      "Testing Accuracy: 43.29 %\n",
      "Test Loss:  100.6940245628357\n",
      "Epoch [23/500], Loss: 14.1158, Accuracy: 89.61 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  100.88616943359375\n",
      "Epoch [24/500], Loss: 13.8241, Accuracy: 89.60 %\n",
      "Testing Accuracy: 43.79 %\n",
      "Test Loss:  102.25711226463318\n",
      "Epoch [25/500], Loss: 13.2994, Accuracy: 90.34 %\n",
      "Testing Accuracy: 43.36 %\n",
      "Test Loss:  98.74373745918274\n",
      "Epoch [26/500], Loss: 13.2154, Accuracy: 90.42 %\n",
      "Testing Accuracy: 43.98 %\n",
      "Test Loss:  103.2621660232544\n",
      "Epoch [27/500], Loss: 14.6037, Accuracy: 89.27 %\n",
      "Testing Accuracy: 43.76 %\n",
      "Test Loss:  102.00500535964966\n",
      "Epoch [28/500], Loss: 13.9270, Accuracy: 89.70 %\n",
      "Testing Accuracy: 43.34 %\n",
      "Test Loss:  102.2179491519928\n",
      "Epoch [29/500], Loss: 14.3978, Accuracy: 89.12 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  97.76630079746246\n",
      "Epoch [30/500], Loss: 14.4294, Accuracy: 89.25 %\n",
      "Testing Accuracy: 44.29 %\n",
      "Test Loss:  101.64893960952759\n",
      "Epoch [31/500], Loss: 13.2772, Accuracy: 90.23 %\n",
      "Testing Accuracy: 44.06 %\n",
      "Test Loss:  101.04932570457458\n",
      "Epoch [32/500], Loss: 13.7353, Accuracy: 89.95 %\n",
      "Testing Accuracy: 43.57 %\n",
      "Test Loss:  100.00523543357849\n",
      "Epoch [33/500], Loss: 13.6731, Accuracy: 90.01 %\n",
      "Testing Accuracy: 42.64 %\n",
      "Test Loss:  103.1970899105072\n",
      "Epoch [34/500], Loss: 13.8474, Accuracy: 89.86 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  98.92311215400696\n",
      "Epoch [35/500], Loss: 13.8414, Accuracy: 89.77 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  102.0907130241394\n",
      "Epoch [36/500], Loss: 13.8089, Accuracy: 89.83 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  100.63638710975647\n",
      "Epoch [37/500], Loss: 13.7601, Accuracy: 89.72 %\n",
      "Testing Accuracy: 43.13 %\n",
      "Test Loss:  103.82924222946167\n",
      "Epoch [38/500], Loss: 13.9161, Accuracy: 89.70 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  100.32763910293579\n",
      "Epoch [39/500], Loss: 13.6235, Accuracy: 90.04 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  100.77537846565247\n",
      "Epoch [40/500], Loss: 12.8811, Accuracy: 90.68 %\n",
      "Testing Accuracy: 43.36 %\n",
      "Test Loss:  101.8647301197052\n",
      "Epoch [41/500], Loss: 12.9611, Accuracy: 90.83 %\n",
      "Testing Accuracy: 43.39 %\n",
      "Test Loss:  99.56275391578674\n",
      "Epoch [42/500], Loss: 12.7864, Accuracy: 90.68 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  98.79105234146118\n",
      "Epoch [43/500], Loss: 13.4371, Accuracy: 90.16 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  103.44944143295288\n",
      "Epoch [44/500], Loss: 13.5150, Accuracy: 90.17 %\n",
      "Testing Accuracy: 43.86 %\n",
      "Test Loss:  102.54607486724854\n",
      "Epoch [45/500], Loss: 13.8589, Accuracy: 89.85 %\n",
      "Testing Accuracy: 43.90 %\n",
      "Test Loss:  101.01046657562256\n",
      "Epoch [46/500], Loss: 13.1982, Accuracy: 90.40 %\n",
      "Testing Accuracy: 43.51 %\n",
      "Test Loss:  100.97157454490662\n",
      "Epoch [47/500], Loss: 13.2718, Accuracy: 90.31 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  102.7848014831543\n",
      "Epoch [48/500], Loss: 13.0963, Accuracy: 90.38 %\n",
      "Testing Accuracy: 43.33 %\n",
      "Test Loss:  98.9455668926239\n",
      "Epoch [49/500], Loss: 13.3359, Accuracy: 90.20 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  101.73155665397644\n",
      "Testing Accuracy: 43.75 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        778240   \n",
      "Net/Dropout[dropout]/onnx::Relu   760      \n",
      "Net/Linear[fc2]/onnx::Gemm        4180     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "783,180 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214]\n",
      "pruning hidden size:  380\n",
      "with hidden layer:  380\n",
      "removing:  (246, 198, 34)\n",
      "--- 42.342267990112305 seconds ---\n",
      "Epoch [1/500], Loss: 13.1319, Accuracy: 90.54 %\n",
      "Testing Accuracy: 42.79 %\n",
      "Test Loss:  100.84372544288635\n",
      "Epoch [2/500], Loss: 14.3318, Accuracy: 89.28 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  102.1899025440216\n",
      "Epoch [3/500], Loss: 14.2772, Accuracy: 89.33 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  102.01772832870483\n",
      "Epoch [4/500], Loss: 13.6291, Accuracy: 90.00 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  101.1345763206482\n",
      "Epoch [5/500], Loss: 13.6444, Accuracy: 89.83 %\n",
      "Testing Accuracy: 43.85 %\n",
      "Test Loss:  101.14790296554565\n",
      "Epoch [6/500], Loss: 14.1274, Accuracy: 89.59 %\n",
      "Testing Accuracy: 43.11 %\n",
      "Test Loss:  99.45184206962585\n",
      "Epoch [7/500], Loss: 14.5741, Accuracy: 89.15 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  102.2202787399292\n",
      "Epoch [8/500], Loss: 14.3058, Accuracy: 89.54 %\n",
      "Testing Accuracy: 42.72 %\n",
      "Test Loss:  104.89907455444336\n",
      "Epoch [9/500], Loss: 13.2692, Accuracy: 90.36 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  102.35540914535522\n",
      "Epoch [10/500], Loss: 13.1451, Accuracy: 90.29 %\n",
      "Testing Accuracy: 43.85 %\n",
      "Test Loss:  100.8283224105835\n",
      "Epoch [11/500], Loss: 12.5686, Accuracy: 90.99 %\n",
      "Testing Accuracy: 43.14 %\n",
      "Test Loss:  102.64325308799744\n",
      "Epoch [12/500], Loss: 13.2073, Accuracy: 90.46 %\n",
      "Testing Accuracy: 43.84 %\n",
      "Test Loss:  99.16844844818115\n",
      "Epoch [13/500], Loss: 13.5850, Accuracy: 90.12 %\n",
      "Testing Accuracy: 43.91 %\n",
      "Test Loss:  101.43543004989624\n",
      "Epoch [14/500], Loss: 13.4811, Accuracy: 90.08 %\n",
      "Testing Accuracy: 43.68 %\n",
      "Test Loss:  101.76508140563965\n",
      "Epoch [15/500], Loss: 13.9864, Accuracy: 89.50 %\n",
      "Testing Accuracy: 42.95 %\n",
      "Test Loss:  100.0873818397522\n",
      "Epoch [16/500], Loss: 13.6370, Accuracy: 89.99 %\n",
      "Testing Accuracy: 44.16 %\n",
      "Test Loss:  103.67374777793884\n",
      "Epoch [17/500], Loss: 13.5823, Accuracy: 90.08 %\n",
      "Testing Accuracy: 43.66 %\n",
      "Test Loss:  100.36805319786072\n",
      "Epoch [18/500], Loss: 13.7220, Accuracy: 89.89 %\n",
      "Testing Accuracy: 43.34 %\n",
      "Test Loss:  104.06326675415039\n",
      "Epoch [19/500], Loss: 13.9959, Accuracy: 89.74 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  101.03571152687073\n",
      "Epoch [20/500], Loss: 13.5762, Accuracy: 89.95 %\n",
      "Testing Accuracy: 43.85 %\n",
      "Test Loss:  102.48564720153809\n",
      "Epoch [21/500], Loss: 14.2458, Accuracy: 89.40 %\n",
      "Testing Accuracy: 44.41 %\n",
      "Test Loss:  103.45632863044739\n",
      "Epoch [22/500], Loss: 13.9110, Accuracy: 89.73 %\n",
      "Testing Accuracy: 43.28 %\n",
      "Test Loss:  102.42248225212097\n",
      "Epoch [23/500], Loss: 13.5609, Accuracy: 89.86 %\n",
      "Testing Accuracy: 42.66 %\n",
      "Test Loss:  101.45346093177795\n",
      "Epoch [24/500], Loss: 13.1554, Accuracy: 90.48 %\n",
      "Testing Accuracy: 43.36 %\n",
      "Test Loss:  101.06489038467407\n",
      "Epoch [25/500], Loss: 13.1017, Accuracy: 90.39 %\n",
      "Testing Accuracy: 43.07 %\n",
      "Test Loss:  101.6481442451477\n",
      "Epoch [26/500], Loss: 13.6087, Accuracy: 90.00 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  103.26947236061096\n",
      "Epoch [27/500], Loss: 13.3497, Accuracy: 90.35 %\n",
      "Testing Accuracy: 43.92 %\n",
      "Test Loss:  100.08188652992249\n",
      "Epoch [28/500], Loss: 13.3578, Accuracy: 90.16 %\n",
      "Testing Accuracy: 43.58 %\n",
      "Test Loss:  102.69403982162476\n",
      "Epoch [29/500], Loss: 13.0513, Accuracy: 90.46 %\n",
      "Testing Accuracy: 44.01 %\n",
      "Test Loss:  101.8450927734375\n",
      "Epoch [30/500], Loss: 14.4395, Accuracy: 89.04 %\n",
      "Testing Accuracy: 43.13 %\n",
      "Test Loss:  100.88319683074951\n",
      "Epoch [31/500], Loss: 14.0461, Accuracy: 89.60 %\n",
      "Testing Accuracy: 43.39 %\n",
      "Test Loss:  99.47442770004272\n",
      "Epoch [32/500], Loss: 13.2555, Accuracy: 90.22 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  100.94596481323242\n",
      "Testing Accuracy: 43.61 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        772096   \n",
      "Net/Dropout[dropout]/onnx::Relu   754      \n",
      "Net/Linear[fc2]/onnx::Gemm        4147     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "776,997 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075]\n",
      "pruning hidden size:  377\n",
      "with hidden layer:  377\n",
      "removing:  (235, 232, 121)\n",
      "--- 42.179239988327026 seconds ---\n",
      "Epoch [1/500], Loss: 13.0669, Accuracy: 90.47 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  102.47034931182861\n",
      "Epoch [2/500], Loss: 14.1999, Accuracy: 89.39 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  103.24590182304382\n",
      "Epoch [3/500], Loss: 13.6668, Accuracy: 89.92 %\n",
      "Testing Accuracy: 43.51 %\n",
      "Test Loss:  101.7477457523346\n",
      "Epoch [4/500], Loss: 13.0236, Accuracy: 90.48 %\n",
      "Testing Accuracy: 43.18 %\n",
      "Test Loss:  104.50394010543823\n",
      "Epoch [5/500], Loss: 13.2139, Accuracy: 90.34 %\n",
      "Testing Accuracy: 43.70 %\n",
      "Test Loss:  105.18250322341919\n",
      "Epoch [6/500], Loss: 14.3966, Accuracy: 89.29 %\n",
      "Testing Accuracy: 43.47 %\n",
      "Test Loss:  100.89891743659973\n",
      "Epoch [7/500], Loss: 13.1832, Accuracy: 90.33 %\n",
      "Testing Accuracy: 42.87 %\n",
      "Test Loss:  102.50471305847168\n",
      "Epoch [8/500], Loss: 13.3407, Accuracy: 90.33 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  101.58050751686096\n",
      "Epoch [9/500], Loss: 12.9677, Accuracy: 90.49 %\n",
      "Testing Accuracy: 43.42 %\n",
      "Test Loss:  103.90350651741028\n",
      "Epoch [10/500], Loss: 13.9358, Accuracy: 89.55 %\n",
      "Testing Accuracy: 43.89 %\n",
      "Test Loss:  102.50782227516174\n",
      "Epoch [11/500], Loss: 13.3420, Accuracy: 90.27 %\n",
      "Testing Accuracy: 43.48 %\n",
      "Test Loss:  101.64597988128662\n",
      "Epoch [12/500], Loss: 13.8447, Accuracy: 89.94 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  101.10947012901306\n",
      "Epoch [13/500], Loss: 13.5598, Accuracy: 90.10 %\n",
      "Testing Accuracy: 43.33 %\n",
      "Test Loss:  102.38466668128967\n",
      "Epoch [14/500], Loss: 13.5314, Accuracy: 90.07 %\n",
      "Testing Accuracy: 42.95 %\n",
      "Test Loss:  101.88837504386902\n",
      "Epoch [15/500], Loss: 13.0614, Accuracy: 90.52 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  102.1477324962616\n",
      "Epoch [16/500], Loss: 13.9375, Accuracy: 89.75 %\n",
      "Testing Accuracy: 43.47 %\n",
      "Test Loss:  102.40326189994812\n",
      "Epoch [17/500], Loss: 13.5190, Accuracy: 90.12 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  101.98926830291748\n",
      "Epoch [18/500], Loss: 12.9433, Accuracy: 90.67 %\n",
      "Testing Accuracy: 43.65 %\n",
      "Test Loss:  100.09769797325134\n",
      "Epoch [19/500], Loss: 13.7312, Accuracy: 89.86 %\n",
      "Testing Accuracy: 44.07 %\n",
      "Test Loss:  103.66822957992554\n",
      "Epoch [20/500], Loss: 13.8564, Accuracy: 89.73 %\n",
      "Testing Accuracy: 42.93 %\n",
      "Test Loss:  102.95539927482605\n",
      "Epoch [21/500], Loss: 13.5801, Accuracy: 90.07 %\n",
      "Testing Accuracy: 43.26 %\n",
      "Test Loss:  99.83484649658203\n",
      "Epoch [22/500], Loss: 13.3867, Accuracy: 90.27 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  101.46761870384216\n",
      "Epoch [23/500], Loss: 13.3143, Accuracy: 90.33 %\n",
      "Testing Accuracy: 43.64 %\n",
      "Test Loss:  98.90337467193604\n",
      "Epoch [24/500], Loss: 14.5470, Accuracy: 89.16 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  102.21443009376526\n",
      "Epoch [25/500], Loss: 13.9106, Accuracy: 89.89 %\n",
      "Testing Accuracy: 44.14 %\n",
      "Test Loss:  101.80384469032288\n",
      "Epoch [26/500], Loss: 13.0018, Accuracy: 90.58 %\n",
      "Testing Accuracy: 43.99 %\n",
      "Test Loss:  101.35777258872986\n",
      "Epoch [27/500], Loss: 13.2551, Accuracy: 90.23 %\n",
      "Testing Accuracy: 43.74 %\n",
      "Test Loss:  99.47019982337952\n",
      "Epoch [28/500], Loss: 12.7508, Accuracy: 90.84 %\n",
      "Testing Accuracy: 43.68 %\n",
      "Test Loss:  101.6192078590393\n",
      "Epoch [29/500], Loss: 12.9212, Accuracy: 90.69 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  104.29520201683044\n",
      "Epoch [30/500], Loss: 13.2970, Accuracy: 90.38 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  102.18712949752808\n",
      "Epoch [31/500], Loss: 12.8455, Accuracy: 90.61 %\n",
      "Testing Accuracy: 44.18 %\n",
      "Test Loss:  101.53851985931396\n",
      "Epoch [32/500], Loss: 12.8291, Accuracy: 90.70 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  103.28430032730103\n",
      "Epoch [33/500], Loss: 13.7368, Accuracy: 89.81 %\n",
      "Testing Accuracy: 43.20 %\n",
      "Test Loss:  100.6451325416565\n",
      "Epoch [34/500], Loss: 14.3842, Accuracy: 89.12 %\n",
      "Testing Accuracy: 43.31 %\n",
      "Test Loss:  101.96848440170288\n",
      "Epoch [35/500], Loss: 12.9115, Accuracy: 90.59 %\n",
      "Testing Accuracy: 44.01 %\n",
      "Test Loss:  101.84755110740662\n",
      "Epoch [36/500], Loss: 13.2408, Accuracy: 90.28 %\n",
      "Testing Accuracy: 43.65 %\n",
      "Test Loss:  101.18968534469604\n",
      "Epoch [37/500], Loss: 13.7788, Accuracy: 89.87 %\n",
      "Testing Accuracy: 43.95 %\n",
      "Test Loss:  103.16370606422424\n",
      "Epoch [38/500], Loss: 13.0553, Accuracy: 90.50 %\n",
      "Testing Accuracy: 43.77 %\n",
      "Test Loss:  103.94344282150269\n",
      "Epoch [39/500], Loss: 12.9159, Accuracy: 90.51 %\n",
      "Testing Accuracy: 43.48 %\n",
      "Test Loss:  102.31030368804932\n",
      "Epoch [40/500], Loss: 13.5391, Accuracy: 90.02 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  103.32249784469604\n",
      "Epoch [41/500], Loss: 13.7853, Accuracy: 89.86 %\n",
      "Testing Accuracy: 42.38 %\n",
      "Test Loss:  103.25401782989502\n",
      "Epoch [42/500], Loss: 13.3125, Accuracy: 90.24 %\n",
      "Testing Accuracy: 42.85 %\n",
      "Test Loss:  103.04299664497375\n",
      "Epoch [43/500], Loss: 13.1555, Accuracy: 90.35 %\n",
      "Testing Accuracy: 43.99 %\n",
      "Test Loss:  102.87238240242004\n",
      "Testing Accuracy: 43.99 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        765952   \n",
      "Net/Dropout[dropout]/onnx::Relu   748      \n",
      "Net/Linear[fc2]/onnx::Gemm        4114     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "770,814 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124]\n",
      "pruning hidden size:  374\n",
      "with hidden layer:  374\n",
      "removing:  (253, 285, 208)\n",
      "--- 40.802165269851685 seconds ---\n",
      "Epoch [1/500], Loss: 12.6171, Accuracy: 90.75 %\n",
      "Testing Accuracy: 43.69 %\n",
      "Test Loss:  102.77157783508301\n",
      "Epoch [2/500], Loss: 13.2792, Accuracy: 90.28 %\n",
      "Testing Accuracy: 44.04 %\n",
      "Test Loss:  100.46866393089294\n",
      "Epoch [3/500], Loss: 12.7994, Accuracy: 90.65 %\n",
      "Testing Accuracy: 43.03 %\n",
      "Test Loss:  101.7378945350647\n",
      "Epoch [4/500], Loss: 13.5765, Accuracy: 89.93 %\n",
      "Testing Accuracy: 43.18 %\n",
      "Test Loss:  101.07757043838501\n",
      "Epoch [5/500], Loss: 13.1917, Accuracy: 90.42 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  104.31921315193176\n",
      "Epoch [6/500], Loss: 12.6012, Accuracy: 90.96 %\n",
      "Testing Accuracy: 43.68 %\n",
      "Test Loss:  102.08414196968079\n",
      "Epoch [7/500], Loss: 12.7273, Accuracy: 90.95 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  101.25539088249207\n",
      "Epoch [8/500], Loss: 13.5680, Accuracy: 90.03 %\n",
      "Testing Accuracy: 43.58 %\n",
      "Test Loss:  102.45488977432251\n",
      "Epoch [9/500], Loss: 12.8769, Accuracy: 90.44 %\n",
      "Testing Accuracy: 43.89 %\n",
      "Test Loss:  101.49153542518616\n",
      "Epoch [10/500], Loss: 13.1732, Accuracy: 90.39 %\n",
      "Testing Accuracy: 42.97 %\n",
      "Test Loss:  104.97818040847778\n",
      "Epoch [11/500], Loss: 13.1579, Accuracy: 90.40 %\n",
      "Testing Accuracy: 43.47 %\n",
      "Test Loss:  103.37951231002808\n",
      "Epoch [12/500], Loss: 12.5441, Accuracy: 91.04 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  104.0715229511261\n",
      "Epoch [13/500], Loss: 13.5330, Accuracy: 89.92 %\n",
      "Testing Accuracy: 44.54 %\n",
      "Test Loss:  100.75336933135986\n",
      "Epoch [14/500], Loss: 13.1521, Accuracy: 90.32 %\n",
      "Testing Accuracy: 43.88 %\n",
      "Test Loss:  102.67595529556274\n",
      "Epoch [15/500], Loss: 13.2979, Accuracy: 90.19 %\n",
      "Testing Accuracy: 43.90 %\n",
      "Test Loss:  104.30127167701721\n",
      "Epoch [16/500], Loss: 13.6839, Accuracy: 89.97 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  101.67717289924622\n",
      "Epoch [17/500], Loss: 13.0711, Accuracy: 90.38 %\n",
      "Testing Accuracy: 43.69 %\n",
      "Test Loss:  100.82583332061768\n",
      "Epoch [18/500], Loss: 12.9411, Accuracy: 90.69 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  103.22606205940247\n",
      "Epoch [19/500], Loss: 12.3194, Accuracy: 91.24 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  102.296306848526\n",
      "Epoch [20/500], Loss: 12.7358, Accuracy: 90.82 %\n",
      "Testing Accuracy: 43.57 %\n",
      "Test Loss:  104.03177189826965\n",
      "Epoch [21/500], Loss: 12.8116, Accuracy: 90.67 %\n",
      "Testing Accuracy: 43.73 %\n",
      "Test Loss:  104.8604063987732\n",
      "Epoch [22/500], Loss: 12.5186, Accuracy: 90.95 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  103.0058262348175\n",
      "Testing Accuracy: 43.30 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        759808   \n",
      "Net/Dropout[dropout]/onnx::Relu   742      \n",
      "Net/Linear[fc2]/onnx::Gemm        4081     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "764,631 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946]\n",
      "pruning hidden size:  371\n",
      "with hidden layer:  371\n",
      "removing:  (250, 230, 24)\n",
      "--- 40.69378089904785 seconds ---\n",
      "Epoch [1/500], Loss: 12.2813, Accuracy: 91.19 %\n",
      "Testing Accuracy: 43.76 %\n",
      "Test Loss:  103.06213760375977\n",
      "Epoch [2/500], Loss: 12.3365, Accuracy: 91.08 %\n",
      "Testing Accuracy: 44.27 %\n",
      "Test Loss:  102.36032295227051\n",
      "Epoch [3/500], Loss: 13.7782, Accuracy: 89.88 %\n",
      "Testing Accuracy: 44.08 %\n",
      "Test Loss:  103.8989474773407\n",
      "Epoch [4/500], Loss: 13.2838, Accuracy: 90.35 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  99.96818232536316\n",
      "Epoch [5/500], Loss: 12.8798, Accuracy: 90.54 %\n",
      "Testing Accuracy: 43.18 %\n",
      "Test Loss:  103.76879239082336\n",
      "Epoch [6/500], Loss: 13.5082, Accuracy: 90.13 %\n",
      "Testing Accuracy: 43.69 %\n",
      "Test Loss:  105.06878185272217\n",
      "Epoch [7/500], Loss: 13.3962, Accuracy: 90.07 %\n",
      "Testing Accuracy: 43.66 %\n",
      "Test Loss:  104.69647121429443\n",
      "Epoch [8/500], Loss: 12.7711, Accuracy: 90.75 %\n",
      "Testing Accuracy: 43.67 %\n",
      "Test Loss:  104.39402461051941\n",
      "Epoch [9/500], Loss: 12.8686, Accuracy: 90.59 %\n",
      "Testing Accuracy: 44.06 %\n",
      "Test Loss:  101.51005411148071\n",
      "Epoch [10/500], Loss: 13.3331, Accuracy: 90.14 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  102.94299960136414\n",
      "Epoch [11/500], Loss: 14.4339, Accuracy: 89.23 %\n",
      "Testing Accuracy: 44.11 %\n",
      "Test Loss:  104.1860740184784\n",
      "Epoch [12/500], Loss: 14.1862, Accuracy: 89.43 %\n",
      "Testing Accuracy: 43.18 %\n",
      "Test Loss:  103.30595541000366\n",
      "Epoch [13/500], Loss: 13.7435, Accuracy: 90.03 %\n",
      "Testing Accuracy: 43.49 %\n",
      "Test Loss:  101.11299991607666\n",
      "Epoch [14/500], Loss: 14.1215, Accuracy: 89.55 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  104.03054738044739\n",
      "Epoch [15/500], Loss: 15.8059, Accuracy: 88.21 %\n",
      "Testing Accuracy: 42.98 %\n",
      "Test Loss:  101.76801323890686\n",
      "Epoch [16/500], Loss: 15.0316, Accuracy: 88.70 %\n",
      "Testing Accuracy: 43.18 %\n",
      "Test Loss:  104.48301315307617\n",
      "Epoch [17/500], Loss: 14.6919, Accuracy: 89.01 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  101.74321961402893\n",
      "Epoch [18/500], Loss: 13.8104, Accuracy: 89.69 %\n",
      "Testing Accuracy: 43.52 %\n",
      "Test Loss:  102.67524075508118\n",
      "Epoch [19/500], Loss: 14.5921, Accuracy: 89.12 %\n",
      "Testing Accuracy: 42.89 %\n",
      "Test Loss:  105.34821653366089\n",
      "Epoch [20/500], Loss: 16.2450, Accuracy: 87.72 %\n",
      "Testing Accuracy: 42.04 %\n",
      "Test Loss:  101.51278924942017\n",
      "Epoch [21/500], Loss: 16.5119, Accuracy: 87.36 %\n",
      "Testing Accuracy: 43.14 %\n",
      "Test Loss:  101.3042585849762\n",
      "Epoch [22/500], Loss: 16.7361, Accuracy: 87.34 %\n",
      "Testing Accuracy: 42.87 %\n",
      "Test Loss:  100.73533487319946\n",
      "Epoch [23/500], Loss: 15.6592, Accuracy: 88.14 %\n",
      "Testing Accuracy: 43.37 %\n",
      "Test Loss:  100.81265330314636\n",
      "Epoch [24/500], Loss: 15.3218, Accuracy: 88.48 %\n",
      "Testing Accuracy: 43.34 %\n",
      "Test Loss:  101.72486972808838\n",
      "Testing Accuracy: 43.34 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        753664   \n",
      "Net/Dropout[dropout]/onnx::Relu   736      \n",
      "Net/Linear[fc2]/onnx::Gemm        4048     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "758,448 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337]\n",
      "pruning hidden size:  368\n",
      "with hidden layer:  368\n",
      "removing:  (36, 360, 24)\n",
      "--- 39.52749180793762 seconds ---\n",
      "Epoch [1/500], Loss: 14.7311, Accuracy: 89.10 %\n",
      "Testing Accuracy: 42.46 %\n",
      "Test Loss:  100.91946458816528\n",
      "Epoch [2/500], Loss: 16.3128, Accuracy: 87.76 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  100.29467296600342\n",
      "Epoch [3/500], Loss: 15.2436, Accuracy: 88.56 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  100.18262887001038\n",
      "Epoch [4/500], Loss: 16.1622, Accuracy: 87.72 %\n",
      "Testing Accuracy: 43.52 %\n",
      "Test Loss:  99.3297986984253\n",
      "Epoch [5/500], Loss: 15.8195, Accuracy: 87.97 %\n",
      "Testing Accuracy: 42.89 %\n",
      "Test Loss:  100.69971299171448\n",
      "Epoch [6/500], Loss: 15.8258, Accuracy: 88.04 %\n",
      "Testing Accuracy: 43.07 %\n",
      "Test Loss:  98.0470644235611\n",
      "Epoch [7/500], Loss: 15.3615, Accuracy: 88.51 %\n",
      "Testing Accuracy: 42.99 %\n",
      "Test Loss:  101.36463618278503\n",
      "Epoch [8/500], Loss: 16.1223, Accuracy: 87.57 %\n",
      "Testing Accuracy: 43.01 %\n",
      "Test Loss:  100.6780035495758\n",
      "Epoch [9/500], Loss: 15.5946, Accuracy: 88.32 %\n",
      "Testing Accuracy: 43.50 %\n",
      "Test Loss:  103.64081597328186\n",
      "Epoch [10/500], Loss: 16.0024, Accuracy: 87.81 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  100.42183256149292\n",
      "Epoch [11/500], Loss: 15.6539, Accuracy: 88.16 %\n",
      "Testing Accuracy: 42.76 %\n",
      "Test Loss:  100.64697289466858\n",
      "Epoch [12/500], Loss: 16.2095, Accuracy: 87.75 %\n",
      "Testing Accuracy: 42.87 %\n",
      "Test Loss:  101.86407995223999\n",
      "Epoch [13/500], Loss: 15.5672, Accuracy: 88.24 %\n",
      "Testing Accuracy: 42.52 %\n",
      "Test Loss:  101.07843446731567\n",
      "Epoch [14/500], Loss: 15.9071, Accuracy: 88.06 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  102.37199234962463\n",
      "Epoch [15/500], Loss: 17.3559, Accuracy: 86.70 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  100.39413356781006\n",
      "Epoch [16/500], Loss: 16.9208, Accuracy: 87.08 %\n",
      "Testing Accuracy: 43.07 %\n",
      "Test Loss:  99.86490797996521\n",
      "Epoch [17/500], Loss: 16.5690, Accuracy: 87.16 %\n",
      "Testing Accuracy: 43.86 %\n",
      "Test Loss:  99.71628928184509\n",
      "Epoch [18/500], Loss: 15.9582, Accuracy: 88.04 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  102.97456884384155\n",
      "Epoch [19/500], Loss: 15.6068, Accuracy: 88.29 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  100.76310682296753\n",
      "Epoch [20/500], Loss: 16.9602, Accuracy: 87.12 %\n",
      "Testing Accuracy: 42.90 %\n",
      "Test Loss:  101.53250527381897\n",
      "Epoch [21/500], Loss: 17.3900, Accuracy: 86.60 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  104.74397921562195\n",
      "Epoch [22/500], Loss: 17.7688, Accuracy: 86.22 %\n",
      "Testing Accuracy: 42.43 %\n",
      "Test Loss:  100.42879700660706\n",
      "Epoch [23/500], Loss: 16.9010, Accuracy: 86.89 %\n",
      "Testing Accuracy: 43.07 %\n",
      "Test Loss:  102.38678097724915\n",
      "Epoch [24/500], Loss: 16.4137, Accuracy: 87.53 %\n",
      "Testing Accuracy: 43.83 %\n",
      "Test Loss:  100.8857638835907\n",
      "Epoch [25/500], Loss: 15.8206, Accuracy: 88.11 %\n",
      "Testing Accuracy: 42.84 %\n",
      "Test Loss:  101.39918231964111\n",
      "Epoch [26/500], Loss: 15.9088, Accuracy: 87.88 %\n",
      "Testing Accuracy: 43.23 %\n",
      "Test Loss:  100.85025000572205\n",
      "Testing Accuracy: 43.23 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        747520   \n",
      "Net/Dropout[dropout]/onnx::Relu   730      \n",
      "Net/Linear[fc2]/onnx::Gemm        4015     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "752,265 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568]\n",
      "pruning hidden size:  365\n",
      "with hidden layer:  365\n",
      "removing:  (288, 242, 237)\n",
      "--- 39.78549003601074 seconds ---\n",
      "Epoch [1/500], Loss: 15.3446, Accuracy: 88.47 %\n",
      "Testing Accuracy: 43.93 %\n",
      "Test Loss:  101.75198936462402\n",
      "Epoch [2/500], Loss: 16.3020, Accuracy: 87.54 %\n",
      "Testing Accuracy: 43.86 %\n",
      "Test Loss:  100.81040525436401\n",
      "Epoch [3/500], Loss: 15.1901, Accuracy: 88.51 %\n",
      "Testing Accuracy: 42.65 %\n",
      "Test Loss:  99.2166588306427\n",
      "Epoch [4/500], Loss: 15.5608, Accuracy: 88.21 %\n",
      "Testing Accuracy: 43.80 %\n",
      "Test Loss:  102.28581833839417\n",
      "Epoch [5/500], Loss: 15.4389, Accuracy: 88.40 %\n",
      "Testing Accuracy: 43.05 %\n",
      "Test Loss:  102.84982395172119\n",
      "Epoch [6/500], Loss: 14.9072, Accuracy: 88.77 %\n",
      "Testing Accuracy: 42.62 %\n",
      "Test Loss:  101.05500054359436\n",
      "Epoch [7/500], Loss: 16.0972, Accuracy: 87.66 %\n",
      "Testing Accuracy: 43.05 %\n",
      "Test Loss:  101.61466383934021\n",
      "Epoch [8/500], Loss: 14.4053, Accuracy: 89.45 %\n",
      "Testing Accuracy: 42.87 %\n",
      "Test Loss:  101.54285144805908\n",
      "Epoch [9/500], Loss: 14.4029, Accuracy: 89.31 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  101.68848395347595\n",
      "Epoch [10/500], Loss: 14.4910, Accuracy: 89.20 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  104.282381772995\n",
      "Epoch [11/500], Loss: 14.9478, Accuracy: 88.74 %\n",
      "Testing Accuracy: 43.03 %\n",
      "Test Loss:  99.79213786125183\n",
      "Epoch [12/500], Loss: 16.5355, Accuracy: 87.32 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  102.91990971565247\n",
      "Epoch [13/500], Loss: 15.4228, Accuracy: 88.31 %\n",
      "Testing Accuracy: 42.66 %\n",
      "Test Loss:  100.4576346874237\n",
      "Epoch [14/500], Loss: 15.3197, Accuracy: 88.51 %\n",
      "Testing Accuracy: 43.37 %\n",
      "Test Loss:  101.4773268699646\n",
      "Epoch [15/500], Loss: 14.8169, Accuracy: 88.80 %\n",
      "Testing Accuracy: 43.54 %\n",
      "Test Loss:  100.57669186592102\n",
      "Epoch [16/500], Loss: 14.7084, Accuracy: 89.03 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  99.351886510849\n",
      "Epoch [17/500], Loss: 14.3385, Accuracy: 89.34 %\n",
      "Testing Accuracy: 43.84 %\n",
      "Test Loss:  102.57292628288269\n",
      "Epoch [18/500], Loss: 15.0877, Accuracy: 88.68 %\n",
      "Testing Accuracy: 43.40 %\n",
      "Test Loss:  100.10200524330139\n",
      "Epoch [19/500], Loss: 14.5221, Accuracy: 89.15 %\n",
      "Testing Accuracy: 43.37 %\n",
      "Test Loss:  100.08033585548401\n",
      "Epoch [20/500], Loss: 14.7182, Accuracy: 89.03 %\n",
      "Testing Accuracy: 42.75 %\n",
      "Test Loss:  105.01921820640564\n",
      "Epoch [21/500], Loss: 14.6640, Accuracy: 89.15 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  101.70711922645569\n",
      "Epoch [22/500], Loss: 14.6328, Accuracy: 89.05 %\n",
      "Testing Accuracy: 43.00 %\n",
      "Test Loss:  101.82299137115479\n",
      "Epoch [23/500], Loss: 14.5953, Accuracy: 89.25 %\n",
      "Testing Accuracy: 43.03 %\n",
      "Test Loss:  101.63387489318848\n",
      "Testing Accuracy: 43.03 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        741376   \n",
      "Net/Dropout[dropout]/onnx::Relu   724      \n",
      "Net/Linear[fc2]/onnx::Gemm        3982     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "746,082 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401]\n",
      "pruning hidden size:  362\n",
      "with hidden layer:  362\n",
      "removing:  (329, 219, 92)\n",
      "--- 37.637282371520996 seconds ---\n",
      "Epoch [1/500], Loss: 13.9414, Accuracy: 89.65 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  101.82111024856567\n",
      "Epoch [2/500], Loss: 14.3130, Accuracy: 89.32 %\n",
      "Testing Accuracy: 43.33 %\n",
      "Test Loss:  102.61049222946167\n",
      "Epoch [3/500], Loss: 14.0621, Accuracy: 89.57 %\n",
      "Testing Accuracy: 44.25 %\n",
      "Test Loss:  102.92829751968384\n",
      "Epoch [4/500], Loss: 14.3085, Accuracy: 89.30 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  105.32328414916992\n",
      "Epoch [5/500], Loss: 14.8294, Accuracy: 88.99 %\n",
      "Testing Accuracy: 42.82 %\n",
      "Test Loss:  99.09006929397583\n",
      "Epoch [6/500], Loss: 14.1935, Accuracy: 89.50 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  104.02760052680969\n",
      "Epoch [7/500], Loss: 13.3653, Accuracy: 90.26 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  100.79821443557739\n",
      "Epoch [8/500], Loss: 14.0715, Accuracy: 89.62 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  98.71257305145264\n",
      "Epoch [9/500], Loss: 14.1449, Accuracy: 89.49 %\n",
      "Testing Accuracy: 43.59 %\n",
      "Test Loss:  99.85363626480103\n",
      "Epoch [10/500], Loss: 13.6695, Accuracy: 90.02 %\n",
      "Testing Accuracy: 43.65 %\n",
      "Test Loss:  103.37972831726074\n",
      "Epoch [11/500], Loss: 14.2040, Accuracy: 89.47 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  107.49071002006531\n",
      "Epoch [12/500], Loss: 14.7186, Accuracy: 88.96 %\n",
      "Testing Accuracy: 43.22 %\n",
      "Test Loss:  100.64417052268982\n",
      "Epoch [13/500], Loss: 15.5493, Accuracy: 88.18 %\n",
      "Testing Accuracy: 43.71 %\n",
      "Test Loss:  103.0106999874115\n",
      "Epoch [14/500], Loss: 14.8665, Accuracy: 88.89 %\n",
      "Testing Accuracy: 43.67 %\n",
      "Test Loss:  106.30397415161133\n",
      "Epoch [15/500], Loss: 16.2794, Accuracy: 87.35 %\n",
      "Testing Accuracy: 43.36 %\n",
      "Test Loss:  100.5595314502716\n",
      "Epoch [16/500], Loss: 14.7888, Accuracy: 88.97 %\n",
      "Testing Accuracy: 44.06 %\n",
      "Test Loss:  101.46889400482178\n",
      "Epoch [17/500], Loss: 14.9113, Accuracy: 88.72 %\n",
      "Testing Accuracy: 42.62 %\n",
      "Test Loss:  101.47549438476562\n",
      "Epoch [18/500], Loss: 14.2606, Accuracy: 89.48 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  102.79498100280762\n",
      "Epoch [19/500], Loss: 15.0020, Accuracy: 88.63 %\n",
      "Testing Accuracy: 43.26 %\n",
      "Test Loss:  101.2348051071167\n",
      "Epoch [20/500], Loss: 13.9929, Accuracy: 89.79 %\n",
      "Testing Accuracy: 43.52 %\n",
      "Test Loss:  102.33085799217224\n",
      "Epoch [21/500], Loss: 14.9140, Accuracy: 88.71 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  100.5887839794159\n",
      "Epoch [22/500], Loss: 14.5925, Accuracy: 89.21 %\n",
      "Testing Accuracy: 43.98 %\n",
      "Test Loss:  100.93454003334045\n",
      "Epoch [23/500], Loss: 14.5071, Accuracy: 89.21 %\n",
      "Testing Accuracy: 43.89 %\n",
      "Test Loss:  101.27639293670654\n",
      "Epoch [24/500], Loss: 14.7314, Accuracy: 89.05 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  103.08238577842712\n",
      "Epoch [25/500], Loss: 14.7010, Accuracy: 89.09 %\n",
      "Testing Accuracy: 42.68 %\n",
      "Test Loss:  101.28508043289185\n",
      "Epoch [26/500], Loss: 14.3967, Accuracy: 89.40 %\n",
      "Testing Accuracy: 43.25 %\n",
      "Test Loss:  106.06449508666992\n",
      "Epoch [27/500], Loss: 14.0476, Accuracy: 89.49 %\n",
      "Testing Accuracy: 43.79 %\n",
      "Test Loss:  100.44309997558594\n",
      "Epoch [28/500], Loss: 13.5486, Accuracy: 90.02 %\n",
      "Testing Accuracy: 43.63 %\n",
      "Test Loss:  102.77105069160461\n",
      "Testing Accuracy: 43.63 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        735232   \n",
      "Net/Dropout[dropout]/onnx::Relu   718      \n",
      "Net/Linear[fc2]/onnx::Gemm        3949     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "739,899 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445]\n",
      "pruning hidden size:  359\n",
      "with hidden layer:  359\n",
      "removing:  (292, 162, 135)\n",
      "--- 38.331074714660645 seconds ---\n",
      "Epoch [1/500], Loss: 14.3163, Accuracy: 89.34 %\n",
      "Testing Accuracy: 43.84 %\n",
      "Test Loss:  102.45373821258545\n",
      "Epoch [2/500], Loss: 14.8557, Accuracy: 88.94 %\n",
      "Testing Accuracy: 43.13 %\n",
      "Test Loss:  100.8997106552124\n",
      "Epoch [3/500], Loss: 14.8328, Accuracy: 88.90 %\n",
      "Testing Accuracy: 43.81 %\n",
      "Test Loss:  103.71067786216736\n",
      "Epoch [4/500], Loss: 14.7804, Accuracy: 88.94 %\n",
      "Testing Accuracy: 42.92 %\n",
      "Test Loss:  103.17290353775024\n",
      "Epoch [5/500], Loss: 14.6529, Accuracy: 89.10 %\n",
      "Testing Accuracy: 43.11 %\n",
      "Test Loss:  100.92436981201172\n",
      "Epoch [6/500], Loss: 14.2623, Accuracy: 89.55 %\n",
      "Testing Accuracy: 43.27 %\n",
      "Test Loss:  101.26295328140259\n",
      "Epoch [7/500], Loss: 14.6439, Accuracy: 89.18 %\n",
      "Testing Accuracy: 43.51 %\n",
      "Test Loss:  104.28809809684753\n",
      "Epoch [8/500], Loss: 14.0949, Accuracy: 89.57 %\n",
      "Testing Accuracy: 43.45 %\n",
      "Test Loss:  103.60886836051941\n",
      "Epoch [9/500], Loss: 15.3863, Accuracy: 88.32 %\n",
      "Testing Accuracy: 43.08 %\n",
      "Test Loss:  101.49158048629761\n",
      "Epoch [10/500], Loss: 14.7193, Accuracy: 88.96 %\n",
      "Testing Accuracy: 43.38 %\n",
      "Test Loss:  101.54051041603088\n",
      "Epoch [11/500], Loss: 14.7002, Accuracy: 88.91 %\n",
      "Testing Accuracy: 43.16 %\n",
      "Test Loss:  102.22912573814392\n",
      "Epoch [12/500], Loss: 14.9732, Accuracy: 88.81 %\n",
      "Testing Accuracy: 43.33 %\n",
      "Test Loss:  102.09652376174927\n",
      "Epoch [13/500], Loss: 14.9627, Accuracy: 88.71 %\n",
      "Testing Accuracy: 43.46 %\n",
      "Test Loss:  105.14160633087158\n",
      "Epoch [14/500], Loss: 14.1211, Accuracy: 89.56 %\n",
      "Testing Accuracy: 43.29 %\n",
      "Test Loss:  103.0663161277771\n",
      "Epoch [15/500], Loss: 14.9993, Accuracy: 88.78 %\n",
      "Testing Accuracy: 43.03 %\n",
      "Test Loss:  101.22263431549072\n",
      "Epoch [16/500], Loss: 14.7840, Accuracy: 88.90 %\n",
      "Testing Accuracy: 43.14 %\n",
      "Test Loss:  104.51901268959045\n",
      "Epoch [17/500], Loss: 15.0165, Accuracy: 88.63 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  100.69059491157532\n",
      "Epoch [18/500], Loss: 14.7931, Accuracy: 88.79 %\n",
      "Testing Accuracy: 43.25 %\n",
      "Test Loss:  100.61610102653503\n",
      "Epoch [19/500], Loss: 14.5393, Accuracy: 89.10 %\n",
      "Testing Accuracy: 43.96 %\n",
      "Test Loss:  100.63080668449402\n",
      "Epoch [20/500], Loss: 14.6886, Accuracy: 89.04 %\n",
      "Testing Accuracy: 43.95 %\n",
      "Test Loss:  103.33651971817017\n",
      "Epoch [21/500], Loss: 14.2812, Accuracy: 89.35 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  102.10328364372253\n",
      "Epoch [22/500], Loss: 13.3267, Accuracy: 90.17 %\n",
      "Testing Accuracy: 43.48 %\n",
      "Test Loss:  102.95882964134216\n",
      "Epoch [23/500], Loss: 14.3545, Accuracy: 89.27 %\n",
      "Testing Accuracy: 43.56 %\n",
      "Test Loss:  104.9342200756073\n",
      "Epoch [24/500], Loss: 15.0033, Accuracy: 88.62 %\n",
      "Testing Accuracy: 42.47 %\n",
      "Test Loss:  103.16828799247742\n",
      "Epoch [25/500], Loss: 15.8853, Accuracy: 87.83 %\n",
      "Testing Accuracy: 43.32 %\n",
      "Test Loss:  101.31161499023438\n",
      "Epoch [26/500], Loss: 15.9180, Accuracy: 87.67 %\n",
      "Testing Accuracy: 43.10 %\n",
      "Test Loss:  104.4927670955658\n",
      "Epoch [27/500], Loss: 14.9397, Accuracy: 88.72 %\n",
      "Testing Accuracy: 42.80 %\n",
      "Test Loss:  101.79395699501038\n",
      "Epoch [28/500], Loss: 14.4205, Accuracy: 89.44 %\n",
      "Testing Accuracy: 43.50 %\n",
      "Test Loss:  98.64459657669067\n",
      "Epoch [29/500], Loss: 15.6066, Accuracy: 88.20 %\n",
      "Testing Accuracy: 44.01 %\n",
      "Test Loss:  104.04766416549683\n",
      "Epoch [30/500], Loss: 15.3423, Accuracy: 88.21 %\n",
      "Testing Accuracy: 43.20 %\n",
      "Test Loss:  100.30668425559998\n",
      "Epoch [31/500], Loss: 14.9928, Accuracy: 88.51 %\n",
      "Testing Accuracy: 41.90 %\n",
      "Test Loss:  98.887868642807\n",
      "Epoch [32/500], Loss: 15.1580, Accuracy: 88.62 %\n",
      "Testing Accuracy: 42.95 %\n",
      "Test Loss:  104.08262228965759\n",
      "Epoch [33/500], Loss: 16.1607, Accuracy: 87.65 %\n",
      "Testing Accuracy: 43.16 %\n",
      "Test Loss:  100.65314555168152\n",
      "Epoch [34/500], Loss: 15.2555, Accuracy: 88.15 %\n",
      "Testing Accuracy: 43.26 %\n",
      "Test Loss:  101.06071186065674\n",
      "Epoch [35/500], Loss: 15.2357, Accuracy: 88.46 %\n",
      "Testing Accuracy: 43.20 %\n",
      "Test Loss:  104.48183298110962\n",
      "Epoch [36/500], Loss: 15.2011, Accuracy: 88.46 %\n",
      "Testing Accuracy: 43.28 %\n",
      "Test Loss:  101.17575073242188\n",
      "Epoch [37/500], Loss: 14.6701, Accuracy: 88.95 %\n",
      "Testing Accuracy: 43.19 %\n",
      "Test Loss:  100.88284993171692\n",
      "Epoch [38/500], Loss: 14.7372, Accuracy: 88.84 %\n",
      "Testing Accuracy: 42.98 %\n",
      "Test Loss:  102.29120135307312\n",
      "Epoch [39/500], Loss: 15.0940, Accuracy: 88.71 %\n",
      "Testing Accuracy: 43.63 %\n",
      "Test Loss:  100.69280934333801\n",
      "Epoch [40/500], Loss: 15.4643, Accuracy: 88.25 %\n",
      "Testing Accuracy: 42.89 %\n",
      "Test Loss:  101.7833182811737\n",
      "Epoch [41/500], Loss: 15.1721, Accuracy: 88.46 %\n",
      "Testing Accuracy: 43.32 %\n",
      "Test Loss:  101.5725610256195\n",
      "Epoch [42/500], Loss: 15.0149, Accuracy: 88.70 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  105.10852003097534\n",
      "Epoch [43/500], Loss: 17.2305, Accuracy: 86.64 %\n",
      "Testing Accuracy: 43.53 %\n",
      "Test Loss:  101.08985996246338\n",
      "Epoch [44/500], Loss: 16.2722, Accuracy: 87.61 %\n",
      "Testing Accuracy: 42.89 %\n",
      "Test Loss:  100.740079164505\n",
      "Epoch [45/500], Loss: 16.1508, Accuracy: 87.60 %\n",
      "Testing Accuracy: 43.50 %\n",
      "Test Loss:  103.52980184555054\n",
      "Epoch [46/500], Loss: 14.6458, Accuracy: 89.02 %\n",
      "Testing Accuracy: 43.14 %\n",
      "Test Loss:  105.76855230331421\n",
      "Epoch [47/500], Loss: 14.1419, Accuracy: 89.43 %\n",
      "Testing Accuracy: 43.49 %\n",
      "Test Loss:  105.49610781669617\n",
      "Epoch [48/500], Loss: 15.0550, Accuracy: 88.69 %\n",
      "Testing Accuracy: 42.92 %\n",
      "Test Loss:  100.6990692615509\n",
      "Testing Accuracy: 42.92 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        729088   \n",
      "Net/Dropout[dropout]/onnx::Relu   712      \n",
      "Net/Linear[fc2]/onnx::Gemm        3916     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "733,716 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089]\n",
      "pruning hidden size:  356\n",
      "with hidden layer:  356\n",
      "removing:  (234, 212, 99)\n",
      "--- 36.55071473121643 seconds ---\n",
      "Epoch [1/500], Loss: 15.2462, Accuracy: 88.55 %\n",
      "Testing Accuracy: 43.76 %\n",
      "Test Loss:  102.42772936820984\n",
      "Epoch [2/500], Loss: 15.4152, Accuracy: 88.36 %\n",
      "Testing Accuracy: 42.13 %\n",
      "Test Loss:  105.12931180000305\n",
      "Epoch [3/500], Loss: 17.9552, Accuracy: 86.04 %\n",
      "Testing Accuracy: 42.48 %\n",
      "Test Loss:  102.02311563491821\n",
      "Epoch [4/500], Loss: 16.3150, Accuracy: 87.51 %\n",
      "Testing Accuracy: 42.11 %\n",
      "Test Loss:  102.67567348480225\n",
      "Epoch [5/500], Loss: 16.5158, Accuracy: 87.33 %\n",
      "Testing Accuracy: 43.22 %\n",
      "Test Loss:  101.27656888961792\n",
      "Epoch [6/500], Loss: 16.0623, Accuracy: 87.58 %\n",
      "Testing Accuracy: 42.37 %\n",
      "Test Loss:  106.08678889274597\n",
      "Epoch [7/500], Loss: 16.7088, Accuracy: 87.13 %\n",
      "Testing Accuracy: 42.29 %\n",
      "Test Loss:  98.80944585800171\n",
      "Epoch [8/500], Loss: 17.2776, Accuracy: 86.58 %\n",
      "Testing Accuracy: 43.10 %\n",
      "Test Loss:  100.36174821853638\n",
      "Epoch [9/500], Loss: 15.3527, Accuracy: 88.53 %\n",
      "Testing Accuracy: 42.21 %\n",
      "Test Loss:  99.51244068145752\n",
      "Epoch [10/500], Loss: 16.6916, Accuracy: 87.05 %\n",
      "Testing Accuracy: 43.74 %\n",
      "Test Loss:  102.51080918312073\n",
      "Epoch [11/500], Loss: 15.4516, Accuracy: 88.31 %\n",
      "Testing Accuracy: 42.77 %\n",
      "Test Loss:  99.95299458503723\n",
      "Epoch [12/500], Loss: 15.7288, Accuracy: 87.89 %\n",
      "Testing Accuracy: 43.37 %\n",
      "Test Loss:  100.95848393440247\n",
      "Epoch [13/500], Loss: 16.1637, Accuracy: 87.44 %\n",
      "Testing Accuracy: 42.73 %\n",
      "Test Loss:  100.43248629570007\n",
      "Epoch [14/500], Loss: 16.2460, Accuracy: 87.61 %\n",
      "Testing Accuracy: 43.11 %\n",
      "Test Loss:  102.78109216690063\n",
      "Epoch [15/500], Loss: 16.4754, Accuracy: 87.21 %\n",
      "Testing Accuracy: 42.74 %\n",
      "Test Loss:  100.04067635536194\n",
      "Epoch [16/500], Loss: 16.2373, Accuracy: 87.71 %\n",
      "Testing Accuracy: 42.93 %\n",
      "Test Loss:  103.468745470047\n",
      "Epoch [17/500], Loss: 17.1035, Accuracy: 86.93 %\n",
      "Testing Accuracy: 42.72 %\n",
      "Test Loss:  103.14453077316284\n",
      "Epoch [18/500], Loss: 16.6786, Accuracy: 87.16 %\n",
      "Testing Accuracy: 43.26 %\n",
      "Test Loss:  99.100905418396\n",
      "Epoch [19/500], Loss: 15.5749, Accuracy: 88.17 %\n",
      "Testing Accuracy: 43.15 %\n",
      "Test Loss:  100.89535689353943\n",
      "Epoch [20/500], Loss: 16.6523, Accuracy: 87.28 %\n",
      "Testing Accuracy: 43.49 %\n",
      "Test Loss:  103.07300019264221\n",
      "Epoch [21/500], Loss: 16.0569, Accuracy: 87.82 %\n",
      "Testing Accuracy: 43.44 %\n",
      "Test Loss:  101.24833631515503\n",
      "Epoch [22/500], Loss: 16.0006, Accuracy: 87.59 %\n",
      "Testing Accuracy: 43.34 %\n",
      "Test Loss:  101.44343090057373\n",
      "Epoch [23/500], Loss: 16.4956, Accuracy: 87.27 %\n",
      "Testing Accuracy: 42.71 %\n",
      "Test Loss:  104.32528638839722\n",
      "Epoch [24/500], Loss: 15.8139, Accuracy: 88.07 %\n",
      "Testing Accuracy: 43.27 %\n",
      "Test Loss:  100.61366128921509\n",
      "Epoch [25/500], Loss: 15.1935, Accuracy: 88.51 %\n",
      "Testing Accuracy: 43.36 %\n",
      "Test Loss:  103.64006280899048\n",
      "Epoch [26/500], Loss: 14.9205, Accuracy: 88.77 %\n",
      "Testing Accuracy: 43.32 %\n",
      "Test Loss:  103.8632562160492\n",
      "Epoch [27/500], Loss: 16.4300, Accuracy: 87.48 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  102.77556538581848\n",
      "Testing Accuracy: 43.30 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        722944   \n",
      "Net/Dropout[dropout]/onnx::Relu   706      \n",
      "Net/Linear[fc2]/onnx::Gemm        3883     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "727,533 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138]\n",
      "pruning hidden size:  353\n",
      "with hidden layer:  353\n",
      "removing:  (187, 124, 48)\n",
      "--- 36.819058418273926 seconds ---\n",
      "Epoch [1/500], Loss: 17.5917, Accuracy: 86.46 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  100.34771132469177\n",
      "Epoch [2/500], Loss: 17.2689, Accuracy: 86.70 %\n",
      "Testing Accuracy: 42.62 %\n",
      "Test Loss:  102.31744623184204\n",
      "Epoch [3/500], Loss: 17.3638, Accuracy: 86.46 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  105.37282633781433\n",
      "Epoch [4/500], Loss: 16.9222, Accuracy: 86.97 %\n",
      "Testing Accuracy: 43.07 %\n",
      "Test Loss:  102.06010508537292\n",
      "Epoch [5/500], Loss: 16.9074, Accuracy: 87.09 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  101.48574256896973\n",
      "Epoch [6/500], Loss: 17.2843, Accuracy: 86.57 %\n",
      "Testing Accuracy: 43.08 %\n",
      "Test Loss:  106.72888398170471\n",
      "Epoch [7/500], Loss: 17.3987, Accuracy: 86.55 %\n",
      "Testing Accuracy: 43.01 %\n",
      "Test Loss:  100.72791409492493\n",
      "Epoch [8/500], Loss: 17.5290, Accuracy: 86.35 %\n",
      "Testing Accuracy: 43.75 %\n",
      "Test Loss:  101.65672445297241\n",
      "Epoch [9/500], Loss: 17.7139, Accuracy: 86.21 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  104.16891717910767\n",
      "Epoch [10/500], Loss: 18.0276, Accuracy: 86.16 %\n",
      "Testing Accuracy: 43.32 %\n",
      "Test Loss:  101.11806178092957\n",
      "Epoch [11/500], Loss: 17.7910, Accuracy: 86.26 %\n",
      "Testing Accuracy: 43.16 %\n",
      "Test Loss:  101.90951800346375\n",
      "Epoch [12/500], Loss: 17.1075, Accuracy: 86.71 %\n",
      "Testing Accuracy: 42.88 %\n",
      "Test Loss:  103.79167699813843\n",
      "Epoch [13/500], Loss: 16.6794, Accuracy: 87.25 %\n",
      "Testing Accuracy: 42.99 %\n",
      "Test Loss:  103.21641039848328\n",
      "Epoch [14/500], Loss: 16.3886, Accuracy: 87.40 %\n",
      "Testing Accuracy: 43.57 %\n",
      "Test Loss:  100.33948230743408\n",
      "Epoch [15/500], Loss: 16.7766, Accuracy: 87.00 %\n",
      "Testing Accuracy: 42.56 %\n",
      "Test Loss:  103.62473487854004\n",
      "Epoch [16/500], Loss: 17.6634, Accuracy: 86.21 %\n",
      "Testing Accuracy: 43.11 %\n",
      "Test Loss:  100.32734513282776\n",
      "Epoch [17/500], Loss: 17.3674, Accuracy: 86.70 %\n",
      "Testing Accuracy: 42.46 %\n",
      "Test Loss:  103.1835458278656\n",
      "Epoch [18/500], Loss: 16.5453, Accuracy: 87.24 %\n",
      "Testing Accuracy: 42.77 %\n",
      "Test Loss:  100.56518983840942\n",
      "Epoch [19/500], Loss: 16.2368, Accuracy: 87.70 %\n",
      "Testing Accuracy: 42.69 %\n",
      "Test Loss:  101.23777914047241\n",
      "Epoch [20/500], Loss: 16.1137, Accuracy: 87.80 %\n",
      "Testing Accuracy: 43.30 %\n",
      "Test Loss:  101.3786973953247\n",
      "Epoch [21/500], Loss: 15.5312, Accuracy: 88.24 %\n",
      "Testing Accuracy: 42.32 %\n",
      "Test Loss:  102.00042271614075\n",
      "Epoch [22/500], Loss: 16.2258, Accuracy: 87.63 %\n",
      "Testing Accuracy: 42.52 %\n",
      "Test Loss:  103.15366244316101\n",
      "Epoch [23/500], Loss: 15.8713, Accuracy: 87.98 %\n",
      "Testing Accuracy: 43.23 %\n",
      "Test Loss:  103.94409155845642\n",
      "Epoch [24/500], Loss: 16.4414, Accuracy: 87.44 %\n",
      "Testing Accuracy: 42.63 %\n",
      "Test Loss:  99.35119700431824\n",
      "Epoch [25/500], Loss: 16.4654, Accuracy: 87.47 %\n",
      "Testing Accuracy: 43.61 %\n",
      "Test Loss:  103.90599775314331\n",
      "Epoch [26/500], Loss: 16.3782, Accuracy: 87.36 %\n",
      "Testing Accuracy: 42.86 %\n",
      "Test Loss:  102.87828469276428\n",
      "Epoch [27/500], Loss: 16.7477, Accuracy: 87.13 %\n",
      "Testing Accuracy: 42.57 %\n",
      "Test Loss:  101.3326427936554\n",
      "Epoch [28/500], Loss: 16.6072, Accuracy: 87.14 %\n",
      "Testing Accuracy: 42.99 %\n",
      "Test Loss:  99.42807126045227\n",
      "Epoch [29/500], Loss: 17.5118, Accuracy: 86.54 %\n",
      "Testing Accuracy: 41.74 %\n",
      "Test Loss:  102.31023788452148\n",
      "Epoch [30/500], Loss: 18.0890, Accuracy: 86.00 %\n",
      "Testing Accuracy: 42.58 %\n",
      "Test Loss:  99.56830644607544\n",
      "Epoch [31/500], Loss: 18.1090, Accuracy: 86.13 %\n",
      "Testing Accuracy: 42.48 %\n",
      "Test Loss:  99.49875926971436\n",
      "Epoch [32/500], Loss: 17.8784, Accuracy: 85.92 %\n",
      "Testing Accuracy: 42.61 %\n",
      "Test Loss:  101.18353343009949\n",
      "Epoch [33/500], Loss: 17.4407, Accuracy: 86.67 %\n",
      "Testing Accuracy: 42.28 %\n",
      "Test Loss:  100.28843450546265\n",
      "Epoch [34/500], Loss: 18.2880, Accuracy: 85.77 %\n",
      "Testing Accuracy: 42.78 %\n",
      "Test Loss:  98.18353199958801\n",
      "Epoch [35/500], Loss: 17.0627, Accuracy: 86.86 %\n",
      "Testing Accuracy: 42.90 %\n",
      "Test Loss:  102.82682061195374\n",
      "Epoch [36/500], Loss: 18.5226, Accuracy: 85.73 %\n",
      "Testing Accuracy: 42.59 %\n",
      "Test Loss:  102.13472414016724\n",
      "Epoch [37/500], Loss: 17.7238, Accuracy: 86.27 %\n",
      "Testing Accuracy: 42.79 %\n",
      "Test Loss:  100.59171295166016\n",
      "Epoch [38/500], Loss: 17.6955, Accuracy: 86.35 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  99.23368334770203\n",
      "Epoch [39/500], Loss: 16.8422, Accuracy: 87.17 %\n",
      "Testing Accuracy: 42.56 %\n",
      "Test Loss:  102.77270579338074\n",
      "Epoch [40/500], Loss: 16.8675, Accuracy: 87.07 %\n",
      "Testing Accuracy: 43.32 %\n",
      "Test Loss:  102.55082988739014\n",
      "Epoch [41/500], Loss: 17.5295, Accuracy: 86.43 %\n",
      "Testing Accuracy: 42.64 %\n",
      "Test Loss:  105.64468002319336\n",
      "Epoch [42/500], Loss: 18.4132, Accuracy: 85.70 %\n",
      "Testing Accuracy: 42.23 %\n",
      "Test Loss:  97.98821926116943\n",
      "Epoch [43/500], Loss: 17.7277, Accuracy: 86.50 %\n",
      "Testing Accuracy: 42.90 %\n",
      "Test Loss:  99.96775889396667\n",
      "Epoch [44/500], Loss: 17.7959, Accuracy: 86.31 %\n",
      "Testing Accuracy: 41.96 %\n",
      "Test Loss:  99.6058337688446\n",
      "Epoch [45/500], Loss: 17.6791, Accuracy: 86.44 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  100.54939913749695\n",
      "Epoch [46/500], Loss: 18.1831, Accuracy: 86.18 %\n",
      "Testing Accuracy: 43.51 %\n",
      "Test Loss:  103.18077778816223\n",
      "Epoch [47/500], Loss: 19.0204, Accuracy: 85.27 %\n",
      "Testing Accuracy: 42.11 %\n",
      "Test Loss:  99.97003984451294\n",
      "Epoch [48/500], Loss: 19.2470, Accuracy: 84.96 %\n",
      "Testing Accuracy: 42.82 %\n",
      "Test Loss:  99.43195581436157\n",
      "Epoch [49/500], Loss: 18.5065, Accuracy: 85.62 %\n",
      "Testing Accuracy: 42.95 %\n",
      "Test Loss:  101.255380153656\n",
      "Epoch [50/500], Loss: 18.4929, Accuracy: 85.61 %\n",
      "Testing Accuracy: 43.09 %\n",
      "Test Loss:  98.15411067008972\n",
      "Epoch [51/500], Loss: 18.0528, Accuracy: 85.86 %\n",
      "Testing Accuracy: 42.70 %\n",
      "Test Loss:  100.13187146186829\n",
      "Epoch [52/500], Loss: 19.0722, Accuracy: 85.08 %\n",
      "Testing Accuracy: 42.34 %\n",
      "Test Loss:  99.4028627872467\n",
      "Epoch [53/500], Loss: 18.5214, Accuracy: 85.66 %\n",
      "Testing Accuracy: 42.81 %\n",
      "Test Loss:  99.52049517631531\n",
      "Epoch [54/500], Loss: 19.6207, Accuracy: 84.62 %\n",
      "Testing Accuracy: 42.37 %\n",
      "Test Loss:  99.35602355003357\n",
      "Epoch [55/500], Loss: 20.4475, Accuracy: 83.90 %\n",
      "Testing Accuracy: 42.31 %\n",
      "Test Loss:  100.23362159729004\n",
      "Epoch [56/500], Loss: 19.4254, Accuracy: 84.81 %\n",
      "Testing Accuracy: 43.25 %\n",
      "Test Loss:  100.27748608589172\n",
      "Epoch [57/500], Loss: 20.9243, Accuracy: 83.52 %\n",
      "Testing Accuracy: 42.50 %\n",
      "Test Loss:  96.63546752929688\n",
      "Epoch [58/500], Loss: 19.9514, Accuracy: 84.33 %\n",
      "Testing Accuracy: 41.85 %\n",
      "Test Loss:  100.84590983390808\n",
      "Epoch [59/500], Loss: 22.3212, Accuracy: 82.20 %\n",
      "Testing Accuracy: 42.50 %\n",
      "Test Loss:  98.4536075592041\n",
      "Epoch [60/500], Loss: 23.2539, Accuracy: 81.39 %\n",
      "Testing Accuracy: 40.83 %\n",
      "Test Loss:  100.74544143676758\n",
      "Epoch [61/500], Loss: 23.4971, Accuracy: 81.17 %\n",
      "Testing Accuracy: 41.62 %\n",
      "Test Loss:  100.70855140686035\n",
      "Epoch [62/500], Loss: 22.2353, Accuracy: 82.24 %\n",
      "Testing Accuracy: 41.63 %\n",
      "Test Loss:  99.21138954162598\n",
      "Epoch [63/500], Loss: 23.2151, Accuracy: 81.58 %\n",
      "Testing Accuracy: 42.44 %\n",
      "Test Loss:  94.40099787712097\n",
      "Epoch [64/500], Loss: 24.7761, Accuracy: 80.16 %\n",
      "Testing Accuracy: 41.39 %\n",
      "Test Loss:  99.28430199623108\n",
      "Epoch [65/500], Loss: 25.1481, Accuracy: 79.55 %\n",
      "Testing Accuracy: 41.22 %\n",
      "Test Loss:  98.80876922607422\n",
      "Epoch [66/500], Loss: 26.6995, Accuracy: 78.27 %\n",
      "Testing Accuracy: 41.39 %\n",
      "Test Loss:  96.65679216384888\n",
      "Epoch [67/500], Loss: 27.9350, Accuracy: 77.32 %\n",
      "Testing Accuracy: 41.95 %\n",
      "Test Loss:  96.33067560195923\n",
      "Epoch [68/500], Loss: 27.7626, Accuracy: 77.36 %\n",
      "Testing Accuracy: 41.93 %\n",
      "Test Loss:  94.88889026641846\n",
      "Epoch [69/500], Loss: 28.2164, Accuracy: 76.98 %\n",
      "Testing Accuracy: 40.63 %\n",
      "Test Loss:  100.8673267364502\n",
      "Epoch [70/500], Loss: 28.9323, Accuracy: 76.53 %\n",
      "Testing Accuracy: 40.77 %\n",
      "Test Loss:  91.98391878604889\n",
      "Epoch [71/500], Loss: 31.8946, Accuracy: 73.86 %\n",
      "Testing Accuracy: 40.63 %\n",
      "Test Loss:  90.42429792881012\n",
      "Epoch [72/500], Loss: 30.4698, Accuracy: 74.99 %\n",
      "Testing Accuracy: 39.67 %\n",
      "Test Loss:  98.37964940071106\n",
      "Epoch [73/500], Loss: 32.0805, Accuracy: 73.54 %\n",
      "Testing Accuracy: 40.42 %\n",
      "Test Loss:  92.36138319969177\n",
      "Epoch [74/500], Loss: 31.4198, Accuracy: 74.34 %\n",
      "Testing Accuracy: 40.97 %\n",
      "Test Loss:  94.79127860069275\n",
      "Epoch [75/500], Loss: 32.9528, Accuracy: 72.90 %\n",
      "Testing Accuracy: 40.36 %\n",
      "Test Loss:  92.58224022388458\n",
      "Epoch [76/500], Loss: 34.3913, Accuracy: 71.75 %\n",
      "Testing Accuracy: 40.85 %\n",
      "Test Loss:  91.1957870721817\n",
      "Epoch [77/500], Loss: 31.7727, Accuracy: 73.89 %\n",
      "Testing Accuracy: 40.77 %\n",
      "Test Loss:  93.76513051986694\n",
      "Epoch [78/500], Loss: 32.8335, Accuracy: 72.94 %\n",
      "Testing Accuracy: 40.79 %\n",
      "Test Loss:  91.57501745223999\n",
      "Epoch [79/500], Loss: 32.9110, Accuracy: 73.19 %\n",
      "Testing Accuracy: 41.88 %\n",
      "Test Loss:  91.58213138580322\n",
      "Epoch [80/500], Loss: 34.0273, Accuracy: 72.10 %\n",
      "Testing Accuracy: 41.44 %\n",
      "Test Loss:  90.20796704292297\n",
      "Epoch [81/500], Loss: 34.0965, Accuracy: 71.94 %\n",
      "Testing Accuracy: 40.64 %\n",
      "Test Loss:  92.1755518913269\n",
      "Epoch [82/500], Loss: 35.9146, Accuracy: 70.17 %\n",
      "Testing Accuracy: 40.29 %\n",
      "Test Loss:  92.1488288640976\n",
      "Epoch [83/500], Loss: 37.4123, Accuracy: 69.42 %\n",
      "Testing Accuracy: 40.46 %\n",
      "Test Loss:  89.31953394412994\n",
      "Epoch [84/500], Loss: 38.1494, Accuracy: 68.19 %\n",
      "Testing Accuracy: 40.86 %\n",
      "Test Loss:  86.34356009960175\n",
      "Epoch [85/500], Loss: 39.2800, Accuracy: 67.27 %\n",
      "Testing Accuracy: 40.04 %\n",
      "Test Loss:  87.9151850938797\n",
      "Epoch [86/500], Loss: 40.8827, Accuracy: 65.94 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  88.3172299861908\n",
      "Epoch [87/500], Loss: 41.7839, Accuracy: 65.21 %\n",
      "Testing Accuracy: 40.18 %\n",
      "Test Loss:  86.68574845790863\n",
      "Epoch [88/500], Loss: 44.0943, Accuracy: 63.02 %\n",
      "Testing Accuracy: 40.39 %\n",
      "Test Loss:  87.86427712440491\n",
      "Epoch [89/500], Loss: 42.8064, Accuracy: 64.29 %\n",
      "Testing Accuracy: 41.22 %\n",
      "Test Loss:  88.4603101015091\n",
      "Epoch [90/500], Loss: 43.9458, Accuracy: 63.26 %\n",
      "Testing Accuracy: 40.27 %\n",
      "Test Loss:  85.96762955188751\n",
      "Epoch [91/500], Loss: 44.9245, Accuracy: 62.57 %\n",
      "Testing Accuracy: 39.92 %\n",
      "Test Loss:  86.31414413452148\n",
      "Epoch [92/500], Loss: 44.2417, Accuracy: 63.09 %\n",
      "Testing Accuracy: 40.25 %\n",
      "Test Loss:  85.26469266414642\n",
      "Epoch [93/500], Loss: 45.4341, Accuracy: 61.97 %\n",
      "Testing Accuracy: 40.50 %\n",
      "Test Loss:  83.86792051792145\n",
      "Epoch [94/500], Loss: 46.4870, Accuracy: 61.04 %\n",
      "Testing Accuracy: 40.78 %\n",
      "Test Loss:  85.50298225879669\n",
      "Epoch [95/500], Loss: 47.4029, Accuracy: 60.28 %\n",
      "Testing Accuracy: 39.10 %\n",
      "Test Loss:  83.60378015041351\n",
      "Epoch [96/500], Loss: 49.7684, Accuracy: 58.17 %\n",
      "Testing Accuracy: 40.40 %\n",
      "Test Loss:  85.81173241138458\n",
      "Epoch [97/500], Loss: 49.8711, Accuracy: 58.28 %\n",
      "Testing Accuracy: 38.22 %\n",
      "Test Loss:  86.11566889286041\n",
      "Epoch [98/500], Loss: 53.7889, Accuracy: 54.87 %\n",
      "Testing Accuracy: 38.84 %\n",
      "Test Loss:  83.74865484237671\n",
      "Epoch [99/500], Loss: 54.7079, Accuracy: 54.03 %\n",
      "Testing Accuracy: 38.59 %\n",
      "Test Loss:  81.2192108631134\n",
      "Epoch [100/500], Loss: 52.4277, Accuracy: 55.91 %\n",
      "Testing Accuracy: 39.86 %\n",
      "Test Loss:  80.90083372592926\n",
      "Epoch [101/500], Loss: 51.2041, Accuracy: 57.22 %\n",
      "Testing Accuracy: 40.78 %\n",
      "Test Loss:  81.80718147754669\n",
      "Epoch [102/500], Loss: 51.3601, Accuracy: 57.02 %\n",
      "Testing Accuracy: 40.05 %\n",
      "Test Loss:  80.58465957641602\n",
      "Epoch [103/500], Loss: 52.9641, Accuracy: 55.61 %\n",
      "Testing Accuracy: 39.17 %\n",
      "Test Loss:  82.88332772254944\n",
      "Epoch [104/500], Loss: 54.1260, Accuracy: 54.49 %\n",
      "Testing Accuracy: 39.18 %\n",
      "Test Loss:  84.30707395076752\n",
      "Epoch [105/500], Loss: 54.4470, Accuracy: 54.23 %\n",
      "Testing Accuracy: 39.40 %\n",
      "Test Loss:  81.02955150604248\n",
      "Epoch [106/500], Loss: 54.7692, Accuracy: 53.96 %\n",
      "Testing Accuracy: 39.41 %\n",
      "Test Loss:  79.67660844326019\n",
      "Epoch [107/500], Loss: 55.5869, Accuracy: 53.20 %\n",
      "Testing Accuracy: 38.10 %\n",
      "Test Loss:  81.33103108406067\n",
      "Epoch [108/500], Loss: 55.7547, Accuracy: 52.60 %\n",
      "Testing Accuracy: 39.62 %\n",
      "Test Loss:  80.44127476215363\n",
      "Epoch [109/500], Loss: 56.0143, Accuracy: 52.92 %\n",
      "Testing Accuracy: 39.56 %\n",
      "Test Loss:  78.86932218074799\n",
      "Epoch [110/500], Loss: 57.9346, Accuracy: 51.03 %\n",
      "Testing Accuracy: 39.12 %\n",
      "Test Loss:  81.26313292980194\n",
      "Epoch [111/500], Loss: 57.0360, Accuracy: 51.88 %\n",
      "Testing Accuracy: 39.16 %\n",
      "Test Loss:  81.73466920852661\n",
      "Epoch [112/500], Loss: 57.9266, Accuracy: 51.10 %\n",
      "Testing Accuracy: 38.66 %\n",
      "Test Loss:  80.42425560951233\n",
      "Epoch [113/500], Loss: 58.4841, Accuracy: 50.68 %\n",
      "Testing Accuracy: 38.67 %\n",
      "Test Loss:  80.2652199268341\n",
      "Epoch [114/500], Loss: 59.0297, Accuracy: 50.38 %\n",
      "Testing Accuracy: 37.60 %\n",
      "Test Loss:  78.97347593307495\n",
      "Epoch [115/500], Loss: 62.9082, Accuracy: 46.70 %\n",
      "Testing Accuracy: 38.40 %\n",
      "Test Loss:  78.32929265499115\n",
      "Epoch [116/500], Loss: 61.9855, Accuracy: 47.64 %\n",
      "Testing Accuracy: 39.33 %\n",
      "Test Loss:  79.38019645214081\n",
      "Epoch [117/500], Loss: 60.0217, Accuracy: 49.22 %\n",
      "Testing Accuracy: 39.31 %\n",
      "Test Loss:  78.42332017421722\n",
      "Epoch [118/500], Loss: 60.6423, Accuracy: 48.73 %\n",
      "Testing Accuracy: 39.77 %\n",
      "Test Loss:  78.40384519100189\n",
      "Epoch [119/500], Loss: 61.7295, Accuracy: 47.58 %\n",
      "Testing Accuracy: 38.56 %\n",
      "Test Loss:  79.26218283176422\n",
      "Epoch [120/500], Loss: 63.6194, Accuracy: 46.25 %\n",
      "Testing Accuracy: 38.17 %\n",
      "Test Loss:  76.93517446517944\n",
      "Epoch [121/500], Loss: 63.9542, Accuracy: 45.95 %\n",
      "Testing Accuracy: 38.27 %\n",
      "Test Loss:  78.53780937194824\n",
      "Epoch [122/500], Loss: 63.8698, Accuracy: 45.61 %\n",
      "Testing Accuracy: 38.69 %\n",
      "Test Loss:  79.3728414773941\n",
      "Epoch [123/500], Loss: 63.3758, Accuracy: 46.41 %\n",
      "Testing Accuracy: 38.73 %\n",
      "Test Loss:  79.06861782073975\n",
      "Epoch [124/500], Loss: 63.4603, Accuracy: 45.79 %\n",
      "Testing Accuracy: 39.11 %\n",
      "Test Loss:  78.80377972126007\n",
      "Epoch [125/500], Loss: 63.3373, Accuracy: 46.18 %\n",
      "Testing Accuracy: 38.09 %\n",
      "Test Loss:  77.46714627742767\n",
      "Epoch [126/500], Loss: 64.9316, Accuracy: 44.56 %\n",
      "Testing Accuracy: 38.28 %\n",
      "Test Loss:  78.26323235034943\n",
      "Epoch [127/500], Loss: 65.3880, Accuracy: 44.63 %\n",
      "Testing Accuracy: 38.50 %\n",
      "Test Loss:  77.35320997238159\n",
      "Epoch [128/500], Loss: 64.1999, Accuracy: 45.61 %\n",
      "Testing Accuracy: 38.52 %\n",
      "Test Loss:  77.62851870059967\n",
      "Epoch [129/500], Loss: 64.5415, Accuracy: 45.03 %\n",
      "Testing Accuracy: 39.06 %\n",
      "Test Loss:  78.43117654323578\n",
      "Epoch [130/500], Loss: 63.5459, Accuracy: 46.16 %\n",
      "Testing Accuracy: 40.31 %\n",
      "Test Loss:  76.30040431022644\n",
      "Epoch [131/500], Loss: 64.0134, Accuracy: 46.14 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  79.8792096376419\n",
      "Epoch [132/500], Loss: 65.1350, Accuracy: 45.12 %\n",
      "Testing Accuracy: 37.67 %\n",
      "Test Loss:  77.94073033332825\n",
      "Epoch [133/500], Loss: 65.0283, Accuracy: 45.14 %\n",
      "Testing Accuracy: 39.29 %\n",
      "Test Loss:  78.2120829820633\n",
      "Epoch [134/500], Loss: 64.8811, Accuracy: 44.77 %\n",
      "Testing Accuracy: 39.74 %\n",
      "Test Loss:  75.40340387821198\n",
      "Epoch [135/500], Loss: 64.5910, Accuracy: 45.11 %\n",
      "Testing Accuracy: 39.35 %\n",
      "Test Loss:  77.1902107000351\n",
      "Epoch [136/500], Loss: 67.2643, Accuracy: 43.45 %\n",
      "Testing Accuracy: 38.81 %\n",
      "Test Loss:  76.10729849338531\n",
      "Epoch [137/500], Loss: 66.8851, Accuracy: 43.40 %\n",
      "Testing Accuracy: 38.80 %\n",
      "Test Loss:  76.43526327610016\n",
      "Epoch [138/500], Loss: 67.4730, Accuracy: 42.80 %\n",
      "Testing Accuracy: 39.05 %\n",
      "Test Loss:  76.66841232776642\n",
      "Epoch [139/500], Loss: 67.1612, Accuracy: 43.12 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  76.32973766326904\n",
      "Epoch [140/500], Loss: 65.8872, Accuracy: 44.00 %\n",
      "Testing Accuracy: 40.31 %\n",
      "Test Loss:  75.41561257839203\n",
      "Epoch [141/500], Loss: 65.7739, Accuracy: 44.71 %\n",
      "Testing Accuracy: 39.86 %\n",
      "Test Loss:  75.55252540111542\n",
      "Epoch [142/500], Loss: 67.4591, Accuracy: 43.09 %\n",
      "Testing Accuracy: 39.76 %\n",
      "Test Loss:  75.8472318649292\n",
      "Epoch [143/500], Loss: 66.7816, Accuracy: 43.62 %\n",
      "Testing Accuracy: 39.99 %\n",
      "Test Loss:  76.02540814876556\n",
      "Epoch [144/500], Loss: 67.8520, Accuracy: 42.82 %\n",
      "Testing Accuracy: 38.63 %\n",
      "Test Loss:  77.13364398479462\n",
      "Epoch [145/500], Loss: 69.1632, Accuracy: 42.18 %\n",
      "Testing Accuracy: 38.44 %\n",
      "Test Loss:  78.79421031475067\n",
      "Epoch [146/500], Loss: 70.6221, Accuracy: 40.82 %\n",
      "Testing Accuracy: 38.22 %\n",
      "Test Loss:  78.37023103237152\n",
      "Epoch [147/500], Loss: 69.3497, Accuracy: 41.64 %\n",
      "Testing Accuracy: 38.57 %\n",
      "Test Loss:  77.36815273761749\n",
      "Epoch [148/500], Loss: 69.9456, Accuracy: 40.93 %\n",
      "Testing Accuracy: 38.95 %\n",
      "Test Loss:  76.19822752475739\n",
      "Epoch [149/500], Loss: 69.6258, Accuracy: 41.57 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  75.37455475330353\n",
      "Epoch [150/500], Loss: 68.7012, Accuracy: 42.27 %\n",
      "Testing Accuracy: 38.92 %\n",
      "Test Loss:  77.72999811172485\n",
      "Epoch [151/500], Loss: 68.1712, Accuracy: 42.37 %\n",
      "Testing Accuracy: 39.92 %\n",
      "Test Loss:  75.59896838665009\n",
      "Epoch [152/500], Loss: 67.9081, Accuracy: 42.90 %\n",
      "Testing Accuracy: 38.42 %\n",
      "Test Loss:  76.97763991355896\n",
      "Epoch [153/500], Loss: 68.8171, Accuracy: 42.15 %\n",
      "Testing Accuracy: 40.11 %\n",
      "Test Loss:  74.96141803264618\n",
      "Epoch [154/500], Loss: 69.7652, Accuracy: 41.49 %\n",
      "Testing Accuracy: 38.24 %\n",
      "Test Loss:  78.72778558731079\n",
      "Epoch [155/500], Loss: 70.3711, Accuracy: 40.88 %\n",
      "Testing Accuracy: 39.25 %\n",
      "Test Loss:  76.51028037071228\n",
      "Epoch [156/500], Loss: 69.9016, Accuracy: 41.30 %\n",
      "Testing Accuracy: 39.59 %\n",
      "Test Loss:  77.16509485244751\n",
      "Epoch [157/500], Loss: 69.2368, Accuracy: 41.82 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  75.40277695655823\n",
      "Epoch [158/500], Loss: 71.0198, Accuracy: 40.21 %\n",
      "Testing Accuracy: 39.29 %\n",
      "Test Loss:  75.83799040317535\n",
      "Epoch [159/500], Loss: 70.5694, Accuracy: 40.99 %\n",
      "Testing Accuracy: 38.67 %\n",
      "Test Loss:  77.58949506282806\n",
      "Epoch [160/500], Loss: 70.3342, Accuracy: 40.81 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  75.53061997890472\n",
      "Epoch [161/500], Loss: 69.3378, Accuracy: 41.78 %\n",
      "Testing Accuracy: 39.31 %\n",
      "Test Loss:  76.67653691768646\n",
      "Epoch [162/500], Loss: 70.4588, Accuracy: 40.77 %\n",
      "Testing Accuracy: 39.65 %\n",
      "Test Loss:  76.32948982715607\n",
      "Epoch [163/500], Loss: 69.6193, Accuracy: 41.50 %\n",
      "Testing Accuracy: 40.26 %\n",
      "Test Loss:  76.36853313446045\n",
      "Epoch [164/500], Loss: 69.9198, Accuracy: 41.38 %\n",
      "Testing Accuracy: 40.32 %\n",
      "Test Loss:  76.10234069824219\n",
      "Epoch [165/500], Loss: 69.0419, Accuracy: 42.17 %\n",
      "Testing Accuracy: 39.84 %\n",
      "Test Loss:  75.61675107479095\n",
      "Epoch [166/500], Loss: 69.6436, Accuracy: 41.89 %\n",
      "Testing Accuracy: 39.52 %\n",
      "Test Loss:  75.60130441188812\n",
      "Epoch [167/500], Loss: 70.0254, Accuracy: 41.30 %\n",
      "Testing Accuracy: 40.38 %\n",
      "Test Loss:  75.86893951892853\n",
      "Epoch [168/500], Loss: 69.0878, Accuracy: 41.85 %\n",
      "Testing Accuracy: 40.45 %\n",
      "Test Loss:  74.99093520641327\n",
      "Epoch [169/500], Loss: 68.8957, Accuracy: 41.85 %\n",
      "Testing Accuracy: 40.64 %\n",
      "Test Loss:  74.76120471954346\n",
      "Epoch [170/500], Loss: 68.7713, Accuracy: 42.44 %\n",
      "Testing Accuracy: 40.42 %\n",
      "Test Loss:  75.219571352005\n",
      "Epoch [171/500], Loss: 69.0769, Accuracy: 42.30 %\n",
      "Testing Accuracy: 40.58 %\n",
      "Test Loss:  75.48647832870483\n",
      "Epoch [172/500], Loss: 68.7630, Accuracy: 42.60 %\n",
      "Testing Accuracy: 41.12 %\n",
      "Test Loss:  75.12531244754791\n",
      "Epoch [173/500], Loss: 68.9293, Accuracy: 42.29 %\n",
      "Testing Accuracy: 40.91 %\n",
      "Test Loss:  74.27826118469238\n",
      "Epoch [174/500], Loss: 69.7946, Accuracy: 41.68 %\n",
      "Testing Accuracy: 40.36 %\n",
      "Test Loss:  75.40925109386444\n",
      "Epoch [175/500], Loss: 69.9666, Accuracy: 41.46 %\n",
      "Testing Accuracy: 40.37 %\n",
      "Test Loss:  76.5258082151413\n",
      "Epoch [176/500], Loss: 69.8706, Accuracy: 41.77 %\n",
      "Testing Accuracy: 41.30 %\n",
      "Test Loss:  74.72009789943695\n",
      "Epoch [177/500], Loss: 69.6856, Accuracy: 42.26 %\n",
      "Testing Accuracy: 39.97 %\n",
      "Test Loss:  76.17118227481842\n",
      "Epoch [178/500], Loss: 69.7814, Accuracy: 41.82 %\n",
      "Testing Accuracy: 41.34 %\n",
      "Test Loss:  74.43454277515411\n",
      "Epoch [179/500], Loss: 70.1067, Accuracy: 41.43 %\n",
      "Testing Accuracy: 40.04 %\n",
      "Test Loss:  75.526899933815\n",
      "Epoch [180/500], Loss: 70.1203, Accuracy: 41.80 %\n",
      "Testing Accuracy: 41.27 %\n",
      "Test Loss:  75.63288938999176\n",
      "Epoch [181/500], Loss: 69.4065, Accuracy: 42.22 %\n",
      "Testing Accuracy: 41.57 %\n",
      "Test Loss:  73.8988344669342\n",
      "Epoch [182/500], Loss: 69.3747, Accuracy: 42.33 %\n",
      "Testing Accuracy: 41.59 %\n",
      "Test Loss:  74.15112555027008\n",
      "Epoch [183/500], Loss: 69.4707, Accuracy: 42.21 %\n",
      "Testing Accuracy: 40.98 %\n",
      "Test Loss:  74.61267256736755\n",
      "Epoch [184/500], Loss: 70.1349, Accuracy: 41.89 %\n",
      "Testing Accuracy: 41.84 %\n",
      "Test Loss:  73.76555442810059\n",
      "Epoch [185/500], Loss: 71.1204, Accuracy: 41.30 %\n",
      "Testing Accuracy: 41.23 %\n",
      "Test Loss:  74.40621793270111\n",
      "Epoch [186/500], Loss: 71.1269, Accuracy: 41.52 %\n",
      "Testing Accuracy: 41.60 %\n",
      "Test Loss:  73.98445701599121\n",
      "Epoch [187/500], Loss: 71.4275, Accuracy: 41.02 %\n",
      "Testing Accuracy: 39.87 %\n",
      "Test Loss:  75.96918404102325\n",
      "Epoch [188/500], Loss: 70.9435, Accuracy: 41.70 %\n",
      "Testing Accuracy: 41.67 %\n",
      "Test Loss:  74.63834202289581\n",
      "Epoch [189/500], Loss: 70.8612, Accuracy: 41.65 %\n",
      "Testing Accuracy: 40.91 %\n",
      "Test Loss:  74.67478358745575\n",
      "Epoch [190/500], Loss: 70.6218, Accuracy: 41.94 %\n",
      "Testing Accuracy: 41.12 %\n",
      "Test Loss:  74.88550794124603\n",
      "Epoch [191/500], Loss: 70.1843, Accuracy: 41.94 %\n",
      "Testing Accuracy: 41.30 %\n",
      "Test Loss:  74.27585685253143\n",
      "Epoch [192/500], Loss: 70.0531, Accuracy: 41.97 %\n",
      "Testing Accuracy: 40.60 %\n",
      "Test Loss:  75.21294021606445\n",
      "Epoch [193/500], Loss: 70.3362, Accuracy: 42.09 %\n",
      "Testing Accuracy: 41.34 %\n",
      "Test Loss:  74.7013031244278\n",
      "Epoch [194/500], Loss: 71.8734, Accuracy: 40.89 %\n",
      "Testing Accuracy: 41.76 %\n",
      "Test Loss:  73.95466947555542\n",
      "Epoch [195/500], Loss: 69.9940, Accuracy: 42.36 %\n",
      "Testing Accuracy: 40.67 %\n",
      "Test Loss:  75.04388988018036\n",
      "Epoch [196/500], Loss: 69.9068, Accuracy: 42.31 %\n",
      "Testing Accuracy: 41.15 %\n",
      "Test Loss:  74.45710337162018\n",
      "Epoch [197/500], Loss: 69.8981, Accuracy: 42.12 %\n",
      "Testing Accuracy: 42.22 %\n",
      "Test Loss:  73.54534649848938\n",
      "Epoch [198/500], Loss: 69.6486, Accuracy: 42.56 %\n",
      "Testing Accuracy: 41.90 %\n",
      "Test Loss:  74.26057934761047\n",
      "Epoch [199/500], Loss: 70.6598, Accuracy: 42.08 %\n",
      "Testing Accuracy: 41.51 %\n",
      "Test Loss:  73.97492730617523\n",
      "Epoch [200/500], Loss: 70.5304, Accuracy: 42.04 %\n",
      "Testing Accuracy: 42.36 %\n",
      "Test Loss:  73.11207854747772\n",
      "Epoch [201/500], Loss: 69.8683, Accuracy: 42.47 %\n",
      "Testing Accuracy: 41.72 %\n",
      "Test Loss:  74.04591178894043\n",
      "Epoch [202/500], Loss: 69.5149, Accuracy: 42.69 %\n",
      "Testing Accuracy: 42.19 %\n",
      "Test Loss:  73.01896119117737\n",
      "Epoch [203/500], Loss: 70.1864, Accuracy: 42.28 %\n",
      "Testing Accuracy: 42.48 %\n",
      "Test Loss:  73.16556930541992\n",
      "Epoch [204/500], Loss: 69.4423, Accuracy: 42.72 %\n",
      "Testing Accuracy: 41.02 %\n",
      "Test Loss:  74.60193800926208\n",
      "Epoch [205/500], Loss: 69.5758, Accuracy: 42.66 %\n",
      "Testing Accuracy: 41.48 %\n",
      "Test Loss:  74.82753074169159\n",
      "Epoch [206/500], Loss: 69.6647, Accuracy: 42.46 %\n",
      "Testing Accuracy: 41.49 %\n",
      "Test Loss:  74.53777241706848\n",
      "Epoch [207/500], Loss: 69.4321, Accuracy: 42.87 %\n",
      "Testing Accuracy: 42.14 %\n",
      "Test Loss:  73.82229924201965\n",
      "Epoch [208/500], Loss: 68.5409, Accuracy: 43.14 %\n",
      "Testing Accuracy: 41.73 %\n",
      "Test Loss:  74.74717390537262\n",
      "Epoch [209/500], Loss: 68.9875, Accuracy: 42.88 %\n",
      "Testing Accuracy: 42.14 %\n",
      "Test Loss:  73.48893141746521\n",
      "Epoch [210/500], Loss: 68.0580, Accuracy: 43.58 %\n",
      "Testing Accuracy: 41.86 %\n",
      "Test Loss:  73.05754137039185\n",
      "Epoch [211/500], Loss: 68.3331, Accuracy: 43.35 %\n",
      "Testing Accuracy: 41.76 %\n",
      "Test Loss:  73.0665625333786\n",
      "Epoch [212/500], Loss: 68.1779, Accuracy: 43.58 %\n",
      "Testing Accuracy: 40.77 %\n",
      "Test Loss:  75.07726871967316\n",
      "Epoch [213/500], Loss: 68.1802, Accuracy: 43.20 %\n",
      "Testing Accuracy: 43.17 %\n",
      "Test Loss:  72.32867503166199\n",
      "Epoch [214/500], Loss: 67.7953, Accuracy: 43.77 %\n",
      "Testing Accuracy: 43.05 %\n",
      "Test Loss:  72.36918783187866\n",
      "Epoch [215/500], Loss: 68.5452, Accuracy: 43.20 %\n",
      "Testing Accuracy: 42.09 %\n",
      "Test Loss:  73.90163218975067\n",
      "Epoch [216/500], Loss: 67.8797, Accuracy: 43.69 %\n",
      "Testing Accuracy: 42.35 %\n",
      "Test Loss:  73.7902375459671\n",
      "Epoch [217/500], Loss: 67.6943, Accuracy: 44.19 %\n",
      "Testing Accuracy: 42.60 %\n",
      "Test Loss:  72.71147346496582\n",
      "Epoch [218/500], Loss: 67.7539, Accuracy: 44.07 %\n",
      "Testing Accuracy: 42.95 %\n",
      "Test Loss:  72.25595676898956\n",
      "Epoch [219/500], Loss: 67.7193, Accuracy: 44.01 %\n",
      "Testing Accuracy: 42.10 %\n",
      "Test Loss:  73.18101704120636\n",
      "Epoch [220/500], Loss: 67.7891, Accuracy: 43.93 %\n",
      "Testing Accuracy: 41.22 %\n",
      "Test Loss:  74.94391000270844\n",
      "Epoch [221/500], Loss: 68.2856, Accuracy: 43.59 %\n",
      "Testing Accuracy: 42.37 %\n",
      "Test Loss:  72.55355644226074\n",
      "Epoch [222/500], Loss: 67.0961, Accuracy: 44.48 %\n",
      "Testing Accuracy: 42.39 %\n",
      "Test Loss:  73.23723185062408\n",
      "Epoch [223/500], Loss: 67.4252, Accuracy: 44.26 %\n",
      "Testing Accuracy: 41.21 %\n",
      "Test Loss:  73.48023891448975\n",
      "Epoch [224/500], Loss: 67.7581, Accuracy: 44.01 %\n",
      "Testing Accuracy: 40.96 %\n",
      "Test Loss:  73.72410571575165\n",
      "Epoch [225/500], Loss: 67.8224, Accuracy: 43.82 %\n",
      "Testing Accuracy: 42.32 %\n",
      "Test Loss:  73.90373718738556\n",
      "Epoch [226/500], Loss: 67.2078, Accuracy: 44.56 %\n",
      "Testing Accuracy: 43.11 %\n",
      "Test Loss:  72.07163536548615\n",
      "Epoch [227/500], Loss: 67.5947, Accuracy: 44.42 %\n",
      "Testing Accuracy: 41.82 %\n",
      "Test Loss:  74.0690016746521\n",
      "Epoch [228/500], Loss: 67.1022, Accuracy: 44.63 %\n",
      "Testing Accuracy: 43.66 %\n",
      "Test Loss:  72.01143157482147\n",
      "Epoch [229/500], Loss: 67.1410, Accuracy: 44.23 %\n",
      "Testing Accuracy: 43.12 %\n",
      "Test Loss:  72.26625144481659\n",
      "Epoch [230/500], Loss: 67.0192, Accuracy: 44.56 %\n",
      "Testing Accuracy: 43.55 %\n",
      "Test Loss:  71.99314785003662\n",
      "Epoch [231/500], Loss: 66.4880, Accuracy: 44.99 %\n",
      "Testing Accuracy: 42.97 %\n",
      "Test Loss:  72.69684934616089\n",
      "Epoch [232/500], Loss: 67.0639, Accuracy: 44.57 %\n",
      "Testing Accuracy: 42.97 %\n",
      "Test Loss:  72.9928228855133\n",
      "Epoch [233/500], Loss: 67.1756, Accuracy: 44.44 %\n",
      "Testing Accuracy: 42.06 %\n",
      "Test Loss:  73.8515202999115\n",
      "Epoch [234/500], Loss: 67.9200, Accuracy: 44.28 %\n",
      "Testing Accuracy: 41.75 %\n",
      "Test Loss:  73.663414478302\n",
      "Epoch [235/500], Loss: 69.1409, Accuracy: 43.30 %\n",
      "Testing Accuracy: 42.29 %\n",
      "Test Loss:  73.21692216396332\n",
      "Epoch [236/500], Loss: 69.0192, Accuracy: 43.12 %\n",
      "Testing Accuracy: 42.26 %\n",
      "Test Loss:  72.91523838043213\n",
      "Epoch [237/500], Loss: 69.0832, Accuracy: 43.25 %\n",
      "Testing Accuracy: 42.21 %\n",
      "Test Loss:  73.27581560611725\n",
      "Epoch [238/500], Loss: 68.5584, Accuracy: 43.69 %\n",
      "Testing Accuracy: 42.76 %\n",
      "Test Loss:  72.60960352420807\n",
      "Epoch [239/500], Loss: 68.4963, Accuracy: 43.50 %\n",
      "Testing Accuracy: 42.25 %\n",
      "Test Loss:  72.50486254692078\n",
      "Epoch [240/500], Loss: 68.4268, Accuracy: 43.53 %\n",
      "Testing Accuracy: 42.10 %\n",
      "Test Loss:  73.02123892307281\n",
      "Epoch [241/500], Loss: 68.7881, Accuracy: 43.40 %\n",
      "Testing Accuracy: 42.53 %\n",
      "Test Loss:  73.5287811756134\n",
      "Epoch [242/500], Loss: 68.4998, Accuracy: 43.53 %\n",
      "Testing Accuracy: 41.61 %\n",
      "Test Loss:  72.95271706581116\n",
      "Epoch [243/500], Loss: 67.9971, Accuracy: 44.07 %\n",
      "Testing Accuracy: 41.93 %\n",
      "Test Loss:  73.9522715806961\n",
      "Epoch [244/500], Loss: 68.7948, Accuracy: 43.72 %\n",
      "Testing Accuracy: 41.50 %\n",
      "Test Loss:  73.44227921962738\n",
      "Epoch [245/500], Loss: 70.7566, Accuracy: 42.45 %\n",
      "Testing Accuracy: 42.08 %\n",
      "Test Loss:  73.68426060676575\n",
      "Epoch [246/500], Loss: 69.8129, Accuracy: 42.75 %\n",
      "Testing Accuracy: 42.97 %\n",
      "Test Loss:  72.657102227211\n",
      "Epoch [247/500], Loss: 69.1260, Accuracy: 43.31 %\n",
      "Testing Accuracy: 41.04 %\n",
      "Test Loss:  73.83473575115204\n",
      "Epoch [248/500], Loss: 71.6543, Accuracy: 41.58 %\n",
      "Testing Accuracy: 38.91 %\n",
      "Test Loss:  78.03377866744995\n",
      "Epoch [249/500], Loss: 71.9887, Accuracy: 41.57 %\n",
      "Testing Accuracy: 38.29 %\n",
      "Test Loss:  76.30141246318817\n",
      "Epoch [250/500], Loss: 70.4318, Accuracy: 42.45 %\n",
      "Testing Accuracy: 42.04 %\n",
      "Test Loss:  72.70406377315521\n",
      "Testing Accuracy: 42.04 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        716800   \n",
      "Net/Dropout[dropout]/onnx::Relu   700      \n",
      "Net/Linear[fc2]/onnx::Gemm        3850     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "721,350 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885]\n",
      "pruning hidden size:  350\n",
      "with hidden layer:  350\n",
      "removing:  (189, 291, 21)\n",
      "--- 36.90122389793396 seconds ---\n",
      "Epoch [1/500], Loss: 71.2672, Accuracy: 42.43 %\n",
      "Testing Accuracy: 41.76 %\n",
      "Test Loss:  74.8119832277298\n",
      "Epoch [2/500], Loss: 70.8756, Accuracy: 42.46 %\n",
      "Testing Accuracy: 41.92 %\n",
      "Test Loss:  73.63851749897003\n",
      "Epoch [3/500], Loss: 70.5670, Accuracy: 42.54 %\n",
      "Testing Accuracy: 40.87 %\n",
      "Test Loss:  74.33056247234344\n",
      "Epoch [4/500], Loss: 71.3687, Accuracy: 41.75 %\n",
      "Testing Accuracy: 42.33 %\n",
      "Test Loss:  73.43865752220154\n",
      "Epoch [5/500], Loss: 71.1890, Accuracy: 41.72 %\n",
      "Testing Accuracy: 40.47 %\n",
      "Test Loss:  74.51147031784058\n",
      "Epoch [6/500], Loss: 71.7612, Accuracy: 41.52 %\n",
      "Testing Accuracy: 40.77 %\n",
      "Test Loss:  75.45268034934998\n",
      "Epoch [7/500], Loss: 72.7535, Accuracy: 40.97 %\n",
      "Testing Accuracy: 40.51 %\n",
      "Test Loss:  75.32246375083923\n",
      "Epoch [8/500], Loss: 72.0720, Accuracy: 41.60 %\n",
      "Testing Accuracy: 42.66 %\n",
      "Test Loss:  73.52632749080658\n",
      "Epoch [9/500], Loss: 71.3996, Accuracy: 41.87 %\n",
      "Testing Accuracy: 43.24 %\n",
      "Test Loss:  72.23689234256744\n",
      "Epoch [10/500], Loss: 71.3967, Accuracy: 42.19 %\n",
      "Testing Accuracy: 41.59 %\n",
      "Test Loss:  74.10308790206909\n",
      "Epoch [11/500], Loss: 72.4196, Accuracy: 41.57 %\n",
      "Testing Accuracy: 39.92 %\n",
      "Test Loss:  75.67449903488159\n",
      "Epoch [12/500], Loss: 72.2797, Accuracy: 41.15 %\n",
      "Testing Accuracy: 42.68 %\n",
      "Test Loss:  73.20268535614014\n",
      "Epoch [13/500], Loss: 71.9797, Accuracy: 41.48 %\n",
      "Testing Accuracy: 43.34 %\n",
      "Test Loss:  72.18914830684662\n",
      "Epoch [14/500], Loss: 72.1510, Accuracy: 41.27 %\n",
      "Testing Accuracy: 41.19 %\n",
      "Test Loss:  74.24545240402222\n",
      "Epoch [15/500], Loss: 72.1404, Accuracy: 41.30 %\n",
      "Testing Accuracy: 43.21 %\n",
      "Test Loss:  73.06763434410095\n",
      "Epoch [16/500], Loss: 71.4935, Accuracy: 41.73 %\n",
      "Testing Accuracy: 42.91 %\n",
      "Test Loss:  72.8861632347107\n",
      "Epoch [17/500], Loss: 71.1647, Accuracy: 42.08 %\n",
      "Testing Accuracy: 42.78 %\n",
      "Test Loss:  72.31807851791382\n",
      "Epoch [18/500], Loss: 71.2930, Accuracy: 42.29 %\n",
      "Testing Accuracy: 42.38 %\n",
      "Test Loss:  72.87654197216034\n",
      "Epoch [19/500], Loss: 72.3524, Accuracy: 41.14 %\n",
      "Testing Accuracy: 41.40 %\n",
      "Test Loss:  73.70673108100891\n",
      "Epoch [20/500], Loss: 71.5098, Accuracy: 41.64 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  73.6535382270813\n",
      "Epoch [21/500], Loss: 71.2554, Accuracy: 42.11 %\n",
      "Testing Accuracy: 42.79 %\n",
      "Test Loss:  72.60331571102142\n",
      "Epoch [22/500], Loss: 70.7611, Accuracy: 42.81 %\n",
      "Testing Accuracy: 42.02 %\n",
      "Test Loss:  73.16810274124146\n",
      "Epoch [23/500], Loss: 70.9394, Accuracy: 42.40 %\n",
      "Testing Accuracy: 42.60 %\n",
      "Test Loss:  73.04564487934113\n",
      "Epoch [24/500], Loss: 70.9115, Accuracy: 42.51 %\n",
      "Testing Accuracy: 42.04 %\n",
      "Test Loss:  73.27745318412781\n",
      "Epoch [25/500], Loss: 71.0353, Accuracy: 42.20 %\n",
      "Testing Accuracy: 41.58 %\n",
      "Test Loss:  74.33790957927704\n",
      "Epoch [26/500], Loss: 73.8186, Accuracy: 40.03 %\n",
      "Testing Accuracy: 40.22 %\n",
      "Test Loss:  75.18651473522186\n",
      "Epoch [27/500], Loss: 73.4199, Accuracy: 39.96 %\n",
      "Testing Accuracy: 40.89 %\n",
      "Test Loss:  74.11776602268219\n",
      "Epoch [28/500], Loss: 73.2567, Accuracy: 40.14 %\n",
      "Testing Accuracy: 39.43 %\n",
      "Test Loss:  75.15173149108887\n",
      "Epoch [29/500], Loss: 73.5684, Accuracy: 39.99 %\n",
      "Testing Accuracy: 41.47 %\n",
      "Test Loss:  74.24275052547455\n",
      "Epoch [30/500], Loss: 72.8164, Accuracy: 40.70 %\n",
      "Testing Accuracy: 42.23 %\n",
      "Test Loss:  73.05948829650879\n",
      "Epoch [31/500], Loss: 73.3718, Accuracy: 40.08 %\n",
      "Testing Accuracy: 40.62 %\n",
      "Test Loss:  73.9822508096695\n",
      "Epoch [32/500], Loss: 73.5031, Accuracy: 40.23 %\n",
      "Testing Accuracy: 41.45 %\n",
      "Test Loss:  73.74638164043427\n",
      "Epoch [33/500], Loss: 73.0808, Accuracy: 40.41 %\n",
      "Testing Accuracy: 41.37 %\n",
      "Test Loss:  73.65518009662628\n",
      "Testing Accuracy: 41.37 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        710656   \n",
      "Net/Dropout[dropout]/onnx::Relu   694      \n",
      "Net/Linear[fc2]/onnx::Gemm        3817     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "715,167 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149]\n",
      "pruning hidden size:  347\n",
      "with hidden layer:  347\n",
      "removing:  (99, 87, 27)\n",
      "--- 35.620203256607056 seconds ---\n",
      "Epoch [1/500], Loss: 74.4423, Accuracy: 39.28 %\n",
      "Testing Accuracy: 41.11 %\n",
      "Test Loss:  75.45587038993835\n",
      "Epoch [2/500], Loss: 74.2954, Accuracy: 39.74 %\n",
      "Testing Accuracy: 40.46 %\n",
      "Test Loss:  74.81625008583069\n",
      "Epoch [3/500], Loss: 74.0649, Accuracy: 39.87 %\n",
      "Testing Accuracy: 40.26 %\n",
      "Test Loss:  74.08472299575806\n",
      "Epoch [4/500], Loss: 73.7588, Accuracy: 39.69 %\n",
      "Testing Accuracy: 41.80 %\n",
      "Test Loss:  74.41867196559906\n",
      "Epoch [5/500], Loss: 73.5731, Accuracy: 40.36 %\n",
      "Testing Accuracy: 42.16 %\n",
      "Test Loss:  73.42348122596741\n",
      "Epoch [6/500], Loss: 73.6794, Accuracy: 40.03 %\n",
      "Testing Accuracy: 42.11 %\n",
      "Test Loss:  73.22830581665039\n",
      "Epoch [7/500], Loss: 73.5648, Accuracy: 40.28 %\n",
      "Testing Accuracy: 40.11 %\n",
      "Test Loss:  74.49868619441986\n",
      "Epoch [8/500], Loss: 74.4889, Accuracy: 40.05 %\n",
      "Testing Accuracy: 41.32 %\n",
      "Test Loss:  75.02670085430145\n",
      "Epoch [9/500], Loss: 73.5190, Accuracy: 40.12 %\n",
      "Testing Accuracy: 42.00 %\n",
      "Test Loss:  73.34782826900482\n",
      "Epoch [10/500], Loss: 73.5252, Accuracy: 40.41 %\n",
      "Testing Accuracy: 41.96 %\n",
      "Test Loss:  73.83094346523285\n",
      "Epoch [11/500], Loss: 73.0138, Accuracy: 40.86 %\n",
      "Testing Accuracy: 40.97 %\n",
      "Test Loss:  73.86107397079468\n",
      "Epoch [12/500], Loss: 72.9261, Accuracy: 40.88 %\n",
      "Testing Accuracy: 41.95 %\n",
      "Test Loss:  73.89402186870575\n",
      "Epoch [13/500], Loss: 73.4070, Accuracy: 40.24 %\n",
      "Testing Accuracy: 42.53 %\n",
      "Test Loss:  73.25964617729187\n",
      "Epoch [14/500], Loss: 73.1166, Accuracy: 40.56 %\n",
      "Testing Accuracy: 38.71 %\n",
      "Test Loss:  76.59173369407654\n",
      "Epoch [15/500], Loss: 73.6720, Accuracy: 40.53 %\n",
      "Testing Accuracy: 42.23 %\n",
      "Test Loss:  73.56352365016937\n",
      "Epoch [16/500], Loss: 73.2159, Accuracy: 40.41 %\n",
      "Testing Accuracy: 42.15 %\n",
      "Test Loss:  73.46623241901398\n",
      "Epoch [17/500], Loss: 73.2313, Accuracy: 40.36 %\n",
      "Testing Accuracy: 42.52 %\n",
      "Test Loss:  73.40843904018402\n",
      "Epoch [18/500], Loss: 73.9478, Accuracy: 39.87 %\n",
      "Testing Accuracy: 40.55 %\n",
      "Test Loss:  75.48251116275787\n",
      "Epoch [19/500], Loss: 75.1636, Accuracy: 38.60 %\n",
      "Testing Accuracy: 40.05 %\n",
      "Test Loss:  75.34501218795776\n",
      "Epoch [20/500], Loss: 75.8921, Accuracy: 38.05 %\n",
      "Testing Accuracy: 38.53 %\n",
      "Test Loss:  76.45562839508057\n",
      "Epoch [21/500], Loss: 75.4509, Accuracy: 38.42 %\n",
      "Testing Accuracy: 40.52 %\n",
      "Test Loss:  75.84509575366974\n",
      "Epoch [22/500], Loss: 75.2868, Accuracy: 38.43 %\n",
      "Testing Accuracy: 41.08 %\n",
      "Test Loss:  74.12918901443481\n",
      "Epoch [23/500], Loss: 74.5316, Accuracy: 38.87 %\n",
      "Testing Accuracy: 40.87 %\n",
      "Test Loss:  73.73356032371521\n",
      "Epoch [24/500], Loss: 75.0993, Accuracy: 38.72 %\n",
      "Testing Accuracy: 39.05 %\n",
      "Test Loss:  75.84463083744049\n",
      "Epoch [25/500], Loss: 74.7936, Accuracy: 38.71 %\n",
      "Testing Accuracy: 40.79 %\n",
      "Test Loss:  74.17763364315033\n",
      "Epoch [26/500], Loss: 74.7294, Accuracy: 38.92 %\n",
      "Testing Accuracy: 39.11 %\n",
      "Test Loss:  75.80470097064972\n",
      "Testing Accuracy: 39.11 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        704512   \n",
      "Net/Dropout[dropout]/onnx::Relu   688      \n",
      "Net/Linear[fc2]/onnx::Gemm        3784     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "708,984 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396]\n",
      "pruning hidden size:  344\n",
      "with hidden layer:  344\n",
      "removing:  (228, 207, 18)\n",
      "--- 35.21896243095398 seconds ---\n",
      "Epoch [1/500], Loss: 78.3946, Accuracy: 35.60 %\n",
      "Testing Accuracy: 37.95 %\n",
      "Test Loss:  77.46202898025513\n",
      "Epoch [2/500], Loss: 77.6270, Accuracy: 36.37 %\n",
      "Testing Accuracy: 38.68 %\n",
      "Test Loss:  77.38523840904236\n",
      "Epoch [3/500], Loss: 77.8231, Accuracy: 36.67 %\n",
      "Testing Accuracy: 37.35 %\n",
      "Test Loss:  77.66028106212616\n",
      "Epoch [4/500], Loss: 78.0526, Accuracy: 36.16 %\n",
      "Testing Accuracy: 38.84 %\n",
      "Test Loss:  77.35417699813843\n",
      "Epoch [5/500], Loss: 77.9541, Accuracy: 36.36 %\n",
      "Testing Accuracy: 39.35 %\n",
      "Test Loss:  76.37226068973541\n",
      "Epoch [6/500], Loss: 77.5027, Accuracy: 36.93 %\n",
      "Testing Accuracy: 36.56 %\n",
      "Test Loss:  78.57912111282349\n",
      "Epoch [7/500], Loss: 79.1649, Accuracy: 34.94 %\n",
      "Testing Accuracy: 37.39 %\n",
      "Test Loss:  77.76370775699615\n",
      "Epoch [8/500], Loss: 77.7123, Accuracy: 36.65 %\n",
      "Testing Accuracy: 38.61 %\n",
      "Test Loss:  76.56501030921936\n",
      "Epoch [9/500], Loss: 77.2870, Accuracy: 37.10 %\n",
      "Testing Accuracy: 38.79 %\n",
      "Test Loss:  76.01046788692474\n",
      "Epoch [10/500], Loss: 76.8452, Accuracy: 37.43 %\n",
      "Testing Accuracy: 40.25 %\n",
      "Test Loss:  76.03747880458832\n",
      "Epoch [11/500], Loss: 77.1256, Accuracy: 37.36 %\n",
      "Testing Accuracy: 38.52 %\n",
      "Test Loss:  77.10121822357178\n",
      "Epoch [12/500], Loss: 77.7375, Accuracy: 36.60 %\n",
      "Testing Accuracy: 38.79 %\n",
      "Test Loss:  77.50223100185394\n",
      "Epoch [13/500], Loss: 76.8457, Accuracy: 37.60 %\n",
      "Testing Accuracy: 39.80 %\n",
      "Test Loss:  75.63607513904572\n",
      "Epoch [14/500], Loss: 77.7259, Accuracy: 37.17 %\n",
      "Testing Accuracy: 39.34 %\n",
      "Test Loss:  76.74101507663727\n",
      "Epoch [15/500], Loss: 77.4116, Accuracy: 36.93 %\n",
      "Testing Accuracy: 37.83 %\n",
      "Test Loss:  79.59028565883636\n",
      "Epoch [16/500], Loss: 77.6872, Accuracy: 36.80 %\n",
      "Testing Accuracy: 37.14 %\n",
      "Test Loss:  78.08623909950256\n",
      "Epoch [17/500], Loss: 76.4812, Accuracy: 37.76 %\n",
      "Testing Accuracy: 38.81 %\n",
      "Test Loss:  76.4537250995636\n",
      "Epoch [18/500], Loss: 77.4119, Accuracy: 36.93 %\n",
      "Testing Accuracy: 40.01 %\n",
      "Test Loss:  75.62091958522797\n",
      "Epoch [19/500], Loss: 77.1795, Accuracy: 37.34 %\n",
      "Testing Accuracy: 41.49 %\n",
      "Test Loss:  74.49318945407867\n",
      "Epoch [20/500], Loss: 76.1245, Accuracy: 38.04 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  76.35876274108887\n",
      "Epoch [21/500], Loss: 76.2231, Accuracy: 38.14 %\n",
      "Testing Accuracy: 40.05 %\n",
      "Test Loss:  74.5824887752533\n",
      "Epoch [22/500], Loss: 75.9730, Accuracy: 38.30 %\n",
      "Testing Accuracy: 40.18 %\n",
      "Test Loss:  74.73809766769409\n",
      "Epoch [23/500], Loss: 76.3100, Accuracy: 38.22 %\n",
      "Testing Accuracy: 40.15 %\n",
      "Test Loss:  76.00782608985901\n",
      "Epoch [24/500], Loss: 76.7857, Accuracy: 37.74 %\n",
      "Testing Accuracy: 40.42 %\n",
      "Test Loss:  74.66482543945312\n",
      "Epoch [25/500], Loss: 77.1089, Accuracy: 37.43 %\n",
      "Testing Accuracy: 39.10 %\n",
      "Test Loss:  75.97128975391388\n",
      "Epoch [26/500], Loss: 77.1070, Accuracy: 37.17 %\n",
      "Testing Accuracy: 40.37 %\n",
      "Test Loss:  74.98612189292908\n",
      "Epoch [27/500], Loss: 75.6996, Accuracy: 38.72 %\n",
      "Testing Accuracy: 41.11 %\n",
      "Test Loss:  74.71008718013763\n",
      "Epoch [28/500], Loss: 76.0576, Accuracy: 38.30 %\n",
      "Testing Accuracy: 41.45 %\n",
      "Test Loss:  74.59267044067383\n",
      "Epoch [29/500], Loss: 76.3114, Accuracy: 38.23 %\n",
      "Testing Accuracy: 40.40 %\n",
      "Test Loss:  75.13623297214508\n",
      "Epoch [30/500], Loss: 76.5509, Accuracy: 37.54 %\n",
      "Testing Accuracy: 40.36 %\n",
      "Test Loss:  75.39984691143036\n",
      "Epoch [31/500], Loss: 75.6181, Accuracy: 38.67 %\n",
      "Testing Accuracy: 39.44 %\n",
      "Test Loss:  75.26513028144836\n",
      "Epoch [32/500], Loss: 76.0582, Accuracy: 38.19 %\n",
      "Testing Accuracy: 38.37 %\n",
      "Test Loss:  78.76083755493164\n",
      "Epoch [33/500], Loss: 75.9032, Accuracy: 38.46 %\n",
      "Testing Accuracy: 39.24 %\n",
      "Test Loss:  76.0958468914032\n",
      "Epoch [34/500], Loss: 77.2920, Accuracy: 37.11 %\n",
      "Testing Accuracy: 40.96 %\n",
      "Test Loss:  75.29461884498596\n",
      "Epoch [35/500], Loss: 76.0305, Accuracy: 38.34 %\n",
      "Testing Accuracy: 41.23 %\n",
      "Test Loss:  74.70943450927734\n",
      "Epoch [36/500], Loss: 75.6078, Accuracy: 38.66 %\n",
      "Testing Accuracy: 40.97 %\n",
      "Test Loss:  74.24661362171173\n",
      "Epoch [37/500], Loss: 76.1881, Accuracy: 38.33 %\n",
      "Testing Accuracy: 41.04 %\n",
      "Test Loss:  74.97988557815552\n",
      "Epoch [38/500], Loss: 75.3630, Accuracy: 39.02 %\n",
      "Testing Accuracy: 41.92 %\n",
      "Test Loss:  73.61065721511841\n",
      "Epoch [39/500], Loss: 75.0922, Accuracy: 39.02 %\n",
      "Testing Accuracy: 41.69 %\n",
      "Test Loss:  74.7600771188736\n",
      "Epoch [40/500], Loss: 75.6694, Accuracy: 38.92 %\n",
      "Testing Accuracy: 41.23 %\n",
      "Test Loss:  75.06160640716553\n",
      "Epoch [41/500], Loss: 76.5870, Accuracy: 38.23 %\n",
      "Testing Accuracy: 38.11 %\n",
      "Test Loss:  76.02860641479492\n",
      "Epoch [42/500], Loss: 75.3896, Accuracy: 38.82 %\n",
      "Testing Accuracy: 41.53 %\n",
      "Test Loss:  73.97383224964142\n",
      "Epoch [43/500], Loss: 75.1085, Accuracy: 39.26 %\n",
      "Testing Accuracy: 41.61 %\n",
      "Test Loss:  73.92202234268188\n",
      "Epoch [44/500], Loss: 74.7405, Accuracy: 39.65 %\n",
      "Testing Accuracy: 42.27 %\n",
      "Test Loss:  73.80523681640625\n",
      "Epoch [45/500], Loss: 74.6224, Accuracy: 39.67 %\n",
      "Testing Accuracy: 40.75 %\n",
      "Test Loss:  73.96593904495239\n",
      "Epoch [46/500], Loss: 75.2764, Accuracy: 38.99 %\n",
      "Testing Accuracy: 40.77 %\n",
      "Test Loss:  74.7531327009201\n",
      "Epoch [47/500], Loss: 75.3624, Accuracy: 38.85 %\n",
      "Testing Accuracy: 41.97 %\n",
      "Test Loss:  73.95980751514435\n",
      "Epoch [48/500], Loss: 74.9096, Accuracy: 39.51 %\n",
      "Testing Accuracy: 40.22 %\n",
      "Test Loss:  75.80202949047089\n",
      "Epoch [49/500], Loss: 77.9195, Accuracy: 36.99 %\n",
      "Testing Accuracy: 40.15 %\n",
      "Test Loss:  76.40564131736755\n",
      "Epoch [50/500], Loss: 76.7719, Accuracy: 37.96 %\n",
      "Testing Accuracy: 40.36 %\n",
      "Test Loss:  75.08569300174713\n",
      "Epoch [51/500], Loss: 77.2803, Accuracy: 37.94 %\n",
      "Testing Accuracy: 40.59 %\n",
      "Test Loss:  75.32321405410767\n",
      "Epoch [52/500], Loss: 76.2633, Accuracy: 38.29 %\n",
      "Testing Accuracy: 41.11 %\n",
      "Test Loss:  75.29397261142731\n",
      "Epoch [53/500], Loss: 76.2417, Accuracy: 38.75 %\n",
      "Testing Accuracy: 39.97 %\n",
      "Test Loss:  75.35204994678497\n",
      "Epoch [54/500], Loss: 77.2981, Accuracy: 37.90 %\n",
      "Testing Accuracy: 39.93 %\n",
      "Test Loss:  75.57623326778412\n",
      "Epoch [55/500], Loss: 76.4742, Accuracy: 38.52 %\n",
      "Testing Accuracy: 40.51 %\n",
      "Test Loss:  75.22242772579193\n",
      "Epoch [56/500], Loss: 76.8480, Accuracy: 38.06 %\n",
      "Testing Accuracy: 40.47 %\n",
      "Test Loss:  74.79087853431702\n",
      "Epoch [57/500], Loss: 76.4069, Accuracy: 38.25 %\n",
      "Testing Accuracy: 41.26 %\n",
      "Test Loss:  75.1150324344635\n",
      "Epoch [58/500], Loss: 76.0710, Accuracy: 38.42 %\n",
      "Testing Accuracy: 40.95 %\n",
      "Test Loss:  75.37137222290039\n",
      "Testing Accuracy: 40.95 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        698368   \n",
      "Net/Dropout[dropout]/onnx::Relu   682      \n",
      "Net/Linear[fc2]/onnx::Gemm        3751     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "702,801 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006]\n",
      "pruning hidden size:  341\n",
      "with hidden layer:  341\n",
      "removing:  (137, 322, 2)\n",
      "--- 33.722811460494995 seconds ---\n",
      "Epoch [1/500], Loss: 79.2746, Accuracy: 36.04 %\n",
      "Testing Accuracy: 37.85 %\n",
      "Test Loss:  79.94816243648529\n",
      "Epoch [2/500], Loss: 79.0012, Accuracy: 36.37 %\n",
      "Testing Accuracy: 37.91 %\n",
      "Test Loss:  77.61994779109955\n",
      "Epoch [3/500], Loss: 78.8181, Accuracy: 35.93 %\n",
      "Testing Accuracy: 38.07 %\n",
      "Test Loss:  77.02540576457977\n",
      "Epoch [4/500], Loss: 78.2467, Accuracy: 36.52 %\n",
      "Testing Accuracy: 38.22 %\n",
      "Test Loss:  77.1222335100174\n",
      "Epoch [5/500], Loss: 78.5536, Accuracy: 36.13 %\n",
      "Testing Accuracy: 39.10 %\n",
      "Test Loss:  76.4787780046463\n",
      "Epoch [6/500], Loss: 78.3887, Accuracy: 36.33 %\n",
      "Testing Accuracy: 39.50 %\n",
      "Test Loss:  76.34189665317535\n",
      "Epoch [7/500], Loss: 78.1861, Accuracy: 36.72 %\n",
      "Testing Accuracy: 38.81 %\n",
      "Test Loss:  76.93029963970184\n",
      "Epoch [8/500], Loss: 78.1949, Accuracy: 36.52 %\n",
      "Testing Accuracy: 38.76 %\n",
      "Test Loss:  77.0579559803009\n",
      "Epoch [9/500], Loss: 78.4697, Accuracy: 36.16 %\n",
      "Testing Accuracy: 37.98 %\n",
      "Test Loss:  79.23663175106049\n",
      "Epoch [10/500], Loss: 78.6688, Accuracy: 35.98 %\n",
      "Testing Accuracy: 37.93 %\n",
      "Test Loss:  77.28008008003235\n",
      "Epoch [11/500], Loss: 78.0196, Accuracy: 36.26 %\n",
      "Testing Accuracy: 38.08 %\n",
      "Test Loss:  76.90882158279419\n",
      "Epoch [12/500], Loss: 77.9220, Accuracy: 36.41 %\n",
      "Testing Accuracy: 38.19 %\n",
      "Test Loss:  76.44680714607239\n",
      "Epoch [13/500], Loss: 77.9561, Accuracy: 36.58 %\n",
      "Testing Accuracy: 37.98 %\n",
      "Test Loss:  76.69732439517975\n",
      "Epoch [14/500], Loss: 78.0783, Accuracy: 36.11 %\n",
      "Testing Accuracy: 39.92 %\n",
      "Test Loss:  75.74163687229156\n",
      "Epoch [15/500], Loss: 78.4694, Accuracy: 36.12 %\n",
      "Testing Accuracy: 37.91 %\n",
      "Test Loss:  77.9387172460556\n",
      "Epoch [16/500], Loss: 78.4273, Accuracy: 36.19 %\n",
      "Testing Accuracy: 39.49 %\n",
      "Test Loss:  76.21496093273163\n",
      "Epoch [17/500], Loss: 77.6884, Accuracy: 36.90 %\n",
      "Testing Accuracy: 38.65 %\n",
      "Test Loss:  77.30167937278748\n",
      "Epoch [18/500], Loss: 77.9111, Accuracy: 36.40 %\n",
      "Testing Accuracy: 39.84 %\n",
      "Test Loss:  75.40438234806061\n",
      "Epoch [19/500], Loss: 77.5301, Accuracy: 36.85 %\n",
      "Testing Accuracy: 39.44 %\n",
      "Test Loss:  75.53270494937897\n",
      "Epoch [20/500], Loss: 77.1843, Accuracy: 37.25 %\n",
      "Testing Accuracy: 37.53 %\n",
      "Test Loss:  77.34969198703766\n",
      "Epoch [21/500], Loss: 78.1262, Accuracy: 36.36 %\n",
      "Testing Accuracy: 36.79 %\n",
      "Test Loss:  79.28318166732788\n",
      "Epoch [22/500], Loss: 77.8386, Accuracy: 36.65 %\n",
      "Testing Accuracy: 39.76 %\n",
      "Test Loss:  75.98923301696777\n",
      "Epoch [23/500], Loss: 77.3847, Accuracy: 37.03 %\n",
      "Testing Accuracy: 37.71 %\n",
      "Test Loss:  76.99557316303253\n",
      "Epoch [24/500], Loss: 77.7436, Accuracy: 36.43 %\n",
      "Testing Accuracy: 39.27 %\n",
      "Test Loss:  75.9492369890213\n",
      "Epoch [25/500], Loss: 78.0518, Accuracy: 36.48 %\n",
      "Testing Accuracy: 38.37 %\n",
      "Test Loss:  76.57970702648163\n",
      "Epoch [26/500], Loss: 77.4103, Accuracy: 36.80 %\n",
      "Testing Accuracy: 39.84 %\n",
      "Test Loss:  75.56680297851562\n",
      "Epoch [27/500], Loss: 78.3138, Accuracy: 36.18 %\n",
      "Testing Accuracy: 38.22 %\n",
      "Test Loss:  77.0169825553894\n",
      "Epoch [28/500], Loss: 77.2457, Accuracy: 36.80 %\n",
      "Testing Accuracy: 39.00 %\n",
      "Test Loss:  76.46722376346588\n",
      "Epoch [29/500], Loss: 77.0776, Accuracy: 37.18 %\n",
      "Testing Accuracy: 39.68 %\n",
      "Test Loss:  75.7525624036789\n",
      "Epoch [30/500], Loss: 77.7901, Accuracy: 36.17 %\n",
      "Testing Accuracy: 38.69 %\n",
      "Test Loss:  76.08339536190033\n",
      "Epoch [31/500], Loss: 77.2696, Accuracy: 37.19 %\n",
      "Testing Accuracy: 38.93 %\n",
      "Test Loss:  77.01633775234222\n",
      "Epoch [32/500], Loss: 77.3363, Accuracy: 36.95 %\n",
      "Testing Accuracy: 40.03 %\n",
      "Test Loss:  75.09751296043396\n",
      "Epoch [33/500], Loss: 78.1447, Accuracy: 36.50 %\n",
      "Testing Accuracy: 39.01 %\n",
      "Test Loss:  75.502032995224\n",
      "Epoch [34/500], Loss: 77.0199, Accuracy: 37.37 %\n",
      "Testing Accuracy: 39.09 %\n",
      "Test Loss:  76.62361586093903\n",
      "Epoch [35/500], Loss: 77.0620, Accuracy: 37.20 %\n",
      "Testing Accuracy: 38.54 %\n",
      "Test Loss:  76.6024979352951\n",
      "Epoch [36/500], Loss: 76.8249, Accuracy: 37.65 %\n",
      "Testing Accuracy: 38.60 %\n",
      "Test Loss:  76.30744922161102\n",
      "Epoch [37/500], Loss: 77.1978, Accuracy: 37.03 %\n",
      "Testing Accuracy: 40.22 %\n",
      "Test Loss:  74.95850336551666\n",
      "Epoch [38/500], Loss: 77.4479, Accuracy: 37.18 %\n",
      "Testing Accuracy: 37.27 %\n",
      "Test Loss:  77.5938892364502\n",
      "Epoch [39/500], Loss: 77.1816, Accuracy: 36.91 %\n",
      "Testing Accuracy: 39.43 %\n",
      "Test Loss:  74.98188579082489\n",
      "Epoch [40/500], Loss: 76.6951, Accuracy: 37.83 %\n",
      "Testing Accuracy: 40.09 %\n",
      "Test Loss:  75.27027583122253\n",
      "Epoch [41/500], Loss: 76.7873, Accuracy: 37.61 %\n",
      "Testing Accuracy: 39.08 %\n",
      "Test Loss:  75.95810151100159\n",
      "Epoch [42/500], Loss: 77.2920, Accuracy: 37.62 %\n",
      "Testing Accuracy: 40.52 %\n",
      "Test Loss:  74.41688525676727\n",
      "Epoch [43/500], Loss: 77.2325, Accuracy: 37.37 %\n",
      "Testing Accuracy: 32.55 %\n",
      "Test Loss:  79.92829442024231\n",
      "Epoch [44/500], Loss: 78.6413, Accuracy: 33.78 %\n",
      "Testing Accuracy: 35.85 %\n",
      "Test Loss:  76.74857676029205\n",
      "Epoch [45/500], Loss: 78.6215, Accuracy: 34.19 %\n",
      "Testing Accuracy: 35.29 %\n",
      "Test Loss:  77.96332705020905\n",
      "Epoch [46/500], Loss: 78.9829, Accuracy: 34.11 %\n",
      "Testing Accuracy: 35.40 %\n",
      "Test Loss:  78.81106495857239\n",
      "Epoch [47/500], Loss: 78.5778, Accuracy: 34.66 %\n",
      "Testing Accuracy: 36.65 %\n",
      "Test Loss:  77.60837030410767\n",
      "Epoch [48/500], Loss: 78.2091, Accuracy: 34.95 %\n",
      "Testing Accuracy: 37.31 %\n",
      "Test Loss:  75.93822276592255\n",
      "Epoch [49/500], Loss: 78.0776, Accuracy: 34.98 %\n",
      "Testing Accuracy: 35.91 %\n",
      "Test Loss:  77.40278899669647\n",
      "Epoch [50/500], Loss: 77.9471, Accuracy: 34.84 %\n",
      "Testing Accuracy: 37.78 %\n",
      "Test Loss:  76.00170135498047\n",
      "Epoch [51/500], Loss: 78.2236, Accuracy: 34.84 %\n",
      "Testing Accuracy: 37.29 %\n",
      "Test Loss:  76.36998653411865\n",
      "Epoch [52/500], Loss: 78.5247, Accuracy: 34.43 %\n",
      "Testing Accuracy: 36.49 %\n",
      "Test Loss:  79.247061252594\n",
      "Epoch [53/500], Loss: 78.3825, Accuracy: 34.85 %\n",
      "Testing Accuracy: 36.79 %\n",
      "Test Loss:  76.49625384807587\n",
      "Epoch [54/500], Loss: 78.1881, Accuracy: 34.98 %\n",
      "Testing Accuracy: 37.25 %\n",
      "Test Loss:  76.81813716888428\n",
      "Epoch [55/500], Loss: 77.5644, Accuracy: 35.14 %\n",
      "Testing Accuracy: 38.08 %\n",
      "Test Loss:  75.872687458992\n",
      "Epoch [56/500], Loss: 77.5031, Accuracy: 35.49 %\n",
      "Testing Accuracy: 37.97 %\n",
      "Test Loss:  75.66086983680725\n",
      "Epoch [57/500], Loss: 77.3475, Accuracy: 35.78 %\n",
      "Testing Accuracy: 36.27 %\n",
      "Test Loss:  78.24972689151764\n",
      "Epoch [58/500], Loss: 77.9402, Accuracy: 35.14 %\n",
      "Testing Accuracy: 36.12 %\n",
      "Test Loss:  77.39109098911285\n",
      "Epoch [59/500], Loss: 77.6111, Accuracy: 35.42 %\n",
      "Testing Accuracy: 36.82 %\n",
      "Test Loss:  78.09238886833191\n",
      "Epoch [60/500], Loss: 78.4799, Accuracy: 34.88 %\n",
      "Testing Accuracy: 37.83 %\n",
      "Test Loss:  75.98457455635071\n",
      "Epoch [61/500], Loss: 77.6433, Accuracy: 35.34 %\n",
      "Testing Accuracy: 35.98 %\n",
      "Test Loss:  76.47822725772858\n",
      "Epoch [62/500], Loss: 77.7505, Accuracy: 34.79 %\n",
      "Testing Accuracy: 35.91 %\n",
      "Test Loss:  77.3656632900238\n",
      "Testing Accuracy: 35.91 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        692224   \n",
      "Net/Dropout[dropout]/onnx::Relu   676      \n",
      "Net/Linear[fc2]/onnx::Gemm        3718     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "696,618 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747]\n",
      "pruning hidden size:  338\n",
      "with hidden layer:  338\n",
      "removing:  (256, 211, 105)\n",
      "--- 34.92398810386658 seconds ---\n",
      "Epoch [1/500], Loss: 79.8865, Accuracy: 33.36 %\n",
      "Testing Accuracy: 35.61 %\n",
      "Test Loss:  78.33781456947327\n",
      "Epoch [2/500], Loss: 79.6526, Accuracy: 34.13 %\n",
      "Testing Accuracy: 36.24 %\n",
      "Test Loss:  77.90008807182312\n",
      "Epoch [3/500], Loss: 79.7158, Accuracy: 33.69 %\n",
      "Testing Accuracy: 35.88 %\n",
      "Test Loss:  78.16805720329285\n",
      "Epoch [4/500], Loss: 79.6658, Accuracy: 34.12 %\n",
      "Testing Accuracy: 35.52 %\n",
      "Test Loss:  78.16569972038269\n",
      "Epoch [5/500], Loss: 79.7841, Accuracy: 33.93 %\n",
      "Testing Accuracy: 35.71 %\n",
      "Test Loss:  78.318892121315\n",
      "Epoch [6/500], Loss: 78.8615, Accuracy: 34.37 %\n",
      "Testing Accuracy: 36.77 %\n",
      "Test Loss:  77.60813963413239\n",
      "Epoch [7/500], Loss: 79.0935, Accuracy: 34.18 %\n",
      "Testing Accuracy: 35.60 %\n",
      "Test Loss:  77.75570011138916\n",
      "Epoch [8/500], Loss: 79.6589, Accuracy: 33.75 %\n",
      "Testing Accuracy: 36.65 %\n",
      "Test Loss:  77.78010427951813\n",
      "Epoch [9/500], Loss: 79.1451, Accuracy: 34.47 %\n",
      "Testing Accuracy: 35.87 %\n",
      "Test Loss:  77.73479104042053\n",
      "Epoch [10/500], Loss: 79.0081, Accuracy: 34.36 %\n",
      "Testing Accuracy: 37.01 %\n",
      "Test Loss:  77.95471680164337\n",
      "Epoch [11/500], Loss: 78.9612, Accuracy: 34.53 %\n",
      "Testing Accuracy: 35.74 %\n",
      "Test Loss:  77.80507135391235\n",
      "Epoch [12/500], Loss: 78.8266, Accuracy: 34.49 %\n",
      "Testing Accuracy: 35.79 %\n",
      "Test Loss:  77.67686855792999\n",
      "Epoch [13/500], Loss: 79.2377, Accuracy: 34.09 %\n",
      "Testing Accuracy: 36.83 %\n",
      "Test Loss:  77.23411452770233\n",
      "Epoch [14/500], Loss: 79.1103, Accuracy: 34.65 %\n",
      "Testing Accuracy: 36.23 %\n",
      "Test Loss:  78.12164175510406\n",
      "Epoch [15/500], Loss: 79.4972, Accuracy: 33.85 %\n",
      "Testing Accuracy: 35.42 %\n",
      "Test Loss:  78.98558914661407\n",
      "Epoch [16/500], Loss: 79.1621, Accuracy: 34.53 %\n",
      "Testing Accuracy: 32.18 %\n",
      "Test Loss:  80.18698453903198\n",
      "Epoch [17/500], Loss: 79.7408, Accuracy: 33.75 %\n",
      "Testing Accuracy: 36.17 %\n",
      "Test Loss:  78.13278722763062\n",
      "Epoch [18/500], Loss: 79.1827, Accuracy: 34.47 %\n",
      "Testing Accuracy: 36.55 %\n",
      "Test Loss:  77.30423355102539\n",
      "Epoch [19/500], Loss: 79.4262, Accuracy: 34.08 %\n",
      "Testing Accuracy: 35.40 %\n",
      "Test Loss:  80.1995655298233\n",
      "Epoch [20/500], Loss: 79.1829, Accuracy: 34.35 %\n",
      "Testing Accuracy: 36.74 %\n",
      "Test Loss:  77.640416264534\n",
      "Epoch [21/500], Loss: 78.9402, Accuracy: 34.81 %\n",
      "Testing Accuracy: 37.31 %\n",
      "Test Loss:  76.6382896900177\n",
      "Epoch [22/500], Loss: 78.1882, Accuracy: 35.05 %\n",
      "Testing Accuracy: 36.95 %\n",
      "Test Loss:  76.44937241077423\n",
      "Epoch [23/500], Loss: 78.5954, Accuracy: 34.58 %\n",
      "Testing Accuracy: 36.56 %\n",
      "Test Loss:  77.35549736022949\n",
      "Epoch [24/500], Loss: 78.6012, Accuracy: 34.61 %\n",
      "Testing Accuracy: 36.02 %\n",
      "Test Loss:  77.39343810081482\n",
      "Epoch [25/500], Loss: 78.3584, Accuracy: 34.72 %\n",
      "Testing Accuracy: 37.18 %\n",
      "Test Loss:  77.10356318950653\n",
      "Epoch [26/500], Loss: 78.1288, Accuracy: 35.03 %\n",
      "Testing Accuracy: 36.65 %\n",
      "Test Loss:  77.52614235877991\n",
      "Epoch [27/500], Loss: 78.3577, Accuracy: 34.89 %\n",
      "Testing Accuracy: 35.63 %\n",
      "Test Loss:  77.99435913562775\n",
      "Epoch [28/500], Loss: 78.3507, Accuracy: 34.70 %\n",
      "Testing Accuracy: 36.09 %\n",
      "Test Loss:  77.92994213104248\n",
      "Epoch [29/500], Loss: 79.2495, Accuracy: 34.44 %\n",
      "Testing Accuracy: 37.21 %\n",
      "Test Loss:  76.37725603580475\n",
      "Epoch [30/500], Loss: 78.8943, Accuracy: 34.59 %\n",
      "Testing Accuracy: 37.18 %\n",
      "Test Loss:  76.367143034935\n",
      "Epoch [31/500], Loss: 78.4820, Accuracy: 35.17 %\n",
      "Testing Accuracy: 37.06 %\n",
      "Test Loss:  76.74015641212463\n",
      "Epoch [32/500], Loss: 78.2834, Accuracy: 35.05 %\n",
      "Testing Accuracy: 37.62 %\n",
      "Test Loss:  76.41436529159546\n",
      "Epoch [33/500], Loss: 78.0359, Accuracy: 35.10 %\n",
      "Testing Accuracy: 37.76 %\n",
      "Test Loss:  76.11858701705933\n",
      "Epoch [34/500], Loss: 78.7685, Accuracy: 34.68 %\n",
      "Testing Accuracy: 35.17 %\n",
      "Test Loss:  77.73694145679474\n",
      "Epoch [35/500], Loss: 79.3037, Accuracy: 34.13 %\n",
      "Testing Accuracy: 35.20 %\n",
      "Test Loss:  78.72118198871613\n",
      "Epoch [36/500], Loss: 78.8843, Accuracy: 34.75 %\n",
      "Testing Accuracy: 36.59 %\n",
      "Test Loss:  76.5758296251297\n",
      "Epoch [37/500], Loss: 78.8190, Accuracy: 34.26 %\n",
      "Testing Accuracy: 37.39 %\n",
      "Test Loss:  76.33710372447968\n",
      "Epoch [38/500], Loss: 78.2438, Accuracy: 35.17 %\n",
      "Testing Accuracy: 36.22 %\n",
      "Test Loss:  77.06971442699432\n",
      "Epoch [39/500], Loss: 77.9231, Accuracy: 34.95 %\n",
      "Testing Accuracy: 37.56 %\n",
      "Test Loss:  76.75694060325623\n",
      "Epoch [40/500], Loss: 77.9178, Accuracy: 35.44 %\n",
      "Testing Accuracy: 36.89 %\n",
      "Test Loss:  76.77304494380951\n",
      "Epoch [41/500], Loss: 77.6361, Accuracy: 35.44 %\n",
      "Testing Accuracy: 36.91 %\n",
      "Test Loss:  77.01636600494385\n",
      "Epoch [42/500], Loss: 77.7237, Accuracy: 35.47 %\n",
      "Testing Accuracy: 35.81 %\n",
      "Test Loss:  77.41204082965851\n",
      "Epoch [43/500], Loss: 77.9827, Accuracy: 35.42 %\n",
      "Testing Accuracy: 36.77 %\n",
      "Test Loss:  77.32134461402893\n",
      "Epoch [44/500], Loss: 78.5802, Accuracy: 34.71 %\n",
      "Testing Accuracy: 36.93 %\n",
      "Test Loss:  77.10446083545685\n",
      "Epoch [45/500], Loss: 78.5616, Accuracy: 34.51 %\n",
      "Testing Accuracy: 35.51 %\n",
      "Test Loss:  77.64852511882782\n",
      "Epoch [46/500], Loss: 77.7832, Accuracy: 35.27 %\n",
      "Testing Accuracy: 36.77 %\n",
      "Test Loss:  76.29178726673126\n",
      "Epoch [47/500], Loss: 77.3276, Accuracy: 35.78 %\n",
      "Testing Accuracy: 37.40 %\n",
      "Test Loss:  76.284294962883\n",
      "Epoch [48/500], Loss: 77.9190, Accuracy: 35.32 %\n",
      "Testing Accuracy: 38.30 %\n",
      "Test Loss:  75.94664704799652\n",
      "Epoch [49/500], Loss: 78.0318, Accuracy: 35.52 %\n",
      "Testing Accuracy: 35.62 %\n",
      "Test Loss:  77.48038387298584\n",
      "Epoch [50/500], Loss: 78.3159, Accuracy: 35.00 %\n",
      "Testing Accuracy: 36.80 %\n",
      "Test Loss:  76.88935697078705\n",
      "Epoch [51/500], Loss: 77.3795, Accuracy: 35.78 %\n",
      "Testing Accuracy: 37.41 %\n",
      "Test Loss:  76.62530529499054\n",
      "Epoch [52/500], Loss: 77.3746, Accuracy: 35.77 %\n",
      "Testing Accuracy: 38.05 %\n",
      "Test Loss:  75.4906507730484\n",
      "Epoch [53/500], Loss: 77.6552, Accuracy: 35.60 %\n",
      "Testing Accuracy: 37.45 %\n",
      "Test Loss:  77.36448383331299\n",
      "Epoch [54/500], Loss: 77.1812, Accuracy: 36.04 %\n",
      "Testing Accuracy: 37.94 %\n",
      "Test Loss:  76.57121407985687\n",
      "Epoch [55/500], Loss: 77.5885, Accuracy: 35.85 %\n",
      "Testing Accuracy: 35.56 %\n",
      "Test Loss:  76.71048867702484\n",
      "Epoch [56/500], Loss: 77.1466, Accuracy: 36.22 %\n",
      "Testing Accuracy: 37.00 %\n",
      "Test Loss:  77.30331766605377\n",
      "Epoch [57/500], Loss: 77.0376, Accuracy: 36.11 %\n",
      "Testing Accuracy: 38.04 %\n",
      "Test Loss:  75.8127475976944\n",
      "Epoch [58/500], Loss: 76.8937, Accuracy: 36.39 %\n",
      "Testing Accuracy: 38.36 %\n",
      "Test Loss:  76.53145015239716\n",
      "Epoch [59/500], Loss: 76.2220, Accuracy: 36.90 %\n",
      "Testing Accuracy: 37.39 %\n",
      "Test Loss:  76.31503522396088\n",
      "Epoch [60/500], Loss: 77.1247, Accuracy: 36.19 %\n",
      "Testing Accuracy: 36.17 %\n",
      "Test Loss:  77.09351849555969\n",
      "Epoch [61/500], Loss: 77.4873, Accuracy: 35.67 %\n",
      "Testing Accuracy: 39.18 %\n",
      "Test Loss:  74.57371473312378\n",
      "Epoch [62/500], Loss: 76.4429, Accuracy: 36.66 %\n",
      "Testing Accuracy: 39.35 %\n",
      "Test Loss:  75.10930252075195\n",
      "Epoch [63/500], Loss: 76.2378, Accuracy: 37.21 %\n",
      "Testing Accuracy: 39.01 %\n",
      "Test Loss:  75.05783557891846\n",
      "Epoch [64/500], Loss: 76.0530, Accuracy: 37.24 %\n",
      "Testing Accuracy: 37.35 %\n",
      "Test Loss:  75.76575779914856\n",
      "Epoch [65/500], Loss: 76.4076, Accuracy: 36.89 %\n",
      "Testing Accuracy: 38.21 %\n",
      "Test Loss:  75.56611204147339\n",
      "Epoch [66/500], Loss: 76.8878, Accuracy: 36.58 %\n",
      "Testing Accuracy: 36.49 %\n",
      "Test Loss:  78.0343576669693\n",
      "Epoch [67/500], Loss: 76.3565, Accuracy: 36.73 %\n",
      "Testing Accuracy: 39.24 %\n",
      "Test Loss:  74.55791580677032\n",
      "Epoch [68/500], Loss: 76.0813, Accuracy: 37.03 %\n",
      "Testing Accuracy: 38.07 %\n",
      "Test Loss:  75.50729477405548\n",
      "Epoch [69/500], Loss: 76.4969, Accuracy: 36.73 %\n",
      "Testing Accuracy: 38.48 %\n",
      "Test Loss:  75.54210841655731\n",
      "Epoch [70/500], Loss: 76.8288, Accuracy: 36.70 %\n",
      "Testing Accuracy: 39.70 %\n",
      "Test Loss:  74.4111328125\n",
      "Epoch [71/500], Loss: 75.8599, Accuracy: 37.19 %\n",
      "Testing Accuracy: 38.47 %\n",
      "Test Loss:  76.52359342575073\n",
      "Epoch [72/500], Loss: 75.9641, Accuracy: 37.25 %\n",
      "Testing Accuracy: 39.55 %\n",
      "Test Loss:  74.1750340461731\n",
      "Epoch [73/500], Loss: 76.4417, Accuracy: 36.70 %\n",
      "Testing Accuracy: 38.61 %\n",
      "Test Loss:  74.86991822719574\n",
      "Epoch [74/500], Loss: 76.2634, Accuracy: 36.81 %\n",
      "Testing Accuracy: 38.09 %\n",
      "Test Loss:  75.19321596622467\n",
      "Epoch [75/500], Loss: 76.3277, Accuracy: 36.94 %\n",
      "Testing Accuracy: 36.53 %\n",
      "Test Loss:  76.68575477600098\n",
      "Epoch [76/500], Loss: 76.1747, Accuracy: 36.77 %\n",
      "Testing Accuracy: 38.24 %\n",
      "Test Loss:  75.13763737678528\n",
      "Epoch [77/500], Loss: 76.7479, Accuracy: 36.72 %\n",
      "Testing Accuracy: 40.25 %\n",
      "Test Loss:  73.95464158058167\n",
      "Epoch [78/500], Loss: 75.7466, Accuracy: 37.48 %\n",
      "Testing Accuracy: 39.05 %\n",
      "Test Loss:  74.69634532928467\n",
      "Epoch [79/500], Loss: 75.7078, Accuracy: 37.50 %\n",
      "Testing Accuracy: 39.50 %\n",
      "Test Loss:  74.90101611614227\n",
      "Epoch [80/500], Loss: 75.7515, Accuracy: 37.38 %\n",
      "Testing Accuracy: 39.76 %\n",
      "Test Loss:  74.02629232406616\n",
      "Epoch [81/500], Loss: 75.5216, Accuracy: 37.38 %\n",
      "Testing Accuracy: 38.96 %\n",
      "Test Loss:  75.42491340637207\n",
      "Epoch [82/500], Loss: 76.2611, Accuracy: 37.11 %\n",
      "Testing Accuracy: 39.33 %\n",
      "Test Loss:  74.63989341259003\n",
      "Epoch [83/500], Loss: 75.7791, Accuracy: 37.51 %\n",
      "Testing Accuracy: 38.63 %\n",
      "Test Loss:  75.55846703052521\n",
      "Epoch [84/500], Loss: 76.4878, Accuracy: 36.93 %\n",
      "Testing Accuracy: 39.53 %\n",
      "Test Loss:  74.23772299289703\n",
      "Epoch [85/500], Loss: 75.9152, Accuracy: 37.57 %\n",
      "Testing Accuracy: 39.65 %\n",
      "Test Loss:  74.72608852386475\n",
      "Epoch [86/500], Loss: 75.8317, Accuracy: 37.17 %\n",
      "Testing Accuracy: 40.48 %\n",
      "Test Loss:  73.71230220794678\n",
      "Epoch [87/500], Loss: 75.1801, Accuracy: 37.89 %\n",
      "Testing Accuracy: 39.02 %\n",
      "Test Loss:  74.98159289360046\n",
      "Epoch [88/500], Loss: 77.0312, Accuracy: 36.23 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  74.48816478252411\n",
      "Epoch [89/500], Loss: 75.5654, Accuracy: 37.51 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  74.1001467704773\n",
      "Epoch [90/500], Loss: 75.8941, Accuracy: 37.26 %\n",
      "Testing Accuracy: 39.39 %\n",
      "Test Loss:  74.87894260883331\n",
      "Epoch [91/500], Loss: 75.8421, Accuracy: 36.97 %\n",
      "Testing Accuracy: 37.94 %\n",
      "Test Loss:  76.98450243473053\n",
      "Epoch [92/500], Loss: 76.3920, Accuracy: 37.02 %\n",
      "Testing Accuracy: 38.84 %\n",
      "Test Loss:  74.95372939109802\n",
      "Epoch [93/500], Loss: 75.8865, Accuracy: 37.21 %\n",
      "Testing Accuracy: 38.86 %\n",
      "Test Loss:  74.9047429561615\n",
      "Epoch [94/500], Loss: 75.7200, Accuracy: 37.52 %\n",
      "Testing Accuracy: 36.88 %\n",
      "Test Loss:  75.87745821475983\n",
      "Epoch [95/500], Loss: 76.4753, Accuracy: 36.54 %\n",
      "Testing Accuracy: 38.73 %\n",
      "Test Loss:  75.5809907913208\n",
      "Epoch [96/500], Loss: 76.0607, Accuracy: 36.94 %\n",
      "Testing Accuracy: 40.14 %\n",
      "Test Loss:  73.85687506198883\n",
      "Epoch [97/500], Loss: 75.3465, Accuracy: 37.73 %\n",
      "Testing Accuracy: 39.99 %\n",
      "Test Loss:  74.2550858259201\n",
      "Epoch [98/500], Loss: 75.7762, Accuracy: 37.39 %\n",
      "Testing Accuracy: 39.88 %\n",
      "Test Loss:  73.83480834960938\n",
      "Epoch [99/500], Loss: 75.1714, Accuracy: 37.79 %\n",
      "Testing Accuracy: 40.23 %\n",
      "Test Loss:  74.12616956233978\n",
      "Epoch [100/500], Loss: 75.4912, Accuracy: 37.43 %\n",
      "Testing Accuracy: 38.25 %\n",
      "Test Loss:  74.85056734085083\n",
      "Epoch [101/500], Loss: 75.2639, Accuracy: 37.83 %\n",
      "Testing Accuracy: 39.46 %\n",
      "Test Loss:  74.66976356506348\n",
      "Epoch [102/500], Loss: 75.5486, Accuracy: 37.60 %\n",
      "Testing Accuracy: 38.24 %\n",
      "Test Loss:  75.6127678155899\n",
      "Epoch [103/500], Loss: 76.4303, Accuracy: 36.55 %\n",
      "Testing Accuracy: 37.73 %\n",
      "Test Loss:  75.9839916229248\n",
      "Epoch [104/500], Loss: 74.9503, Accuracy: 38.14 %\n",
      "Testing Accuracy: 40.60 %\n",
      "Test Loss:  73.45607078075409\n",
      "Epoch [105/500], Loss: 75.5687, Accuracy: 37.27 %\n",
      "Testing Accuracy: 36.01 %\n",
      "Test Loss:  76.27297675609589\n",
      "Epoch [106/500], Loss: 76.1423, Accuracy: 37.02 %\n",
      "Testing Accuracy: 38.74 %\n",
      "Test Loss:  74.88157510757446\n",
      "Epoch [107/500], Loss: 75.6338, Accuracy: 37.36 %\n",
      "Testing Accuracy: 37.35 %\n",
      "Test Loss:  75.90497052669525\n",
      "Epoch [108/500], Loss: 75.6013, Accuracy: 37.44 %\n",
      "Testing Accuracy: 38.42 %\n",
      "Test Loss:  75.18876135349274\n",
      "Epoch [109/500], Loss: 75.3502, Accuracy: 37.73 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  74.05144035816193\n",
      "Epoch [110/500], Loss: 75.4190, Accuracy: 37.40 %\n",
      "Testing Accuracy: 39.11 %\n",
      "Test Loss:  75.07906436920166\n",
      "Epoch [111/500], Loss: 75.2936, Accuracy: 37.53 %\n",
      "Testing Accuracy: 39.55 %\n",
      "Test Loss:  73.97191286087036\n",
      "Epoch [112/500], Loss: 75.7574, Accuracy: 37.64 %\n",
      "Testing Accuracy: 40.51 %\n",
      "Test Loss:  73.50053858757019\n",
      "Epoch [113/500], Loss: 75.7813, Accuracy: 37.15 %\n",
      "Testing Accuracy: 38.75 %\n",
      "Test Loss:  74.46717238426208\n",
      "Epoch [114/500], Loss: 75.4226, Accuracy: 37.48 %\n",
      "Testing Accuracy: 39.12 %\n",
      "Test Loss:  74.18104755878448\n",
      "Epoch [115/500], Loss: 75.1479, Accuracy: 37.72 %\n",
      "Testing Accuracy: 38.29 %\n",
      "Test Loss:  74.99289035797119\n",
      "Epoch [116/500], Loss: 75.0741, Accuracy: 37.92 %\n",
      "Testing Accuracy: 38.47 %\n",
      "Test Loss:  75.64528441429138\n",
      "Epoch [117/500], Loss: 74.7659, Accuracy: 38.37 %\n",
      "Testing Accuracy: 37.91 %\n",
      "Test Loss:  75.99927055835724\n",
      "Epoch [118/500], Loss: 75.1207, Accuracy: 37.95 %\n",
      "Testing Accuracy: 39.57 %\n",
      "Test Loss:  74.07719421386719\n",
      "Epoch [119/500], Loss: 75.4506, Accuracy: 37.45 %\n",
      "Testing Accuracy: 39.88 %\n",
      "Test Loss:  74.78839123249054\n",
      "Epoch [120/500], Loss: 75.3355, Accuracy: 37.61 %\n",
      "Testing Accuracy: 38.99 %\n",
      "Test Loss:  73.8927412033081\n",
      "Epoch [121/500], Loss: 75.0020, Accuracy: 37.95 %\n",
      "Testing Accuracy: 40.20 %\n",
      "Test Loss:  73.55742466449738\n",
      "Epoch [122/500], Loss: 75.2951, Accuracy: 37.47 %\n",
      "Testing Accuracy: 38.63 %\n",
      "Test Loss:  75.02098846435547\n",
      "Epoch [123/500], Loss: 75.7315, Accuracy: 37.54 %\n",
      "Testing Accuracy: 38.02 %\n",
      "Test Loss:  76.34191024303436\n",
      "Epoch [124/500], Loss: 75.3205, Accuracy: 37.55 %\n",
      "Testing Accuracy: 40.15 %\n",
      "Test Loss:  73.41971182823181\n",
      "Epoch [125/500], Loss: 74.7853, Accuracy: 38.27 %\n",
      "Testing Accuracy: 39.50 %\n",
      "Test Loss:  75.40119087696075\n",
      "Epoch [126/500], Loss: 74.9039, Accuracy: 38.21 %\n",
      "Testing Accuracy: 39.76 %\n",
      "Test Loss:  73.70276999473572\n",
      "Epoch [127/500], Loss: 75.1860, Accuracy: 37.94 %\n",
      "Testing Accuracy: 38.68 %\n",
      "Test Loss:  74.07688307762146\n",
      "Epoch [128/500], Loss: 75.2791, Accuracy: 37.70 %\n",
      "Testing Accuracy: 38.22 %\n",
      "Test Loss:  74.95882773399353\n",
      "Epoch [129/500], Loss: 74.6919, Accuracy: 38.11 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  73.98614537715912\n",
      "Epoch [130/500], Loss: 74.7170, Accuracy: 38.15 %\n",
      "Testing Accuracy: 39.19 %\n",
      "Test Loss:  75.52487826347351\n",
      "Epoch [131/500], Loss: 75.3099, Accuracy: 38.10 %\n",
      "Testing Accuracy: 38.62 %\n",
      "Test Loss:  74.6285548210144\n",
      "Epoch [132/500], Loss: 75.1652, Accuracy: 37.77 %\n",
      "Testing Accuracy: 38.65 %\n",
      "Test Loss:  74.72927248477936\n",
      "Epoch [133/500], Loss: 75.1109, Accuracy: 38.05 %\n",
      "Testing Accuracy: 39.95 %\n",
      "Test Loss:  73.60413122177124\n",
      "Epoch [134/500], Loss: 74.7497, Accuracy: 38.32 %\n",
      "Testing Accuracy: 40.61 %\n",
      "Test Loss:  73.0402067899704\n",
      "Epoch [135/500], Loss: 74.9816, Accuracy: 38.14 %\n",
      "Testing Accuracy: 38.73 %\n",
      "Test Loss:  74.90614199638367\n",
      "Epoch [136/500], Loss: 74.6085, Accuracy: 38.57 %\n",
      "Testing Accuracy: 40.17 %\n",
      "Test Loss:  73.33932137489319\n",
      "Epoch [137/500], Loss: 74.6962, Accuracy: 38.12 %\n",
      "Testing Accuracy: 38.87 %\n",
      "Test Loss:  75.13708519935608\n",
      "Epoch [138/500], Loss: 75.0148, Accuracy: 38.01 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  73.96357119083405\n",
      "Epoch [139/500], Loss: 74.3116, Accuracy: 38.38 %\n",
      "Testing Accuracy: 40.52 %\n",
      "Test Loss:  73.60324144363403\n",
      "Epoch [140/500], Loss: 74.5053, Accuracy: 38.27 %\n",
      "Testing Accuracy: 37.50 %\n",
      "Test Loss:  76.18488228321075\n",
      "Epoch [141/500], Loss: 75.7331, Accuracy: 37.47 %\n",
      "Testing Accuracy: 34.18 %\n",
      "Test Loss:  77.90755069255829\n",
      "Epoch [142/500], Loss: 76.0610, Accuracy: 36.87 %\n",
      "Testing Accuracy: 38.67 %\n",
      "Test Loss:  74.2678120136261\n",
      "Epoch [143/500], Loss: 74.9763, Accuracy: 37.86 %\n",
      "Testing Accuracy: 39.83 %\n",
      "Test Loss:  73.62863838672638\n",
      "Epoch [144/500], Loss: 74.5968, Accuracy: 38.15 %\n",
      "Testing Accuracy: 39.57 %\n",
      "Test Loss:  74.0017204284668\n",
      "Epoch [145/500], Loss: 74.7762, Accuracy: 38.05 %\n",
      "Testing Accuracy: 40.53 %\n",
      "Test Loss:  73.42530477046967\n",
      "Epoch [146/500], Loss: 74.5264, Accuracy: 38.34 %\n",
      "Testing Accuracy: 39.99 %\n",
      "Test Loss:  73.47306025028229\n",
      "Epoch [147/500], Loss: 74.1112, Accuracy: 38.53 %\n",
      "Testing Accuracy: 38.92 %\n",
      "Test Loss:  74.67529678344727\n",
      "Epoch [148/500], Loss: 74.4875, Accuracy: 38.59 %\n",
      "Testing Accuracy: 40.10 %\n",
      "Test Loss:  74.18611907958984\n",
      "Epoch [149/500], Loss: 74.6868, Accuracy: 38.20 %\n",
      "Testing Accuracy: 38.57 %\n",
      "Test Loss:  74.25597560405731\n",
      "Epoch [150/500], Loss: 74.5430, Accuracy: 38.07 %\n",
      "Testing Accuracy: 40.09 %\n",
      "Test Loss:  73.20497596263885\n",
      "Epoch [151/500], Loss: 74.4017, Accuracy: 38.39 %\n",
      "Testing Accuracy: 39.08 %\n",
      "Test Loss:  74.09798812866211\n",
      "Epoch [152/500], Loss: 74.9879, Accuracy: 37.90 %\n",
      "Testing Accuracy: 36.16 %\n",
      "Test Loss:  76.04957556724548\n",
      "Epoch [153/500], Loss: 74.6129, Accuracy: 38.23 %\n",
      "Testing Accuracy: 37.68 %\n",
      "Test Loss:  75.44528889656067\n",
      "Epoch [154/500], Loss: 74.2598, Accuracy: 38.59 %\n",
      "Testing Accuracy: 38.07 %\n",
      "Test Loss:  74.60764575004578\n",
      "Testing Accuracy: 38.07 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        686080   \n",
      "Net/Dropout[dropout]/onnx::Relu   670      \n",
      "Net/Linear[fc2]/onnx::Gemm        3685     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "690,435 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188]\n",
      "pruning hidden size:  335\n",
      "with hidden layer:  335\n",
      "removing:  (317, 201, 187)\n",
      "--- 33.742276191711426 seconds ---\n",
      "Epoch [1/500], Loss: 79.9016, Accuracy: 32.26 %\n",
      "Testing Accuracy: 34.02 %\n",
      "Test Loss:  78.90551590919495\n",
      "Epoch [2/500], Loss: 78.1096, Accuracy: 34.71 %\n",
      "Testing Accuracy: 35.39 %\n",
      "Test Loss:  77.99452042579651\n",
      "Epoch [3/500], Loss: 77.3103, Accuracy: 35.24 %\n",
      "Testing Accuracy: 38.19 %\n",
      "Test Loss:  75.29598248004913\n",
      "Epoch [4/500], Loss: 76.7587, Accuracy: 35.76 %\n",
      "Testing Accuracy: 37.73 %\n",
      "Test Loss:  75.29002690315247\n",
      "Epoch [5/500], Loss: 76.5039, Accuracy: 35.63 %\n",
      "Testing Accuracy: 37.33 %\n",
      "Test Loss:  76.43057191371918\n",
      "Epoch [6/500], Loss: 76.9202, Accuracy: 35.80 %\n",
      "Testing Accuracy: 37.84 %\n",
      "Test Loss:  75.51589715480804\n",
      "Epoch [7/500], Loss: 76.4896, Accuracy: 35.74 %\n",
      "Testing Accuracy: 35.88 %\n",
      "Test Loss:  76.39036810398102\n",
      "Epoch [8/500], Loss: 77.4211, Accuracy: 35.17 %\n",
      "Testing Accuracy: 37.25 %\n",
      "Test Loss:  75.51370632648468\n",
      "Epoch [9/500], Loss: 76.4506, Accuracy: 35.80 %\n",
      "Testing Accuracy: 35.64 %\n",
      "Test Loss:  76.8817949295044\n",
      "Epoch [10/500], Loss: 76.6656, Accuracy: 35.74 %\n",
      "Testing Accuracy: 37.87 %\n",
      "Test Loss:  74.98023676872253\n",
      "Epoch [11/500], Loss: 76.5726, Accuracy: 35.95 %\n",
      "Testing Accuracy: 36.93 %\n",
      "Test Loss:  76.33736300468445\n",
      "Epoch [12/500], Loss: 76.6892, Accuracy: 35.82 %\n",
      "Testing Accuracy: 37.41 %\n",
      "Test Loss:  75.83009111881256\n",
      "Epoch [13/500], Loss: 76.5340, Accuracy: 36.16 %\n",
      "Testing Accuracy: 37.64 %\n",
      "Test Loss:  76.16300225257874\n",
      "Epoch [14/500], Loss: 76.0376, Accuracy: 36.27 %\n",
      "Testing Accuracy: 37.80 %\n",
      "Test Loss:  75.11465549468994\n",
      "Epoch [15/500], Loss: 76.2977, Accuracy: 36.67 %\n",
      "Testing Accuracy: 36.03 %\n",
      "Test Loss:  76.2071224451065\n",
      "Epoch [16/500], Loss: 76.8044, Accuracy: 35.57 %\n",
      "Testing Accuracy: 37.89 %\n",
      "Test Loss:  75.12400889396667\n",
      "Epoch [17/500], Loss: 77.2199, Accuracy: 35.30 %\n",
      "Testing Accuracy: 37.63 %\n",
      "Test Loss:  74.99274802207947\n",
      "Epoch [18/500], Loss: 76.7295, Accuracy: 35.80 %\n",
      "Testing Accuracy: 34.13 %\n",
      "Test Loss:  78.01415383815765\n",
      "Epoch [19/500], Loss: 76.7611, Accuracy: 36.12 %\n",
      "Testing Accuracy: 37.05 %\n",
      "Test Loss:  76.23872315883636\n",
      "Epoch [20/500], Loss: 76.1994, Accuracy: 36.42 %\n",
      "Testing Accuracy: 36.81 %\n",
      "Test Loss:  75.8707605600357\n",
      "Epoch [21/500], Loss: 76.1872, Accuracy: 36.50 %\n",
      "Testing Accuracy: 35.91 %\n",
      "Test Loss:  76.6061600446701\n",
      "Epoch [22/500], Loss: 77.0779, Accuracy: 35.78 %\n",
      "Testing Accuracy: 36.84 %\n",
      "Test Loss:  76.50859129428864\n",
      "Epoch [23/500], Loss: 78.0606, Accuracy: 35.14 %\n",
      "Testing Accuracy: 37.62 %\n",
      "Test Loss:  75.72095024585724\n",
      "Epoch [24/500], Loss: 77.1685, Accuracy: 35.62 %\n",
      "Testing Accuracy: 37.55 %\n",
      "Test Loss:  76.0350798368454\n",
      "Epoch [25/500], Loss: 76.5443, Accuracy: 36.06 %\n",
      "Testing Accuracy: 37.72 %\n",
      "Test Loss:  75.17988646030426\n",
      "Epoch [26/500], Loss: 76.8053, Accuracy: 35.82 %\n",
      "Testing Accuracy: 36.76 %\n",
      "Test Loss:  76.15268683433533\n",
      "Epoch [27/500], Loss: 76.3089, Accuracy: 36.16 %\n",
      "Testing Accuracy: 37.18 %\n",
      "Test Loss:  76.17530143260956\n",
      "Epoch [28/500], Loss: 76.3690, Accuracy: 36.46 %\n",
      "Testing Accuracy: 38.19 %\n",
      "Test Loss:  75.17474842071533\n",
      "Epoch [29/500], Loss: 76.5023, Accuracy: 36.18 %\n",
      "Testing Accuracy: 37.43 %\n",
      "Test Loss:  76.02346014976501\n",
      "Epoch [30/500], Loss: 76.4097, Accuracy: 36.58 %\n",
      "Testing Accuracy: 37.23 %\n",
      "Test Loss:  75.4496945142746\n",
      "Testing Accuracy: 37.23 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        679936   \n",
      "Net/Dropout[dropout]/onnx::Relu   664      \n",
      "Net/Linear[fc2]/onnx::Gemm        3652     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "684,252 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148]\n",
      "pruning hidden size:  332\n",
      "with hidden layer:  332\n",
      "removing:  (230, 164, 111)\n",
      "--- 32.568941831588745 seconds ---\n",
      "Epoch [1/500], Loss: 79.3827, Accuracy: 32.51 %\n",
      "Testing Accuracy: 35.26 %\n",
      "Test Loss:  77.62217259407043\n",
      "Epoch [2/500], Loss: 79.1763, Accuracy: 33.43 %\n",
      "Testing Accuracy: 35.21 %\n",
      "Test Loss:  77.81026589870453\n",
      "Epoch [3/500], Loss: 78.9124, Accuracy: 33.49 %\n",
      "Testing Accuracy: 35.62 %\n",
      "Test Loss:  77.36915266513824\n",
      "Epoch [4/500], Loss: 79.4293, Accuracy: 33.40 %\n",
      "Testing Accuracy: 36.03 %\n",
      "Test Loss:  77.31163036823273\n",
      "Epoch [5/500], Loss: 78.7224, Accuracy: 34.38 %\n",
      "Testing Accuracy: 33.52 %\n",
      "Test Loss:  82.0262736082077\n",
      "Epoch [6/500], Loss: 78.9506, Accuracy: 34.26 %\n",
      "Testing Accuracy: 37.11 %\n",
      "Test Loss:  76.80068063735962\n",
      "Epoch [7/500], Loss: 79.0935, Accuracy: 34.10 %\n",
      "Testing Accuracy: 34.84 %\n",
      "Test Loss:  80.20217907428741\n",
      "Epoch [8/500], Loss: 78.8661, Accuracy: 34.41 %\n",
      "Testing Accuracy: 36.96 %\n",
      "Test Loss:  76.86450219154358\n",
      "Epoch [9/500], Loss: 78.2134, Accuracy: 34.76 %\n",
      "Testing Accuracy: 37.07 %\n",
      "Test Loss:  76.65040361881256\n",
      "Epoch [10/500], Loss: 78.4891, Accuracy: 34.71 %\n",
      "Testing Accuracy: 36.90 %\n",
      "Test Loss:  77.38532423973083\n",
      "Epoch [11/500], Loss: 78.6190, Accuracy: 34.73 %\n",
      "Testing Accuracy: 35.73 %\n",
      "Test Loss:  77.83402240276337\n",
      "Epoch [12/500], Loss: 78.5012, Accuracy: 34.72 %\n",
      "Testing Accuracy: 37.25 %\n",
      "Test Loss:  76.84858524799347\n",
      "Epoch [13/500], Loss: 78.3172, Accuracy: 34.85 %\n",
      "Testing Accuracy: 37.54 %\n",
      "Test Loss:  77.22009837627411\n",
      "Epoch [14/500], Loss: 77.7829, Accuracy: 35.32 %\n",
      "Testing Accuracy: 38.38 %\n",
      "Test Loss:  76.1546026468277\n",
      "Epoch [15/500], Loss: 77.6603, Accuracy: 35.69 %\n",
      "Testing Accuracy: 37.22 %\n",
      "Test Loss:  76.54063832759857\n",
      "Epoch [16/500], Loss: 77.6030, Accuracy: 35.54 %\n",
      "Testing Accuracy: 38.00 %\n",
      "Test Loss:  75.8159304857254\n",
      "Epoch [17/500], Loss: 77.8172, Accuracy: 35.36 %\n",
      "Testing Accuracy: 38.31 %\n",
      "Test Loss:  76.24731159210205\n",
      "Epoch [18/500], Loss: 77.6395, Accuracy: 35.56 %\n",
      "Testing Accuracy: 38.40 %\n",
      "Test Loss:  75.91537177562714\n",
      "Epoch [19/500], Loss: 77.8245, Accuracy: 35.39 %\n",
      "Testing Accuracy: 35.02 %\n",
      "Test Loss:  78.47629737854004\n",
      "Epoch [20/500], Loss: 77.7181, Accuracy: 35.50 %\n",
      "Testing Accuracy: 38.15 %\n",
      "Test Loss:  76.49281060695648\n",
      "Epoch [21/500], Loss: 76.8456, Accuracy: 36.36 %\n",
      "Testing Accuracy: 38.48 %\n",
      "Test Loss:  75.70200383663177\n",
      "Epoch [22/500], Loss: 77.0057, Accuracy: 36.31 %\n",
      "Testing Accuracy: 38.15 %\n",
      "Test Loss:  77.29120337963104\n",
      "Epoch [23/500], Loss: 77.7853, Accuracy: 35.54 %\n",
      "Testing Accuracy: 37.91 %\n",
      "Test Loss:  76.0532648563385\n",
      "Epoch [24/500], Loss: 77.4247, Accuracy: 35.81 %\n",
      "Testing Accuracy: 38.48 %\n",
      "Test Loss:  76.10589730739594\n",
      "Epoch [25/500], Loss: 77.2737, Accuracy: 36.03 %\n",
      "Testing Accuracy: 39.02 %\n",
      "Test Loss:  75.10226011276245\n",
      "Epoch [26/500], Loss: 76.9130, Accuracy: 36.75 %\n",
      "Testing Accuracy: 38.28 %\n",
      "Test Loss:  75.6194236278534\n",
      "Epoch [27/500], Loss: 76.8022, Accuracy: 36.62 %\n",
      "Testing Accuracy: 38.74 %\n",
      "Test Loss:  75.7129418849945\n",
      "Epoch [28/500], Loss: 77.0976, Accuracy: 36.60 %\n",
      "Testing Accuracy: 37.76 %\n",
      "Test Loss:  75.72740113735199\n",
      "Epoch [29/500], Loss: 77.3498, Accuracy: 36.18 %\n",
      "Testing Accuracy: 38.40 %\n",
      "Test Loss:  75.38476502895355\n",
      "Epoch [30/500], Loss: 76.9003, Accuracy: 36.67 %\n",
      "Testing Accuracy: 38.87 %\n",
      "Test Loss:  74.82324349880219\n",
      "Epoch [31/500], Loss: 76.7617, Accuracy: 36.84 %\n",
      "Testing Accuracy: 38.82 %\n",
      "Test Loss:  76.09619843959808\n",
      "Epoch [32/500], Loss: 76.4816, Accuracy: 36.91 %\n",
      "Testing Accuracy: 38.88 %\n",
      "Test Loss:  75.7399263381958\n",
      "Epoch [33/500], Loss: 77.0500, Accuracy: 36.38 %\n",
      "Testing Accuracy: 38.81 %\n",
      "Test Loss:  75.43501675128937\n",
      "Epoch [34/500], Loss: 76.2846, Accuracy: 37.31 %\n",
      "Testing Accuracy: 39.23 %\n",
      "Test Loss:  75.47659885883331\n",
      "Epoch [35/500], Loss: 77.6212, Accuracy: 36.21 %\n",
      "Testing Accuracy: 39.24 %\n",
      "Test Loss:  74.72296714782715\n",
      "Epoch [36/500], Loss: 76.3348, Accuracy: 37.18 %\n",
      "Testing Accuracy: 38.23 %\n",
      "Test Loss:  76.03385174274445\n",
      "Epoch [37/500], Loss: 76.2176, Accuracy: 37.16 %\n",
      "Testing Accuracy: 38.94 %\n",
      "Test Loss:  75.28902268409729\n",
      "Epoch [38/500], Loss: 76.5136, Accuracy: 37.15 %\n",
      "Testing Accuracy: 39.10 %\n",
      "Test Loss:  74.98065626621246\n",
      "Epoch [39/500], Loss: 76.5412, Accuracy: 37.10 %\n",
      "Testing Accuracy: 38.78 %\n",
      "Test Loss:  76.02763545513153\n",
      "Epoch [40/500], Loss: 76.4880, Accuracy: 37.11 %\n",
      "Testing Accuracy: 38.04 %\n",
      "Test Loss:  76.65790915489197\n",
      "Epoch [41/500], Loss: 76.7406, Accuracy: 36.95 %\n",
      "Testing Accuracy: 38.02 %\n",
      "Test Loss:  77.01092076301575\n",
      "Epoch [42/500], Loss: 76.2632, Accuracy: 37.20 %\n",
      "Testing Accuracy: 37.80 %\n",
      "Test Loss:  75.37543845176697\n",
      "Epoch [43/500], Loss: 76.0313, Accuracy: 37.57 %\n",
      "Testing Accuracy: 37.33 %\n",
      "Test Loss:  76.30665922164917\n",
      "Epoch [44/500], Loss: 75.9843, Accuracy: 37.48 %\n",
      "Testing Accuracy: 38.72 %\n",
      "Test Loss:  76.47501540184021\n",
      "Epoch [45/500], Loss: 76.2246, Accuracy: 37.44 %\n",
      "Testing Accuracy: 39.17 %\n",
      "Test Loss:  75.3144965171814\n",
      "Epoch [46/500], Loss: 76.1170, Accuracy: 37.47 %\n",
      "Testing Accuracy: 39.53 %\n",
      "Test Loss:  75.4505889415741\n",
      "Epoch [47/500], Loss: 76.3230, Accuracy: 37.67 %\n",
      "Testing Accuracy: 39.14 %\n",
      "Test Loss:  74.77373027801514\n",
      "Epoch [48/500], Loss: 76.5114, Accuracy: 37.01 %\n",
      "Testing Accuracy: 38.88 %\n",
      "Test Loss:  75.3482369184494\n",
      "Epoch [49/500], Loss: 76.2744, Accuracy: 37.41 %\n",
      "Testing Accuracy: 39.53 %\n",
      "Test Loss:  74.94970035552979\n",
      "Epoch [50/500], Loss: 76.0585, Accuracy: 37.33 %\n",
      "Testing Accuracy: 38.05 %\n",
      "Test Loss:  77.09367871284485\n",
      "Epoch [51/500], Loss: 76.4166, Accuracy: 37.14 %\n",
      "Testing Accuracy: 39.04 %\n",
      "Test Loss:  76.09002220630646\n",
      "Epoch [52/500], Loss: 77.0829, Accuracy: 36.85 %\n",
      "Testing Accuracy: 39.04 %\n",
      "Test Loss:  75.36741721630096\n",
      "Epoch [53/500], Loss: 76.2072, Accuracy: 37.14 %\n",
      "Testing Accuracy: 37.67 %\n",
      "Test Loss:  76.28378856182098\n",
      "Epoch [54/500], Loss: 76.4572, Accuracy: 37.04 %\n",
      "Testing Accuracy: 39.23 %\n",
      "Test Loss:  75.14418733119965\n",
      "Epoch [55/500], Loss: 76.0036, Accuracy: 37.35 %\n",
      "Testing Accuracy: 39.41 %\n",
      "Test Loss:  74.60213577747345\n",
      "Epoch [56/500], Loss: 76.3968, Accuracy: 37.22 %\n",
      "Testing Accuracy: 39.43 %\n",
      "Test Loss:  75.10085785388947\n",
      "Epoch [57/500], Loss: 75.7237, Accuracy: 37.90 %\n",
      "Testing Accuracy: 38.94 %\n",
      "Test Loss:  74.58741796016693\n",
      "Epoch [58/500], Loss: 76.7332, Accuracy: 36.65 %\n",
      "Testing Accuracy: 39.71 %\n",
      "Test Loss:  74.6405314207077\n",
      "Epoch [59/500], Loss: 76.1332, Accuracy: 37.75 %\n",
      "Testing Accuracy: 39.11 %\n",
      "Test Loss:  75.6770510673523\n",
      "Epoch [60/500], Loss: 75.8539, Accuracy: 37.82 %\n",
      "Testing Accuracy: 39.91 %\n",
      "Test Loss:  74.6303619146347\n",
      "Epoch [61/500], Loss: 76.2360, Accuracy: 37.39 %\n",
      "Testing Accuracy: 40.09 %\n",
      "Test Loss:  74.65909230709076\n",
      "Epoch [62/500], Loss: 76.3220, Accuracy: 37.33 %\n",
      "Testing Accuracy: 37.45 %\n",
      "Test Loss:  77.66064870357513\n",
      "Epoch [63/500], Loss: 76.4470, Accuracy: 37.28 %\n",
      "Testing Accuracy: 39.67 %\n",
      "Test Loss:  74.45370483398438\n",
      "Epoch [64/500], Loss: 75.6803, Accuracy: 38.05 %\n",
      "Testing Accuracy: 38.33 %\n",
      "Test Loss:  74.89905989170074\n",
      "Epoch [65/500], Loss: 75.8076, Accuracy: 37.90 %\n",
      "Testing Accuracy: 39.63 %\n",
      "Test Loss:  74.49294412136078\n",
      "Epoch [66/500], Loss: 75.6704, Accuracy: 37.55 %\n",
      "Testing Accuracy: 39.25 %\n",
      "Test Loss:  75.1869785785675\n",
      "Epoch [67/500], Loss: 76.3660, Accuracy: 37.35 %\n",
      "Testing Accuracy: 36.65 %\n",
      "Test Loss:  76.89265787601471\n",
      "Epoch [68/500], Loss: 77.0946, Accuracy: 36.64 %\n",
      "Testing Accuracy: 37.49 %\n",
      "Test Loss:  76.86042594909668\n",
      "Epoch [69/500], Loss: 76.7928, Accuracy: 37.25 %\n",
      "Testing Accuracy: 39.82 %\n",
      "Test Loss:  75.11148369312286\n",
      "Epoch [70/500], Loss: 75.6529, Accuracy: 37.74 %\n",
      "Testing Accuracy: 39.39 %\n",
      "Test Loss:  75.61932742595673\n",
      "Epoch [71/500], Loss: 75.8987, Accuracy: 37.80 %\n",
      "Testing Accuracy: 38.47 %\n",
      "Test Loss:  74.94855844974518\n",
      "Epoch [72/500], Loss: 76.4843, Accuracy: 37.33 %\n",
      "Testing Accuracy: 39.12 %\n",
      "Test Loss:  75.41935694217682\n",
      "Epoch [73/500], Loss: 75.8068, Accuracy: 37.87 %\n",
      "Testing Accuracy: 40.03 %\n",
      "Test Loss:  74.1338210105896\n",
      "Epoch [74/500], Loss: 75.2092, Accuracy: 38.38 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  74.64993739128113\n",
      "Epoch [75/500], Loss: 75.5925, Accuracy: 38.14 %\n",
      "Testing Accuracy: 37.62 %\n",
      "Test Loss:  76.66342163085938\n",
      "Epoch [76/500], Loss: 76.0409, Accuracy: 37.84 %\n",
      "Testing Accuracy: 37.72 %\n",
      "Test Loss:  76.910768866539\n",
      "Epoch [77/500], Loss: 76.4509, Accuracy: 37.35 %\n",
      "Testing Accuracy: 40.01 %\n",
      "Test Loss:  74.01757156848907\n",
      "Epoch [78/500], Loss: 75.4430, Accuracy: 38.43 %\n",
      "Testing Accuracy: 38.30 %\n",
      "Test Loss:  78.14063608646393\n",
      "Epoch [79/500], Loss: 75.7653, Accuracy: 38.29 %\n",
      "Testing Accuracy: 39.36 %\n",
      "Test Loss:  76.53509473800659\n",
      "Epoch [80/500], Loss: 75.8548, Accuracy: 38.29 %\n",
      "Testing Accuracy: 40.11 %\n",
      "Test Loss:  74.37678372859955\n",
      "Epoch [81/500], Loss: 75.8276, Accuracy: 37.81 %\n",
      "Testing Accuracy: 39.80 %\n",
      "Test Loss:  74.50691342353821\n",
      "Epoch [82/500], Loss: 75.7899, Accuracy: 38.08 %\n",
      "Testing Accuracy: 38.63 %\n",
      "Test Loss:  75.38431119918823\n",
      "Epoch [83/500], Loss: 75.6533, Accuracy: 37.98 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  74.64874184131622\n",
      "Epoch [84/500], Loss: 75.9435, Accuracy: 38.01 %\n",
      "Testing Accuracy: 39.18 %\n",
      "Test Loss:  74.87514209747314\n",
      "Epoch [85/500], Loss: 75.5870, Accuracy: 38.01 %\n",
      "Testing Accuracy: 41.03 %\n",
      "Test Loss:  73.54347610473633\n",
      "Epoch [86/500], Loss: 75.2301, Accuracy: 38.23 %\n",
      "Testing Accuracy: 39.47 %\n",
      "Test Loss:  75.47496449947357\n",
      "Epoch [87/500], Loss: 75.5417, Accuracy: 38.38 %\n",
      "Testing Accuracy: 39.30 %\n",
      "Test Loss:  74.78050696849823\n",
      "Epoch [88/500], Loss: 75.3630, Accuracy: 38.27 %\n",
      "Testing Accuracy: 39.68 %\n",
      "Test Loss:  74.6478613615036\n",
      "Epoch [89/500], Loss: 75.0799, Accuracy: 38.49 %\n",
      "Testing Accuracy: 40.10 %\n",
      "Test Loss:  74.50352561473846\n",
      "Epoch [90/500], Loss: 74.9027, Accuracy: 38.60 %\n",
      "Testing Accuracy: 40.12 %\n",
      "Test Loss:  74.34768605232239\n",
      "Epoch [91/500], Loss: 75.0728, Accuracy: 38.71 %\n",
      "Testing Accuracy: 39.99 %\n",
      "Test Loss:  74.95166838169098\n",
      "Epoch [92/500], Loss: 75.2145, Accuracy: 38.31 %\n",
      "Testing Accuracy: 40.03 %\n",
      "Test Loss:  74.33744788169861\n",
      "Epoch [93/500], Loss: 75.3520, Accuracy: 38.22 %\n",
      "Testing Accuracy: 39.76 %\n",
      "Test Loss:  74.80140995979309\n",
      "Epoch [94/500], Loss: 74.9079, Accuracy: 38.50 %\n",
      "Testing Accuracy: 39.64 %\n",
      "Test Loss:  74.95137369632721\n",
      "Epoch [95/500], Loss: 75.7547, Accuracy: 37.83 %\n",
      "Testing Accuracy: 40.87 %\n",
      "Test Loss:  73.5410213470459\n",
      "Epoch [96/500], Loss: 75.4905, Accuracy: 37.90 %\n",
      "Testing Accuracy: 39.03 %\n",
      "Test Loss:  76.03675282001495\n",
      "Epoch [97/500], Loss: 75.8271, Accuracy: 38.16 %\n",
      "Testing Accuracy: 38.26 %\n",
      "Test Loss:  75.55891621112823\n",
      "Epoch [98/500], Loss: 75.4965, Accuracy: 38.12 %\n",
      "Testing Accuracy: 39.55 %\n",
      "Test Loss:  74.30750584602356\n",
      "Epoch [99/500], Loss: 75.5844, Accuracy: 37.95 %\n",
      "Testing Accuracy: 40.41 %\n",
      "Test Loss:  74.35637676715851\n",
      "Epoch [100/500], Loss: 75.7198, Accuracy: 37.92 %\n",
      "Testing Accuracy: 40.27 %\n",
      "Test Loss:  74.55073893070221\n",
      "Epoch [101/500], Loss: 74.6737, Accuracy: 38.79 %\n",
      "Testing Accuracy: 40.93 %\n",
      "Test Loss:  73.65543329715729\n",
      "Epoch [102/500], Loss: 74.7782, Accuracy: 38.62 %\n",
      "Testing Accuracy: 40.31 %\n",
      "Test Loss:  74.30556106567383\n",
      "Epoch [103/500], Loss: 74.7905, Accuracy: 38.86 %\n",
      "Testing Accuracy: 40.62 %\n",
      "Test Loss:  73.52058291435242\n",
      "Epoch [104/500], Loss: 75.3135, Accuracy: 38.31 %\n",
      "Testing Accuracy: 40.77 %\n",
      "Test Loss:  74.13959240913391\n",
      "Epoch [105/500], Loss: 76.0628, Accuracy: 37.85 %\n",
      "Testing Accuracy: 38.99 %\n",
      "Test Loss:  75.78572022914886\n",
      "Epoch [106/500], Loss: 75.4276, Accuracy: 37.67 %\n",
      "Testing Accuracy: 40.19 %\n",
      "Test Loss:  73.86471962928772\n",
      "Epoch [107/500], Loss: 75.7181, Accuracy: 38.41 %\n",
      "Testing Accuracy: 39.58 %\n",
      "Test Loss:  74.65618479251862\n",
      "Epoch [108/500], Loss: 75.0950, Accuracy: 38.62 %\n",
      "Testing Accuracy: 39.36 %\n",
      "Test Loss:  75.26347661018372\n",
      "Epoch [109/500], Loss: 74.8854, Accuracy: 38.81 %\n",
      "Testing Accuracy: 39.70 %\n",
      "Test Loss:  74.34133243560791\n",
      "Epoch [110/500], Loss: 74.4737, Accuracy: 39.07 %\n",
      "Testing Accuracy: 39.71 %\n",
      "Test Loss:  75.0707631111145\n",
      "Epoch [111/500], Loss: 75.3251, Accuracy: 38.17 %\n",
      "Testing Accuracy: 40.97 %\n",
      "Test Loss:  73.21122908592224\n",
      "Epoch [112/500], Loss: 74.5205, Accuracy: 39.02 %\n",
      "Testing Accuracy: 40.73 %\n",
      "Test Loss:  73.82490634918213\n",
      "Epoch [113/500], Loss: 74.5287, Accuracy: 39.02 %\n",
      "Testing Accuracy: 40.62 %\n",
      "Test Loss:  73.71309685707092\n",
      "Epoch [114/500], Loss: 75.5234, Accuracy: 38.18 %\n",
      "Testing Accuracy: 38.02 %\n",
      "Test Loss:  75.48294031620026\n",
      "Epoch [115/500], Loss: 75.5582, Accuracy: 38.01 %\n",
      "Testing Accuracy: 39.77 %\n",
      "Test Loss:  74.69278478622437\n",
      "Epoch [116/500], Loss: 75.4143, Accuracy: 37.88 %\n",
      "Testing Accuracy: 40.58 %\n",
      "Test Loss:  74.28481757640839\n",
      "Epoch [117/500], Loss: 75.2290, Accuracy: 38.51 %\n",
      "Testing Accuracy: 40.09 %\n",
      "Test Loss:  73.97705519199371\n",
      "Epoch [118/500], Loss: 74.8868, Accuracy: 38.38 %\n",
      "Testing Accuracy: 40.01 %\n",
      "Test Loss:  74.65111541748047\n",
      "Epoch [119/500], Loss: 75.0341, Accuracy: 38.93 %\n",
      "Testing Accuracy: 39.93 %\n",
      "Test Loss:  74.26145017147064\n",
      "Epoch [120/500], Loss: 75.1316, Accuracy: 38.67 %\n",
      "Testing Accuracy: 40.52 %\n",
      "Test Loss:  74.34423100948334\n",
      "Epoch [121/500], Loss: 74.3622, Accuracy: 39.22 %\n",
      "Testing Accuracy: 40.70 %\n",
      "Test Loss:  73.39494717121124\n",
      "Epoch [122/500], Loss: 74.5289, Accuracy: 38.99 %\n",
      "Testing Accuracy: 39.73 %\n",
      "Test Loss:  74.5807785987854\n",
      "Epoch [123/500], Loss: 74.5549, Accuracy: 39.37 %\n",
      "Testing Accuracy: 40.60 %\n",
      "Test Loss:  74.26000463962555\n",
      "Epoch [124/500], Loss: 74.6512, Accuracy: 39.07 %\n",
      "Testing Accuracy: 39.79 %\n",
      "Test Loss:  74.70177948474884\n",
      "Epoch [125/500], Loss: 74.4251, Accuracy: 39.46 %\n",
      "Testing Accuracy: 40.28 %\n",
      "Test Loss:  73.96287846565247\n",
      "Epoch [126/500], Loss: 75.0977, Accuracy: 38.88 %\n",
      "Testing Accuracy: 40.05 %\n",
      "Test Loss:  73.98678624629974\n",
      "Epoch [127/500], Loss: 75.8441, Accuracy: 37.67 %\n",
      "Testing Accuracy: 38.91 %\n",
      "Test Loss:  74.8073959350586\n",
      "Epoch [128/500], Loss: 74.9230, Accuracy: 38.84 %\n",
      "Testing Accuracy: 38.09 %\n",
      "Test Loss:  75.71831130981445\n",
      "Epoch [129/500], Loss: 75.3726, Accuracy: 38.07 %\n",
      "Testing Accuracy: 39.81 %\n",
      "Test Loss:  74.20924079418182\n",
      "Epoch [130/500], Loss: 75.7272, Accuracy: 38.00 %\n",
      "Testing Accuracy: 37.47 %\n",
      "Test Loss:  76.88090944290161\n",
      "Epoch [131/500], Loss: 75.1376, Accuracy: 38.07 %\n",
      "Testing Accuracy: 40.64 %\n",
      "Test Loss:  73.8538430929184\n",
      "Testing Accuracy: 40.64 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        673792   \n",
      "Net/Dropout[dropout]/onnx::Relu   658      \n",
      "Net/Linear[fc2]/onnx::Gemm        3619     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "678,069 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421]\n",
      "pruning hidden size:  329\n",
      "with hidden layer:  329\n",
      "removing:  (191, 286, 273)\n",
      "--- 31.9071843624115 seconds ---\n",
      "Epoch [1/500], Loss: 82.0220, Accuracy: 31.11 %\n",
      "Testing Accuracy: 32.62 %\n",
      "Test Loss:  81.06497025489807\n",
      "Epoch [2/500], Loss: 81.5412, Accuracy: 31.42 %\n",
      "Testing Accuracy: 32.09 %\n",
      "Test Loss:  81.38757884502411\n",
      "Epoch [3/500], Loss: 81.4801, Accuracy: 31.70 %\n",
      "Testing Accuracy: 33.87 %\n",
      "Test Loss:  79.61749219894409\n",
      "Epoch [4/500], Loss: 81.4355, Accuracy: 31.57 %\n",
      "Testing Accuracy: 32.72 %\n",
      "Test Loss:  81.04731595516205\n",
      "Epoch [5/500], Loss: 80.8383, Accuracy: 31.48 %\n",
      "Testing Accuracy: 33.78 %\n",
      "Test Loss:  79.52827334403992\n",
      "Epoch [6/500], Loss: 82.0039, Accuracy: 30.86 %\n",
      "Testing Accuracy: 32.85 %\n",
      "Test Loss:  81.55903315544128\n",
      "Epoch [7/500], Loss: 81.0273, Accuracy: 31.91 %\n",
      "Testing Accuracy: 34.19 %\n",
      "Test Loss:  79.20373749732971\n",
      "Epoch [8/500], Loss: 80.2924, Accuracy: 32.27 %\n",
      "Testing Accuracy: 33.30 %\n",
      "Test Loss:  80.36369276046753\n",
      "Epoch [9/500], Loss: 80.5706, Accuracy: 31.92 %\n",
      "Testing Accuracy: 33.15 %\n",
      "Test Loss:  80.37041401863098\n",
      "Epoch [10/500], Loss: 80.9960, Accuracy: 31.70 %\n",
      "Testing Accuracy: 33.40 %\n",
      "Test Loss:  80.2526468038559\n",
      "Epoch [11/500], Loss: 80.6533, Accuracy: 31.96 %\n",
      "Testing Accuracy: 34.24 %\n",
      "Test Loss:  78.86000669002533\n",
      "Epoch [12/500], Loss: 80.2134, Accuracy: 32.39 %\n",
      "Testing Accuracy: 34.22 %\n",
      "Test Loss:  79.19651663303375\n",
      "Epoch [13/500], Loss: 80.0077, Accuracy: 32.38 %\n",
      "Testing Accuracy: 34.53 %\n",
      "Test Loss:  78.90455639362335\n",
      "Epoch [14/500], Loss: 80.1762, Accuracy: 32.08 %\n",
      "Testing Accuracy: 34.73 %\n",
      "Test Loss:  79.26659989356995\n",
      "Epoch [15/500], Loss: 80.1497, Accuracy: 32.44 %\n",
      "Testing Accuracy: 33.75 %\n",
      "Test Loss:  79.67261874675751\n",
      "Epoch [16/500], Loss: 80.1335, Accuracy: 32.57 %\n",
      "Testing Accuracy: 34.45 %\n",
      "Test Loss:  78.86101019382477\n",
      "Epoch [17/500], Loss: 81.2723, Accuracy: 31.17 %\n",
      "Testing Accuracy: 32.80 %\n",
      "Test Loss:  80.32198226451874\n",
      "Epoch [18/500], Loss: 80.8048, Accuracy: 31.95 %\n",
      "Testing Accuracy: 34.09 %\n",
      "Test Loss:  80.15865468978882\n",
      "Epoch [19/500], Loss: 80.2975, Accuracy: 32.26 %\n",
      "Testing Accuracy: 35.11 %\n",
      "Test Loss:  78.4551751613617\n",
      "Epoch [20/500], Loss: 79.8324, Accuracy: 32.63 %\n",
      "Testing Accuracy: 35.51 %\n",
      "Test Loss:  78.0377790927887\n",
      "Epoch [21/500], Loss: 79.9087, Accuracy: 32.62 %\n",
      "Testing Accuracy: 34.98 %\n",
      "Test Loss:  78.72242903709412\n",
      "Epoch [22/500], Loss: 79.5488, Accuracy: 33.18 %\n",
      "Testing Accuracy: 34.54 %\n",
      "Test Loss:  80.19178318977356\n",
      "Epoch [23/500], Loss: 79.9903, Accuracy: 32.71 %\n",
      "Testing Accuracy: 33.03 %\n",
      "Test Loss:  81.96830177307129\n",
      "Epoch [24/500], Loss: 80.1049, Accuracy: 32.76 %\n",
      "Testing Accuracy: 33.13 %\n",
      "Test Loss:  79.57101738452911\n",
      "Epoch [25/500], Loss: 80.0105, Accuracy: 33.10 %\n",
      "Testing Accuracy: 35.22 %\n",
      "Test Loss:  79.41727101802826\n",
      "Epoch [26/500], Loss: 79.9866, Accuracy: 33.13 %\n",
      "Testing Accuracy: 35.51 %\n",
      "Test Loss:  79.41007089614868\n",
      "Epoch [27/500], Loss: 79.3575, Accuracy: 33.87 %\n",
      "Testing Accuracy: 36.26 %\n",
      "Test Loss:  78.4802051782608\n",
      "Epoch [28/500], Loss: 79.3020, Accuracy: 34.02 %\n",
      "Testing Accuracy: 37.10 %\n",
      "Test Loss:  77.6288251876831\n",
      "Epoch [29/500], Loss: 79.4500, Accuracy: 34.21 %\n",
      "Testing Accuracy: 36.82 %\n",
      "Test Loss:  77.97873723506927\n",
      "Epoch [30/500], Loss: 79.0454, Accuracy: 34.64 %\n",
      "Testing Accuracy: 37.25 %\n",
      "Test Loss:  77.98364531993866\n",
      "Epoch [31/500], Loss: 79.4975, Accuracy: 34.50 %\n",
      "Testing Accuracy: 36.99 %\n",
      "Test Loss:  77.92397224903107\n",
      "Epoch [32/500], Loss: 79.0563, Accuracy: 34.79 %\n",
      "Testing Accuracy: 37.06 %\n",
      "Test Loss:  78.35754239559174\n",
      "Epoch [33/500], Loss: 78.6531, Accuracy: 35.10 %\n",
      "Testing Accuracy: 37.76 %\n",
      "Test Loss:  77.27580857276917\n",
      "Epoch [34/500], Loss: 79.3677, Accuracy: 34.88 %\n",
      "Testing Accuracy: 36.07 %\n",
      "Test Loss:  78.05308735370636\n",
      "Epoch [35/500], Loss: 79.0174, Accuracy: 35.06 %\n",
      "Testing Accuracy: 38.13 %\n",
      "Test Loss:  77.12987244129181\n",
      "Epoch [36/500], Loss: 78.9878, Accuracy: 35.17 %\n",
      "Testing Accuracy: 37.28 %\n",
      "Test Loss:  77.78806793689728\n",
      "Epoch [37/500], Loss: 78.4562, Accuracy: 35.84 %\n",
      "Testing Accuracy: 37.77 %\n",
      "Test Loss:  77.1244740486145\n",
      "Epoch [38/500], Loss: 78.8387, Accuracy: 35.07 %\n",
      "Testing Accuracy: 37.20 %\n",
      "Test Loss:  77.86409282684326\n",
      "Epoch [39/500], Loss: 78.6578, Accuracy: 35.70 %\n",
      "Testing Accuracy: 38.31 %\n",
      "Test Loss:  76.8668407201767\n",
      "Epoch [40/500], Loss: 78.2041, Accuracy: 35.93 %\n",
      "Testing Accuracy: 38.29 %\n",
      "Test Loss:  76.3272635936737\n",
      "Epoch [41/500], Loss: 78.5018, Accuracy: 35.76 %\n",
      "Testing Accuracy: 38.03 %\n",
      "Test Loss:  77.56968212127686\n",
      "Epoch [42/500], Loss: 78.6404, Accuracy: 35.36 %\n",
      "Testing Accuracy: 37.85 %\n",
      "Test Loss:  77.19016778469086\n",
      "Epoch [43/500], Loss: 77.9955, Accuracy: 36.12 %\n",
      "Testing Accuracy: 37.38 %\n",
      "Test Loss:  76.92923426628113\n",
      "Epoch [44/500], Loss: 78.7409, Accuracy: 35.61 %\n",
      "Testing Accuracy: 38.50 %\n",
      "Test Loss:  76.81869864463806\n",
      "Epoch [45/500], Loss: 78.6217, Accuracy: 35.63 %\n",
      "Testing Accuracy: 38.11 %\n",
      "Test Loss:  77.18243300914764\n",
      "Epoch [46/500], Loss: 78.6056, Accuracy: 35.66 %\n",
      "Testing Accuracy: 35.72 %\n",
      "Test Loss:  79.92082750797272\n",
      "Epoch [47/500], Loss: 78.9354, Accuracy: 35.70 %\n",
      "Testing Accuracy: 37.43 %\n",
      "Test Loss:  77.98840177059174\n",
      "Epoch [48/500], Loss: 78.4250, Accuracy: 35.72 %\n",
      "Testing Accuracy: 38.61 %\n",
      "Test Loss:  76.57548952102661\n",
      "Epoch [49/500], Loss: 78.2723, Accuracy: 36.16 %\n",
      "Testing Accuracy: 38.56 %\n",
      "Test Loss:  76.99957537651062\n",
      "Epoch [50/500], Loss: 77.6857, Accuracy: 36.37 %\n",
      "Testing Accuracy: 38.01 %\n",
      "Test Loss:  76.81283605098724\n",
      "Epoch [51/500], Loss: 78.7588, Accuracy: 35.45 %\n",
      "Testing Accuracy: 37.78 %\n",
      "Test Loss:  76.73306679725647\n",
      "Epoch [52/500], Loss: 78.2516, Accuracy: 35.84 %\n",
      "Testing Accuracy: 38.33 %\n",
      "Test Loss:  76.74936282634735\n",
      "Epoch [53/500], Loss: 77.8994, Accuracy: 36.20 %\n",
      "Testing Accuracy: 37.41 %\n",
      "Test Loss:  77.11317372322083\n",
      "Epoch [54/500], Loss: 77.7707, Accuracy: 36.43 %\n",
      "Testing Accuracy: 38.14 %\n",
      "Test Loss:  77.36179661750793\n",
      "Epoch [55/500], Loss: 77.7479, Accuracy: 36.57 %\n",
      "Testing Accuracy: 37.51 %\n",
      "Test Loss:  77.2094339132309\n",
      "Epoch [56/500], Loss: 78.2195, Accuracy: 35.98 %\n",
      "Testing Accuracy: 38.42 %\n",
      "Test Loss:  76.76194417476654\n",
      "Epoch [57/500], Loss: 78.1034, Accuracy: 36.41 %\n",
      "Testing Accuracy: 38.64 %\n",
      "Test Loss:  77.46783578395844\n",
      "Epoch [58/500], Loss: 78.2515, Accuracy: 36.48 %\n",
      "Testing Accuracy: 37.97 %\n",
      "Test Loss:  76.6460452079773\n",
      "Epoch [59/500], Loss: 78.0399, Accuracy: 36.20 %\n",
      "Testing Accuracy: 38.46 %\n",
      "Test Loss:  76.91729235649109\n",
      "Epoch [60/500], Loss: 77.8268, Accuracy: 36.47 %\n",
      "Testing Accuracy: 37.72 %\n",
      "Test Loss:  78.0165581703186\n",
      "Testing Accuracy: 37.72 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        667648   \n",
      "Net/Dropout[dropout]/onnx::Relu   652      \n",
      "Net/Linear[fc2]/onnx::Gemm        3586     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "671,886 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967]\n",
      "pruning hidden size:  326\n",
      "with hidden layer:  326\n",
      "removing:  (179, 284, 272)\n",
      "--- 31.457446336746216 seconds ---\n",
      "Epoch [1/500], Loss: 77.8872, Accuracy: 36.21 %\n",
      "Testing Accuracy: 38.16 %\n",
      "Test Loss:  76.33675646781921\n",
      "Epoch [2/500], Loss: 78.6599, Accuracy: 35.93 %\n",
      "Testing Accuracy: 38.40 %\n",
      "Test Loss:  76.75630939006805\n",
      "Epoch [3/500], Loss: 77.7634, Accuracy: 36.31 %\n",
      "Testing Accuracy: 39.06 %\n",
      "Test Loss:  75.95975482463837\n",
      "Epoch [4/500], Loss: 78.0279, Accuracy: 36.08 %\n",
      "Testing Accuracy: 39.19 %\n",
      "Test Loss:  76.08959913253784\n",
      "Epoch [5/500], Loss: 79.7813, Accuracy: 34.77 %\n",
      "Testing Accuracy: 35.39 %\n",
      "Test Loss:  78.40849936008453\n",
      "Epoch [6/500], Loss: 78.0845, Accuracy: 35.98 %\n",
      "Testing Accuracy: 37.58 %\n",
      "Test Loss:  77.50292909145355\n",
      "Epoch [7/500], Loss: 77.9892, Accuracy: 35.87 %\n",
      "Testing Accuracy: 36.95 %\n",
      "Test Loss:  77.40940296649933\n",
      "Epoch [8/500], Loss: 78.2943, Accuracy: 36.07 %\n",
      "Testing Accuracy: 39.17 %\n",
      "Test Loss:  76.2563009262085\n",
      "Epoch [9/500], Loss: 77.6937, Accuracy: 36.46 %\n",
      "Testing Accuracy: 38.47 %\n",
      "Test Loss:  76.07465922832489\n",
      "Epoch [10/500], Loss: 77.6966, Accuracy: 36.51 %\n",
      "Testing Accuracy: 37.85 %\n",
      "Test Loss:  76.96137034893036\n",
      "Epoch [11/500], Loss: 78.5540, Accuracy: 35.73 %\n",
      "Testing Accuracy: 38.78 %\n",
      "Test Loss:  76.88796651363373\n",
      "Epoch [12/500], Loss: 77.9436, Accuracy: 36.14 %\n",
      "Testing Accuracy: 39.24 %\n",
      "Test Loss:  75.35728812217712\n",
      "Epoch [13/500], Loss: 77.6235, Accuracy: 36.50 %\n",
      "Testing Accuracy: 38.66 %\n",
      "Test Loss:  76.40842509269714\n",
      "Epoch [14/500], Loss: 77.8038, Accuracy: 36.54 %\n",
      "Testing Accuracy: 38.84 %\n",
      "Test Loss:  76.2382550239563\n",
      "Epoch [15/500], Loss: 77.4951, Accuracy: 36.90 %\n",
      "Testing Accuracy: 36.46 %\n",
      "Test Loss:  77.82479238510132\n",
      "Epoch [16/500], Loss: 77.5042, Accuracy: 36.81 %\n",
      "Testing Accuracy: 36.83 %\n",
      "Test Loss:  78.13216388225555\n",
      "Epoch [17/500], Loss: 77.8639, Accuracy: 36.12 %\n",
      "Testing Accuracy: 38.07 %\n",
      "Test Loss:  76.80386352539062\n",
      "Epoch [18/500], Loss: 77.2802, Accuracy: 37.01 %\n",
      "Testing Accuracy: 39.10 %\n",
      "Test Loss:  75.98768484592438\n",
      "Epoch [19/500], Loss: 77.3439, Accuracy: 36.69 %\n",
      "Testing Accuracy: 39.24 %\n",
      "Test Loss:  76.23972594738007\n",
      "Epoch [20/500], Loss: 77.0138, Accuracy: 36.85 %\n",
      "Testing Accuracy: 38.22 %\n",
      "Test Loss:  77.34214353561401\n",
      "Epoch [21/500], Loss: 77.9276, Accuracy: 36.42 %\n",
      "Testing Accuracy: 39.17 %\n",
      "Test Loss:  75.68322014808655\n",
      "Epoch [22/500], Loss: 77.4060, Accuracy: 36.63 %\n",
      "Testing Accuracy: 38.92 %\n",
      "Test Loss:  75.75071918964386\n",
      "Epoch [23/500], Loss: 77.5709, Accuracy: 36.64 %\n",
      "Testing Accuracy: 37.54 %\n",
      "Test Loss:  77.29568266868591\n",
      "Epoch [24/500], Loss: 77.3545, Accuracy: 36.56 %\n",
      "Testing Accuracy: 38.73 %\n",
      "Test Loss:  76.41972184181213\n",
      "Epoch [25/500], Loss: 78.0249, Accuracy: 36.09 %\n",
      "Testing Accuracy: 38.48 %\n",
      "Test Loss:  77.8848649263382\n",
      "Epoch [26/500], Loss: 78.0557, Accuracy: 36.08 %\n",
      "Testing Accuracy: 39.60 %\n",
      "Test Loss:  75.50974452495575\n",
      "Epoch [27/500], Loss: 77.5392, Accuracy: 36.58 %\n",
      "Testing Accuracy: 37.75 %\n",
      "Test Loss:  78.1425529718399\n",
      "Epoch [28/500], Loss: 77.1651, Accuracy: 37.00 %\n",
      "Testing Accuracy: 39.08 %\n",
      "Test Loss:  75.53322064876556\n",
      "Epoch [29/500], Loss: 77.1162, Accuracy: 37.04 %\n",
      "Testing Accuracy: 39.26 %\n",
      "Test Loss:  75.81970918178558\n",
      "Epoch [30/500], Loss: 77.2744, Accuracy: 36.78 %\n",
      "Testing Accuracy: 38.73 %\n",
      "Test Loss:  75.72241115570068\n",
      "Epoch [31/500], Loss: 76.8021, Accuracy: 37.08 %\n",
      "Testing Accuracy: 39.53 %\n",
      "Test Loss:  75.44275689125061\n",
      "Epoch [32/500], Loss: 77.5265, Accuracy: 36.63 %\n",
      "Testing Accuracy: 38.89 %\n",
      "Test Loss:  75.49335551261902\n",
      "Testing Accuracy: 38.89 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        661504   \n",
      "Net/Dropout[dropout]/onnx::Relu   646      \n",
      "Net/Linear[fc2]/onnx::Gemm        3553     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "665,703 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857]\n",
      "pruning hidden size:  323\n",
      "with hidden layer:  323\n",
      "removing:  (140, 282, 271)\n",
      "--- 30.781564712524414 seconds ---\n",
      "Epoch [1/500], Loss: 76.8482, Accuracy: 37.29 %\n",
      "Testing Accuracy: 37.24 %\n",
      "Test Loss:  77.01133668422699\n",
      "Epoch [2/500], Loss: 77.4407, Accuracy: 36.73 %\n",
      "Testing Accuracy: 39.02 %\n",
      "Test Loss:  75.88520908355713\n",
      "Epoch [3/500], Loss: 77.3275, Accuracy: 36.89 %\n",
      "Testing Accuracy: 39.47 %\n",
      "Test Loss:  75.49787950515747\n",
      "Epoch [4/500], Loss: 77.6261, Accuracy: 36.35 %\n",
      "Testing Accuracy: 39.58 %\n",
      "Test Loss:  75.3845683336258\n",
      "Epoch [5/500], Loss: 77.7526, Accuracy: 36.52 %\n",
      "Testing Accuracy: 38.51 %\n",
      "Test Loss:  76.31626617908478\n",
      "Epoch [6/500], Loss: 77.2176, Accuracy: 37.29 %\n",
      "Testing Accuracy: 39.06 %\n",
      "Test Loss:  76.29119038581848\n",
      "Epoch [7/500], Loss: 77.0459, Accuracy: 37.22 %\n",
      "Testing Accuracy: 39.57 %\n",
      "Test Loss:  75.2321230173111\n",
      "Epoch [8/500], Loss: 76.6614, Accuracy: 37.16 %\n",
      "Testing Accuracy: 39.06 %\n",
      "Test Loss:  76.24253940582275\n",
      "Epoch [9/500], Loss: 77.3266, Accuracy: 36.94 %\n",
      "Testing Accuracy: 38.40 %\n",
      "Test Loss:  76.32962572574615\n",
      "Epoch [10/500], Loss: 77.6279, Accuracy: 36.56 %\n",
      "Testing Accuracy: 38.73 %\n",
      "Test Loss:  77.19718158245087\n",
      "Epoch [11/500], Loss: 77.1854, Accuracy: 36.94 %\n",
      "Testing Accuracy: 37.96 %\n",
      "Test Loss:  77.88008880615234\n",
      "Epoch [12/500], Loss: 77.2100, Accuracy: 36.62 %\n",
      "Testing Accuracy: 37.29 %\n",
      "Test Loss:  76.96626698970795\n",
      "Epoch [13/500], Loss: 76.9277, Accuracy: 37.25 %\n",
      "Testing Accuracy: 38.98 %\n",
      "Test Loss:  76.2926037311554\n",
      "Epoch [14/500], Loss: 76.8052, Accuracy: 37.15 %\n",
      "Testing Accuracy: 38.89 %\n",
      "Test Loss:  76.32625758647919\n",
      "Epoch [15/500], Loss: 76.7587, Accuracy: 37.45 %\n",
      "Testing Accuracy: 39.13 %\n",
      "Test Loss:  76.01774954795837\n",
      "Epoch [16/500], Loss: 77.5788, Accuracy: 36.57 %\n",
      "Testing Accuracy: 38.83 %\n",
      "Test Loss:  75.93582952022552\n",
      "Epoch [17/500], Loss: 77.2794, Accuracy: 37.16 %\n",
      "Testing Accuracy: 39.81 %\n",
      "Test Loss:  75.70961666107178\n",
      "Epoch [18/500], Loss: 76.8521, Accuracy: 37.06 %\n",
      "Testing Accuracy: 39.41 %\n",
      "Test Loss:  75.74068915843964\n",
      "Epoch [19/500], Loss: 77.1820, Accuracy: 36.67 %\n",
      "Testing Accuracy: 37.94 %\n",
      "Test Loss:  77.13399481773376\n",
      "Epoch [20/500], Loss: 76.5793, Accuracy: 37.60 %\n",
      "Testing Accuracy: 36.51 %\n",
      "Test Loss:  77.92673993110657\n",
      "Epoch [21/500], Loss: 77.3782, Accuracy: 36.46 %\n",
      "Testing Accuracy: 38.21 %\n",
      "Test Loss:  76.8027834892273\n",
      "Epoch [22/500], Loss: 76.4817, Accuracy: 37.59 %\n",
      "Testing Accuracy: 38.49 %\n",
      "Test Loss:  77.3922883272171\n",
      "Epoch [23/500], Loss: 76.7471, Accuracy: 37.27 %\n",
      "Testing Accuracy: 39.74 %\n",
      "Test Loss:  75.13640284538269\n",
      "Epoch [24/500], Loss: 76.6416, Accuracy: 37.35 %\n",
      "Testing Accuracy: 37.94 %\n",
      "Test Loss:  79.43732178211212\n",
      "Epoch [25/500], Loss: 77.6145, Accuracy: 36.75 %\n",
      "Testing Accuracy: 38.76 %\n",
      "Test Loss:  76.16214215755463\n",
      "Epoch [26/500], Loss: 76.2464, Accuracy: 37.64 %\n",
      "Testing Accuracy: 39.58 %\n",
      "Test Loss:  75.25176322460175\n",
      "Epoch [27/500], Loss: 76.7410, Accuracy: 37.32 %\n",
      "Testing Accuracy: 38.20 %\n",
      "Test Loss:  76.27743995189667\n",
      "Epoch [28/500], Loss: 76.8433, Accuracy: 37.14 %\n",
      "Testing Accuracy: 39.55 %\n",
      "Test Loss:  75.93305969238281\n",
      "Epoch [29/500], Loss: 76.6423, Accuracy: 37.28 %\n",
      "Testing Accuracy: 38.94 %\n",
      "Test Loss:  75.92811155319214\n",
      "Epoch [30/500], Loss: 77.2574, Accuracy: 37.10 %\n",
      "Testing Accuracy: 38.24 %\n",
      "Test Loss:  77.11506307125092\n",
      "Epoch [31/500], Loss: 76.7137, Accuracy: 37.49 %\n",
      "Testing Accuracy: 39.27 %\n",
      "Test Loss:  76.20263493061066\n",
      "Epoch [32/500], Loss: 76.3923, Accuracy: 37.64 %\n",
      "Testing Accuracy: 38.26 %\n",
      "Test Loss:  76.6201354265213\n",
      "Epoch [33/500], Loss: 76.8495, Accuracy: 37.02 %\n",
      "Testing Accuracy: 38.79 %\n",
      "Test Loss:  76.28849601745605\n",
      "Epoch [34/500], Loss: 76.8006, Accuracy: 37.33 %\n",
      "Testing Accuracy: 38.92 %\n",
      "Test Loss:  75.77870571613312\n",
      "Epoch [35/500], Loss: 76.9064, Accuracy: 37.25 %\n",
      "Testing Accuracy: 37.97 %\n",
      "Test Loss:  75.69932091236115\n",
      "Epoch [36/500], Loss: 76.3862, Accuracy: 37.57 %\n",
      "Testing Accuracy: 39.95 %\n",
      "Test Loss:  74.75942122936249\n",
      "Epoch [37/500], Loss: 76.7645, Accuracy: 37.27 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  75.63418078422546\n",
      "Epoch [38/500], Loss: 76.1926, Accuracy: 37.85 %\n",
      "Testing Accuracy: 38.85 %\n",
      "Test Loss:  75.74533545970917\n",
      "Epoch [39/500], Loss: 76.8323, Accuracy: 37.25 %\n",
      "Testing Accuracy: 39.91 %\n",
      "Test Loss:  75.61357390880585\n",
      "Epoch [40/500], Loss: 76.5590, Accuracy: 37.53 %\n",
      "Testing Accuracy: 38.80 %\n",
      "Test Loss:  76.13101816177368\n",
      "Epoch [41/500], Loss: 76.3861, Accuracy: 37.60 %\n",
      "Testing Accuracy: 39.74 %\n",
      "Test Loss:  74.97962093353271\n",
      "Epoch [42/500], Loss: 76.2848, Accuracy: 37.61 %\n",
      "Testing Accuracy: 37.46 %\n",
      "Test Loss:  76.78692376613617\n",
      "Epoch [43/500], Loss: 76.4695, Accuracy: 37.22 %\n",
      "Testing Accuracy: 39.89 %\n",
      "Test Loss:  76.06776058673859\n",
      "Epoch [44/500], Loss: 76.8954, Accuracy: 37.37 %\n",
      "Testing Accuracy: 40.32 %\n",
      "Test Loss:  74.95488846302032\n",
      "Epoch [45/500], Loss: 76.2119, Accuracy: 37.61 %\n",
      "Testing Accuracy: 39.79 %\n",
      "Test Loss:  75.58750927448273\n",
      "Epoch [46/500], Loss: 76.6405, Accuracy: 37.26 %\n",
      "Testing Accuracy: 38.11 %\n",
      "Test Loss:  76.79725348949432\n",
      "Epoch [47/500], Loss: 76.6144, Accuracy: 37.46 %\n",
      "Testing Accuracy: 39.48 %\n",
      "Test Loss:  76.07635760307312\n",
      "Epoch [48/500], Loss: 76.1961, Accuracy: 37.83 %\n",
      "Testing Accuracy: 38.59 %\n",
      "Test Loss:  75.71074783802032\n",
      "Epoch [49/500], Loss: 75.9309, Accuracy: 38.09 %\n",
      "Testing Accuracy: 37.45 %\n",
      "Test Loss:  76.21410691738129\n",
      "Epoch [50/500], Loss: 76.6315, Accuracy: 37.06 %\n",
      "Testing Accuracy: 38.42 %\n",
      "Test Loss:  77.54714941978455\n",
      "Epoch [51/500], Loss: 76.0178, Accuracy: 38.02 %\n",
      "Testing Accuracy: 37.99 %\n",
      "Test Loss:  77.66244435310364\n",
      "Epoch [52/500], Loss: 75.8582, Accuracy: 38.04 %\n",
      "Testing Accuracy: 40.17 %\n",
      "Test Loss:  75.42895889282227\n",
      "Epoch [53/500], Loss: 75.8456, Accuracy: 38.34 %\n",
      "Testing Accuracy: 37.72 %\n",
      "Test Loss:  77.86992609500885\n",
      "Epoch [54/500], Loss: 77.6876, Accuracy: 36.55 %\n",
      "Testing Accuracy: 39.98 %\n",
      "Test Loss:  75.14662742614746\n",
      "Epoch [55/500], Loss: 75.6810, Accuracy: 38.58 %\n",
      "Testing Accuracy: 39.55 %\n",
      "Test Loss:  75.37635970115662\n",
      "Epoch [56/500], Loss: 75.9334, Accuracy: 38.24 %\n",
      "Testing Accuracy: 39.94 %\n",
      "Test Loss:  75.03303015232086\n",
      "Testing Accuracy: 39.94 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        655360   \n",
      "Net/Dropout[dropout]/onnx::Relu   640      \n",
      "Net/Linear[fc2]/onnx::Gemm        3520     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "659,520 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978]\n",
      "pruning hidden size:  320\n",
      "with hidden layer:  320\n",
      "removing:  (168, 280, 270)\n",
      "--- 29.896573305130005 seconds ---\n",
      "Epoch [1/500], Loss: 76.2061, Accuracy: 37.87 %\n",
      "Testing Accuracy: 39.53 %\n",
      "Test Loss:  75.17648541927338\n",
      "Epoch [2/500], Loss: 76.3292, Accuracy: 37.89 %\n",
      "Testing Accuracy: 39.27 %\n",
      "Test Loss:  76.09588193893433\n",
      "Epoch [3/500], Loss: 76.3990, Accuracy: 37.52 %\n",
      "Testing Accuracy: 38.03 %\n",
      "Test Loss:  76.29869592189789\n",
      "Epoch [4/500], Loss: 75.8415, Accuracy: 38.25 %\n",
      "Testing Accuracy: 37.51 %\n",
      "Test Loss:  76.9158593416214\n",
      "Epoch [5/500], Loss: 76.2340, Accuracy: 37.77 %\n",
      "Testing Accuracy: 39.68 %\n",
      "Test Loss:  75.62184190750122\n",
      "Epoch [6/500], Loss: 76.3217, Accuracy: 37.70 %\n",
      "Testing Accuracy: 39.05 %\n",
      "Test Loss:  75.28391003608704\n",
      "Epoch [7/500], Loss: 76.5910, Accuracy: 37.48 %\n",
      "Testing Accuracy: 39.02 %\n",
      "Test Loss:  75.42874038219452\n",
      "Epoch [8/500], Loss: 76.0501, Accuracy: 37.91 %\n",
      "Testing Accuracy: 39.03 %\n",
      "Test Loss:  75.5180686712265\n",
      "Epoch [9/500], Loss: 76.1640, Accuracy: 38.15 %\n",
      "Testing Accuracy: 38.37 %\n",
      "Test Loss:  76.66518580913544\n",
      "Epoch [10/500], Loss: 78.2070, Accuracy: 36.17 %\n",
      "Testing Accuracy: 39.55 %\n",
      "Test Loss:  76.25776505470276\n",
      "Epoch [11/500], Loss: 76.4111, Accuracy: 37.73 %\n",
      "Testing Accuracy: 39.51 %\n",
      "Test Loss:  75.42033541202545\n",
      "Epoch [12/500], Loss: 76.5567, Accuracy: 37.62 %\n",
      "Testing Accuracy: 38.68 %\n",
      "Test Loss:  76.17262780666351\n",
      "Epoch [13/500], Loss: 75.8684, Accuracy: 37.89 %\n",
      "Testing Accuracy: 39.64 %\n",
      "Test Loss:  75.31030225753784\n",
      "Epoch [14/500], Loss: 75.8688, Accuracy: 38.23 %\n",
      "Testing Accuracy: 38.27 %\n",
      "Test Loss:  75.88109064102173\n",
      "Epoch [15/500], Loss: 76.5584, Accuracy: 37.44 %\n",
      "Testing Accuracy: 39.70 %\n",
      "Test Loss:  76.26005136966705\n",
      "Epoch [16/500], Loss: 75.8856, Accuracy: 38.36 %\n",
      "Testing Accuracy: 39.83 %\n",
      "Test Loss:  74.75721037387848\n",
      "Epoch [17/500], Loss: 75.8341, Accuracy: 38.23 %\n",
      "Testing Accuracy: 39.71 %\n",
      "Test Loss:  75.51408290863037\n",
      "Epoch [18/500], Loss: 75.9108, Accuracy: 38.16 %\n",
      "Testing Accuracy: 40.15 %\n",
      "Test Loss:  74.60049748420715\n",
      "Epoch [19/500], Loss: 76.3382, Accuracy: 38.06 %\n",
      "Testing Accuracy: 39.52 %\n",
      "Test Loss:  75.27755749225616\n",
      "Epoch [20/500], Loss: 75.9735, Accuracy: 38.00 %\n",
      "Testing Accuracy: 39.13 %\n",
      "Test Loss:  75.79300081729889\n",
      "Epoch [21/500], Loss: 76.0060, Accuracy: 37.67 %\n",
      "Testing Accuracy: 40.62 %\n",
      "Test Loss:  74.24166584014893\n",
      "Epoch [22/500], Loss: 76.2582, Accuracy: 37.88 %\n",
      "Testing Accuracy: 39.31 %\n",
      "Test Loss:  75.00383949279785\n",
      "Epoch [23/500], Loss: 76.0703, Accuracy: 37.95 %\n",
      "Testing Accuracy: 38.10 %\n",
      "Test Loss:  75.38943529129028\n",
      "Epoch [24/500], Loss: 76.0941, Accuracy: 38.01 %\n",
      "Testing Accuracy: 37.35 %\n",
      "Test Loss:  77.64020705223083\n",
      "Epoch [25/500], Loss: 76.3808, Accuracy: 37.93 %\n",
      "Testing Accuracy: 38.34 %\n",
      "Test Loss:  76.18290853500366\n",
      "Epoch [26/500], Loss: 76.9393, Accuracy: 37.15 %\n",
      "Testing Accuracy: 40.69 %\n",
      "Test Loss:  74.30254054069519\n",
      "Epoch [27/500], Loss: 75.5736, Accuracy: 38.41 %\n",
      "Testing Accuracy: 39.03 %\n",
      "Test Loss:  75.38834381103516\n",
      "Epoch [28/500], Loss: 76.1316, Accuracy: 37.65 %\n",
      "Testing Accuracy: 40.02 %\n",
      "Test Loss:  74.93000543117523\n",
      "Epoch [29/500], Loss: 75.6457, Accuracy: 38.32 %\n",
      "Testing Accuracy: 39.86 %\n",
      "Test Loss:  74.64807450771332\n",
      "Epoch [30/500], Loss: 76.8039, Accuracy: 37.41 %\n",
      "Testing Accuracy: 39.96 %\n",
      "Test Loss:  75.15877532958984\n",
      "Epoch [31/500], Loss: 75.8620, Accuracy: 38.08 %\n",
      "Testing Accuracy: 37.25 %\n",
      "Test Loss:  76.81624782085419\n",
      "Epoch [32/500], Loss: 75.9831, Accuracy: 37.86 %\n",
      "Testing Accuracy: 40.12 %\n",
      "Test Loss:  74.97037196159363\n",
      "Epoch [33/500], Loss: 76.2357, Accuracy: 38.10 %\n",
      "Testing Accuracy: 40.16 %\n",
      "Test Loss:  75.58965682983398\n",
      "Epoch [34/500], Loss: 75.5605, Accuracy: 38.32 %\n",
      "Testing Accuracy: 39.38 %\n",
      "Test Loss:  75.94380497932434\n",
      "Epoch [35/500], Loss: 76.8472, Accuracy: 37.52 %\n",
      "Testing Accuracy: 39.76 %\n",
      "Test Loss:  75.1119933128357\n",
      "Epoch [36/500], Loss: 75.9900, Accuracy: 38.23 %\n",
      "Testing Accuracy: 40.37 %\n",
      "Test Loss:  74.84682834148407\n",
      "Epoch [37/500], Loss: 75.9170, Accuracy: 38.27 %\n",
      "Testing Accuracy: 40.62 %\n",
      "Test Loss:  74.54401338100433\n",
      "Epoch [38/500], Loss: 75.6964, Accuracy: 38.37 %\n",
      "Testing Accuracy: 38.37 %\n",
      "Test Loss:  76.20232701301575\n",
      "Epoch [39/500], Loss: 75.7447, Accuracy: 38.09 %\n",
      "Testing Accuracy: 40.01 %\n",
      "Test Loss:  75.35906732082367\n",
      "Epoch [40/500], Loss: 76.0887, Accuracy: 37.91 %\n",
      "Testing Accuracy: 39.70 %\n",
      "Test Loss:  74.949413895607\n",
      "Epoch [41/500], Loss: 75.3966, Accuracy: 38.52 %\n",
      "Testing Accuracy: 39.57 %\n",
      "Test Loss:  76.05473113059998\n",
      "Testing Accuracy: 39.57 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        649216   \n",
      "Net/Dropout[dropout]/onnx::Relu   634      \n",
      "Net/Linear[fc2]/onnx::Gemm        3487     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "653,337 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386]\n",
      "pruning hidden size:  317\n",
      "with hidden layer:  317\n",
      "removing:  (315, 278, 269)\n",
      "--- 29.043532609939575 seconds ---\n",
      "Epoch [1/500], Loss: 75.5928, Accuracy: 38.46 %\n",
      "Testing Accuracy: 39.66 %\n",
      "Test Loss:  75.4331567287445\n",
      "Epoch [2/500], Loss: 75.9959, Accuracy: 37.82 %\n",
      "Testing Accuracy: 39.13 %\n",
      "Test Loss:  74.93756067752838\n",
      "Epoch [3/500], Loss: 76.2774, Accuracy: 37.82 %\n",
      "Testing Accuracy: 39.85 %\n",
      "Test Loss:  76.41196489334106\n",
      "Epoch [4/500], Loss: 75.6771, Accuracy: 38.60 %\n",
      "Testing Accuracy: 38.75 %\n",
      "Test Loss:  75.7237594127655\n",
      "Epoch [5/500], Loss: 75.5845, Accuracy: 38.37 %\n",
      "Testing Accuracy: 39.15 %\n",
      "Test Loss:  75.18459391593933\n",
      "Epoch [6/500], Loss: 75.5989, Accuracy: 38.21 %\n",
      "Testing Accuracy: 40.21 %\n",
      "Test Loss:  75.1460108757019\n",
      "Epoch [7/500], Loss: 76.3094, Accuracy: 37.86 %\n",
      "Testing Accuracy: 37.41 %\n",
      "Test Loss:  77.48153829574585\n",
      "Epoch [8/500], Loss: 76.2360, Accuracy: 38.22 %\n",
      "Testing Accuracy: 39.16 %\n",
      "Test Loss:  74.81604623794556\n",
      "Epoch [9/500], Loss: 75.9329, Accuracy: 37.91 %\n",
      "Testing Accuracy: 39.43 %\n",
      "Test Loss:  75.1506689786911\n",
      "Epoch [10/500], Loss: 75.5907, Accuracy: 38.36 %\n",
      "Testing Accuracy: 39.64 %\n",
      "Test Loss:  75.30620384216309\n",
      "Epoch [11/500], Loss: 75.6249, Accuracy: 38.26 %\n",
      "Testing Accuracy: 40.01 %\n",
      "Test Loss:  74.49833714962006\n",
      "Epoch [12/500], Loss: 75.2974, Accuracy: 38.55 %\n",
      "Testing Accuracy: 38.46 %\n",
      "Test Loss:  75.43928182125092\n",
      "Epoch [13/500], Loss: 75.9038, Accuracy: 38.01 %\n",
      "Testing Accuracy: 40.75 %\n",
      "Test Loss:  74.27643477916718\n",
      "Epoch [14/500], Loss: 75.7463, Accuracy: 38.70 %\n",
      "Testing Accuracy: 40.15 %\n",
      "Test Loss:  74.71275079250336\n",
      "Epoch [15/500], Loss: 75.6442, Accuracy: 38.42 %\n",
      "Testing Accuracy: 39.26 %\n",
      "Test Loss:  75.38570201396942\n",
      "Epoch [16/500], Loss: 75.6344, Accuracy: 38.21 %\n",
      "Testing Accuracy: 39.58 %\n",
      "Test Loss:  74.65425324440002\n",
      "Epoch [17/500], Loss: 75.1506, Accuracy: 38.93 %\n",
      "Testing Accuracy: 39.87 %\n",
      "Test Loss:  75.06151378154755\n",
      "Epoch [18/500], Loss: 75.5357, Accuracy: 38.32 %\n",
      "Testing Accuracy: 39.74 %\n",
      "Test Loss:  75.29762399196625\n",
      "Epoch [19/500], Loss: 75.4334, Accuracy: 38.57 %\n",
      "Testing Accuracy: 39.68 %\n",
      "Test Loss:  75.40403580665588\n",
      "Epoch [20/500], Loss: 75.7376, Accuracy: 38.08 %\n",
      "Testing Accuracy: 39.99 %\n",
      "Test Loss:  75.23297166824341\n",
      "Epoch [21/500], Loss: 76.0162, Accuracy: 37.87 %\n",
      "Testing Accuracy: 37.39 %\n",
      "Test Loss:  75.64937996864319\n",
      "Epoch [22/500], Loss: 75.7661, Accuracy: 38.51 %\n",
      "Testing Accuracy: 40.43 %\n",
      "Test Loss:  74.75698506832123\n",
      "Epoch [23/500], Loss: 75.3330, Accuracy: 39.18 %\n",
      "Testing Accuracy: 40.62 %\n",
      "Test Loss:  74.51692163944244\n",
      "Epoch [24/500], Loss: 75.4132, Accuracy: 38.57 %\n",
      "Testing Accuracy: 39.39 %\n",
      "Test Loss:  75.40912556648254\n",
      "Epoch [25/500], Loss: 75.6278, Accuracy: 38.78 %\n",
      "Testing Accuracy: 40.12 %\n",
      "Test Loss:  75.11055850982666\n",
      "Epoch [26/500], Loss: 75.4761, Accuracy: 38.44 %\n",
      "Testing Accuracy: 39.83 %\n",
      "Test Loss:  74.63863599300385\n",
      "Epoch [27/500], Loss: 75.9537, Accuracy: 38.19 %\n",
      "Testing Accuracy: 40.28 %\n",
      "Test Loss:  74.7925672531128\n",
      "Epoch [28/500], Loss: 75.4579, Accuracy: 38.57 %\n",
      "Testing Accuracy: 39.01 %\n",
      "Test Loss:  75.78267860412598\n",
      "Epoch [29/500], Loss: 75.9220, Accuracy: 38.36 %\n",
      "Testing Accuracy: 39.17 %\n",
      "Test Loss:  75.18250572681427\n",
      "Epoch [30/500], Loss: 75.7859, Accuracy: 38.48 %\n",
      "Testing Accuracy: 39.35 %\n",
      "Test Loss:  75.21987569332123\n",
      "Epoch [31/500], Loss: 75.4337, Accuracy: 38.39 %\n",
      "Testing Accuracy: 37.78 %\n",
      "Test Loss:  76.42403304576874\n",
      "Epoch [32/500], Loss: 75.9724, Accuracy: 38.29 %\n",
      "Testing Accuracy: 38.94 %\n",
      "Test Loss:  75.32462346553802\n",
      "Epoch [33/500], Loss: 75.0520, Accuracy: 38.72 %\n",
      "Testing Accuracy: 40.13 %\n",
      "Test Loss:  75.05132532119751\n",
      "Testing Accuracy: 40.13 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        643072   \n",
      "Net/Dropout[dropout]/onnx::Relu   628      \n",
      "Net/Linear[fc2]/onnx::Gemm        3454     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "647,154 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746]\n",
      "pruning hidden size:  314\n",
      "with hidden layer:  314\n",
      "removing:  (295, 221, 172)\n",
      "--- 29.565868139266968 seconds ---\n",
      "Epoch [1/500], Loss: 79.9341, Accuracy: 35.32 %\n",
      "Testing Accuracy: 36.65 %\n",
      "Test Loss:  79.50456619262695\n",
      "Epoch [2/500], Loss: 79.5741, Accuracy: 36.71 %\n",
      "Testing Accuracy: 38.33 %\n",
      "Test Loss:  77.0830249786377\n",
      "Epoch [3/500], Loss: 79.4119, Accuracy: 36.26 %\n",
      "Testing Accuracy: 36.66 %\n",
      "Test Loss:  79.30483281612396\n",
      "Epoch [4/500], Loss: 78.9622, Accuracy: 36.62 %\n",
      "Testing Accuracy: 35.56 %\n",
      "Test Loss:  79.15638899803162\n",
      "Epoch [5/500], Loss: 79.4438, Accuracy: 36.20 %\n",
      "Testing Accuracy: 38.53 %\n",
      "Test Loss:  76.5040522813797\n",
      "Epoch [6/500], Loss: 78.4462, Accuracy: 37.25 %\n",
      "Testing Accuracy: 38.40 %\n",
      "Test Loss:  76.3613873720169\n",
      "Epoch [7/500], Loss: 79.0166, Accuracy: 36.84 %\n",
      "Testing Accuracy: 37.85 %\n",
      "Test Loss:  77.01319169998169\n",
      "Epoch [8/500], Loss: 78.9714, Accuracy: 36.85 %\n",
      "Testing Accuracy: 36.43 %\n",
      "Test Loss:  77.84962177276611\n",
      "Epoch [9/500], Loss: 78.8526, Accuracy: 37.16 %\n",
      "Testing Accuracy: 38.21 %\n",
      "Test Loss:  76.7954649925232\n",
      "Epoch [10/500], Loss: 78.9406, Accuracy: 36.58 %\n",
      "Testing Accuracy: 36.16 %\n",
      "Test Loss:  79.27486038208008\n",
      "Epoch [11/500], Loss: 79.1443, Accuracy: 36.68 %\n",
      "Testing Accuracy: 37.47 %\n",
      "Test Loss:  77.31076920032501\n",
      "Epoch [12/500], Loss: 78.9632, Accuracy: 36.71 %\n",
      "Testing Accuracy: 37.95 %\n",
      "Test Loss:  77.4305055141449\n",
      "Epoch [13/500], Loss: 79.1396, Accuracy: 36.69 %\n",
      "Testing Accuracy: 38.11 %\n",
      "Test Loss:  77.13938999176025\n",
      "Epoch [14/500], Loss: 78.3167, Accuracy: 36.91 %\n",
      "Testing Accuracy: 36.51 %\n",
      "Test Loss:  79.7126544713974\n",
      "Epoch [15/500], Loss: 78.8734, Accuracy: 36.73 %\n",
      "Testing Accuracy: 38.28 %\n",
      "Test Loss:  76.88042950630188\n",
      "Epoch [16/500], Loss: 79.3273, Accuracy: 36.44 %\n",
      "Testing Accuracy: 36.92 %\n",
      "Test Loss:  77.289142370224\n",
      "Epoch [17/500], Loss: 78.7939, Accuracy: 36.91 %\n",
      "Testing Accuracy: 37.09 %\n",
      "Test Loss:  78.1335961818695\n",
      "Epoch [18/500], Loss: 80.0455, Accuracy: 35.86 %\n",
      "Testing Accuracy: 36.78 %\n",
      "Test Loss:  77.40059995651245\n",
      "Epoch [19/500], Loss: 78.6058, Accuracy: 37.27 %\n",
      "Testing Accuracy: 37.47 %\n",
      "Test Loss:  77.18347156047821\n",
      "Epoch [20/500], Loss: 78.7878, Accuracy: 36.63 %\n",
      "Testing Accuracy: 37.63 %\n",
      "Test Loss:  77.7248682975769\n",
      "Epoch [21/500], Loss: 78.3528, Accuracy: 37.20 %\n",
      "Testing Accuracy: 36.56 %\n",
      "Test Loss:  77.80370700359344\n",
      "Epoch [22/500], Loss: 79.3182, Accuracy: 36.17 %\n",
      "Testing Accuracy: 37.18 %\n",
      "Test Loss:  77.90430021286011\n",
      "Epoch [23/500], Loss: 78.7823, Accuracy: 37.24 %\n",
      "Testing Accuracy: 38.09 %\n",
      "Test Loss:  77.24316966533661\n",
      "Epoch [24/500], Loss: 78.2481, Accuracy: 37.15 %\n",
      "Testing Accuracy: 38.56 %\n",
      "Test Loss:  76.7239351272583\n",
      "Epoch [25/500], Loss: 78.2782, Accuracy: 37.37 %\n",
      "Testing Accuracy: 37.96 %\n",
      "Test Loss:  76.6753431558609\n",
      "Epoch [26/500], Loss: 79.6724, Accuracy: 35.76 %\n",
      "Testing Accuracy: 37.04 %\n",
      "Test Loss:  78.62731862068176\n",
      "Testing Accuracy: 37.04 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        636928   \n",
      "Net/Dropout[dropout]/onnx::Relu   622      \n",
      "Net/Linear[fc2]/onnx::Gemm        3421     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "640,971 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746, 37.04266279223352]\n",
      "pruning hidden size:  311\n",
      "with hidden layer:  311\n",
      "removing:  (274, 74, 115)\n",
      "--- 29.989640712738037 seconds ---\n",
      "Epoch [1/500], Loss: 84.6541, Accuracy: 33.74 %\n",
      "Testing Accuracy: 34.79 %\n",
      "Test Loss:  83.09408712387085\n",
      "Epoch [2/500], Loss: 84.3994, Accuracy: 33.46 %\n",
      "Testing Accuracy: 34.77 %\n",
      "Test Loss:  82.2324571609497\n",
      "Epoch [3/500], Loss: 83.9762, Accuracy: 33.54 %\n",
      "Testing Accuracy: 32.27 %\n",
      "Test Loss:  83.78036820888519\n",
      "Epoch [4/500], Loss: 84.6112, Accuracy: 32.66 %\n",
      "Testing Accuracy: 32.31 %\n",
      "Test Loss:  84.38616442680359\n",
      "Epoch [5/500], Loss: 83.8587, Accuracy: 33.01 %\n",
      "Testing Accuracy: 34.73 %\n",
      "Test Loss:  81.38538730144501\n",
      "Epoch [6/500], Loss: 82.9427, Accuracy: 33.51 %\n",
      "Testing Accuracy: 34.26 %\n",
      "Test Loss:  82.25571262836456\n",
      "Epoch [7/500], Loss: 82.7691, Accuracy: 33.67 %\n",
      "Testing Accuracy: 33.95 %\n",
      "Test Loss:  81.65931844711304\n",
      "Epoch [8/500], Loss: 83.0269, Accuracy: 33.70 %\n",
      "Testing Accuracy: 33.71 %\n",
      "Test Loss:  82.56007182598114\n",
      "Epoch [9/500], Loss: 83.0584, Accuracy: 32.97 %\n",
      "Testing Accuracy: 31.99 %\n",
      "Test Loss:  83.91489708423615\n",
      "Epoch [10/500], Loss: 82.8611, Accuracy: 33.18 %\n",
      "Testing Accuracy: 34.87 %\n",
      "Test Loss:  80.7609475851059\n",
      "Epoch [11/500], Loss: 82.4774, Accuracy: 33.54 %\n",
      "Testing Accuracy: 34.82 %\n",
      "Test Loss:  80.5917592048645\n",
      "Epoch [12/500], Loss: 83.1675, Accuracy: 32.65 %\n",
      "Testing Accuracy: 31.79 %\n",
      "Test Loss:  82.95566618442535\n",
      "Epoch [13/500], Loss: 83.0690, Accuracy: 32.81 %\n",
      "Testing Accuracy: 34.14 %\n",
      "Test Loss:  81.5013245344162\n",
      "Epoch [14/500], Loss: 82.3875, Accuracy: 33.68 %\n",
      "Testing Accuracy: 33.87 %\n",
      "Test Loss:  80.93336284160614\n",
      "Epoch [15/500], Loss: 83.3978, Accuracy: 32.46 %\n",
      "Testing Accuracy: 32.73 %\n",
      "Test Loss:  82.20350754261017\n",
      "Epoch [16/500], Loss: 83.8932, Accuracy: 32.16 %\n",
      "Testing Accuracy: 33.48 %\n",
      "Test Loss:  82.27671253681183\n",
      "Epoch [17/500], Loss: 82.4468, Accuracy: 33.22 %\n",
      "Testing Accuracy: 34.81 %\n",
      "Test Loss:  80.2097053527832\n",
      "Epoch [18/500], Loss: 82.4400, Accuracy: 33.23 %\n",
      "Testing Accuracy: 34.56 %\n",
      "Test Loss:  80.18636155128479\n",
      "Epoch [19/500], Loss: 82.4185, Accuracy: 33.31 %\n",
      "Testing Accuracy: 31.88 %\n",
      "Test Loss:  83.91564345359802\n",
      "Epoch [20/500], Loss: 82.1997, Accuracy: 33.29 %\n",
      "Testing Accuracy: 34.28 %\n",
      "Test Loss:  80.64957869052887\n",
      "Epoch [21/500], Loss: 82.8120, Accuracy: 33.33 %\n",
      "Testing Accuracy: 32.28 %\n",
      "Test Loss:  83.01562857627869\n",
      "Epoch [22/500], Loss: 82.6475, Accuracy: 33.22 %\n",
      "Testing Accuracy: 31.29 %\n",
      "Test Loss:  82.71356356143951\n",
      "Epoch [23/500], Loss: 83.0517, Accuracy: 32.83 %\n",
      "Testing Accuracy: 32.64 %\n",
      "Test Loss:  83.28590762615204\n",
      "Epoch [24/500], Loss: 82.9159, Accuracy: 32.77 %\n",
      "Testing Accuracy: 33.30 %\n",
      "Test Loss:  81.89143288135529\n",
      "Epoch [25/500], Loss: 83.4948, Accuracy: 32.60 %\n",
      "Testing Accuracy: 33.61 %\n",
      "Test Loss:  82.00008988380432\n",
      "Epoch [26/500], Loss: 82.3667, Accuracy: 33.32 %\n",
      "Testing Accuracy: 34.53 %\n",
      "Test Loss:  80.34516763687134\n",
      "Epoch [27/500], Loss: 82.7304, Accuracy: 32.90 %\n",
      "Testing Accuracy: 33.75 %\n",
      "Test Loss:  80.9834600687027\n",
      "Epoch [28/500], Loss: 82.6427, Accuracy: 32.78 %\n",
      "Testing Accuracy: 33.47 %\n",
      "Test Loss:  82.49312460422516\n",
      "Epoch [29/500], Loss: 82.9685, Accuracy: 32.66 %\n",
      "Testing Accuracy: 34.43 %\n",
      "Test Loss:  80.97975146770477\n",
      "Epoch [30/500], Loss: 82.6119, Accuracy: 32.86 %\n",
      "Testing Accuracy: 33.32 %\n",
      "Test Loss:  81.98018753528595\n",
      "Epoch [31/500], Loss: 82.6350, Accuracy: 32.88 %\n",
      "Testing Accuracy: 34.17 %\n",
      "Test Loss:  81.23396801948547\n",
      "Epoch [32/500], Loss: 83.0012, Accuracy: 32.78 %\n",
      "Testing Accuracy: 34.38 %\n",
      "Test Loss:  79.96193647384644\n",
      "Epoch [33/500], Loss: 82.3852, Accuracy: 33.44 %\n",
      "Testing Accuracy: 34.93 %\n",
      "Test Loss:  79.63074481487274\n",
      "Epoch [34/500], Loss: 82.9478, Accuracy: 32.61 %\n",
      "Testing Accuracy: 33.06 %\n",
      "Test Loss:  80.88054084777832\n",
      "Epoch [35/500], Loss: 82.3184, Accuracy: 33.03 %\n",
      "Testing Accuracy: 33.15 %\n",
      "Test Loss:  81.6649374961853\n",
      "Epoch [36/500], Loss: 83.2678, Accuracy: 32.31 %\n",
      "Testing Accuracy: 33.85 %\n",
      "Test Loss:  81.10277903079987\n",
      "Epoch [37/500], Loss: 82.3567, Accuracy: 33.12 %\n",
      "Testing Accuracy: 33.43 %\n",
      "Test Loss:  81.18087351322174\n",
      "Epoch [38/500], Loss: 82.3595, Accuracy: 33.04 %\n",
      "Testing Accuracy: 33.98 %\n",
      "Test Loss:  80.07419157028198\n",
      "Epoch [39/500], Loss: 82.7822, Accuracy: 32.81 %\n",
      "Testing Accuracy: 33.95 %\n",
      "Test Loss:  81.01959836483002\n",
      "Epoch [40/500], Loss: 82.6811, Accuracy: 32.66 %\n",
      "Testing Accuracy: 32.61 %\n",
      "Test Loss:  82.65631055831909\n",
      "Epoch [41/500], Loss: 83.0562, Accuracy: 32.26 %\n",
      "Testing Accuracy: 33.89 %\n",
      "Test Loss:  80.97640299797058\n",
      "Epoch [42/500], Loss: 82.2100, Accuracy: 33.01 %\n",
      "Testing Accuracy: 34.30 %\n",
      "Test Loss:  80.3079081773758\n",
      "Epoch [43/500], Loss: 81.9932, Accuracy: 33.22 %\n",
      "Testing Accuracy: 33.95 %\n",
      "Test Loss:  80.8219850063324\n",
      "Epoch [44/500], Loss: 81.8291, Accuracy: 33.45 %\n",
      "Testing Accuracy: 34.06 %\n",
      "Test Loss:  81.13837158679962\n",
      "Epoch [45/500], Loss: 82.4093, Accuracy: 33.16 %\n",
      "Testing Accuracy: 33.87 %\n",
      "Test Loss:  80.77694964408875\n",
      "Epoch [46/500], Loss: 82.0091, Accuracy: 33.43 %\n",
      "Testing Accuracy: 34.42 %\n",
      "Test Loss:  80.3517290353775\n",
      "Epoch [47/500], Loss: 81.6826, Accuracy: 33.47 %\n",
      "Testing Accuracy: 34.09 %\n",
      "Test Loss:  80.14573037624359\n",
      "Epoch [48/500], Loss: 81.9834, Accuracy: 33.19 %\n",
      "Testing Accuracy: 34.10 %\n",
      "Test Loss:  80.44121861457825\n",
      "Epoch [49/500], Loss: 81.7617, Accuracy: 33.53 %\n",
      "Testing Accuracy: 33.40 %\n",
      "Test Loss:  80.68676590919495\n",
      "Epoch [50/500], Loss: 82.7759, Accuracy: 32.28 %\n",
      "Testing Accuracy: 32.05 %\n",
      "Test Loss:  81.04655170440674\n",
      "Epoch [51/500], Loss: 82.4405, Accuracy: 32.72 %\n",
      "Testing Accuracy: 33.62 %\n",
      "Test Loss:  81.59019362926483\n",
      "Epoch [52/500], Loss: 82.8308, Accuracy: 32.39 %\n",
      "Testing Accuracy: 33.62 %\n",
      "Test Loss:  80.78838264942169\n",
      "Epoch [53/500], Loss: 82.4615, Accuracy: 32.85 %\n",
      "Testing Accuracy: 34.02 %\n",
      "Test Loss:  80.51760256290436\n",
      "Testing Accuracy: 34.02 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        630784   \n",
      "Net/Dropout[dropout]/onnx::Relu   616      \n",
      "Net/Linear[fc2]/onnx::Gemm        3388     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "634,788 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746, 37.04266279223352, 34.024567428345]\n",
      "pruning hidden size:  308\n",
      "with hidden layer:  308\n",
      "removing:  (235, 162, 35)\n",
      "--- 28.632866859436035 seconds ---\n",
      "Epoch [1/500], Loss: 85.5484, Accuracy: 27.58 %\n",
      "Testing Accuracy: 28.85 %\n",
      "Test Loss:  83.16995072364807\n",
      "Epoch [2/500], Loss: 84.9398, Accuracy: 27.55 %\n",
      "Testing Accuracy: 28.29 %\n",
      "Test Loss:  84.33869183063507\n",
      "Epoch [3/500], Loss: 84.8321, Accuracy: 27.60 %\n",
      "Testing Accuracy: 27.93 %\n",
      "Test Loss:  84.4125097990036\n",
      "Epoch [4/500], Loss: 85.9378, Accuracy: 27.04 %\n",
      "Testing Accuracy: 26.90 %\n",
      "Test Loss:  86.25670087337494\n",
      "Epoch [5/500], Loss: 85.1730, Accuracy: 27.49 %\n",
      "Testing Accuracy: 27.41 %\n",
      "Test Loss:  85.64025807380676\n",
      "Epoch [6/500], Loss: 85.3594, Accuracy: 27.40 %\n",
      "Testing Accuracy: 29.03 %\n",
      "Test Loss:  83.52098751068115\n",
      "Epoch [7/500], Loss: 85.0392, Accuracy: 27.44 %\n",
      "Testing Accuracy: 28.92 %\n",
      "Test Loss:  82.95099377632141\n",
      "Epoch [8/500], Loss: 85.2196, Accuracy: 27.65 %\n",
      "Testing Accuracy: 28.66 %\n",
      "Test Loss:  84.14590394496918\n",
      "Epoch [9/500], Loss: 85.1875, Accuracy: 27.44 %\n",
      "Testing Accuracy: 29.59 %\n",
      "Test Loss:  82.37462961673737\n",
      "Epoch [10/500], Loss: 84.9081, Accuracy: 27.63 %\n",
      "Testing Accuracy: 29.10 %\n",
      "Test Loss:  83.15735507011414\n",
      "Epoch [11/500], Loss: 84.9665, Accuracy: 27.89 %\n",
      "Testing Accuracy: 29.61 %\n",
      "Test Loss:  82.452343583107\n",
      "Epoch [12/500], Loss: 84.7858, Accuracy: 27.79 %\n",
      "Testing Accuracy: 29.51 %\n",
      "Test Loss:  82.29589426517487\n",
      "Epoch [13/500], Loss: 85.4259, Accuracy: 27.32 %\n",
      "Testing Accuracy: 28.34 %\n",
      "Test Loss:  82.9257287979126\n",
      "Epoch [14/500], Loss: 85.1335, Accuracy: 27.43 %\n",
      "Testing Accuracy: 28.83 %\n",
      "Test Loss:  83.6466246843338\n",
      "Epoch [15/500], Loss: 84.8655, Accuracy: 27.68 %\n",
      "Testing Accuracy: 29.18 %\n",
      "Test Loss:  82.20587253570557\n",
      "Epoch [16/500], Loss: 85.0337, Accuracy: 27.46 %\n",
      "Testing Accuracy: 29.34 %\n",
      "Test Loss:  82.77937030792236\n",
      "Epoch [17/500], Loss: 84.5929, Accuracy: 27.72 %\n",
      "Testing Accuracy: 29.40 %\n",
      "Test Loss:  83.01791274547577\n",
      "Epoch [18/500], Loss: 84.8705, Accuracy: 27.56 %\n",
      "Testing Accuracy: 29.26 %\n",
      "Test Loss:  83.0094462633133\n",
      "Epoch [19/500], Loss: 84.7502, Accuracy: 28.02 %\n",
      "Testing Accuracy: 29.69 %\n",
      "Test Loss:  82.36190843582153\n",
      "Epoch [20/500], Loss: 85.0387, Accuracy: 27.43 %\n",
      "Testing Accuracy: 28.87 %\n",
      "Test Loss:  83.15259647369385\n",
      "Epoch [21/500], Loss: 84.6954, Accuracy: 27.73 %\n",
      "Testing Accuracy: 29.01 %\n",
      "Test Loss:  83.43053770065308\n",
      "Epoch [22/500], Loss: 84.8093, Accuracy: 27.61 %\n",
      "Testing Accuracy: 28.23 %\n",
      "Test Loss:  84.45986831188202\n",
      "Epoch [23/500], Loss: 85.5769, Accuracy: 27.33 %\n",
      "Testing Accuracy: 29.17 %\n",
      "Test Loss:  82.58883833885193\n",
      "Epoch [24/500], Loss: 84.4786, Accuracy: 28.04 %\n",
      "Testing Accuracy: 29.49 %\n",
      "Test Loss:  82.07826125621796\n",
      "Epoch [25/500], Loss: 84.5693, Accuracy: 27.97 %\n",
      "Testing Accuracy: 29.14 %\n",
      "Test Loss:  82.6673812866211\n",
      "Epoch [26/500], Loss: 84.8079, Accuracy: 27.64 %\n",
      "Testing Accuracy: 29.22 %\n",
      "Test Loss:  83.1837626695633\n",
      "Epoch [27/500], Loss: 84.6648, Accuracy: 28.03 %\n",
      "Testing Accuracy: 28.37 %\n",
      "Test Loss:  84.22201824188232\n",
      "Epoch [28/500], Loss: 84.6840, Accuracy: 27.81 %\n",
      "Testing Accuracy: 28.60 %\n",
      "Test Loss:  84.23445689678192\n",
      "Epoch [29/500], Loss: 84.6377, Accuracy: 27.87 %\n",
      "Testing Accuracy: 29.52 %\n",
      "Test Loss:  82.26769852638245\n",
      "Epoch [30/500], Loss: 85.0757, Accuracy: 27.55 %\n",
      "Testing Accuracy: 29.40 %\n",
      "Test Loss:  81.98028588294983\n",
      "Epoch [31/500], Loss: 85.6738, Accuracy: 27.52 %\n",
      "Testing Accuracy: 28.40 %\n",
      "Test Loss:  84.28475332260132\n",
      "Epoch [32/500], Loss: 84.4966, Accuracy: 27.81 %\n",
      "Testing Accuracy: 29.63 %\n",
      "Test Loss:  82.07707214355469\n",
      "Epoch [33/500], Loss: 84.8360, Accuracy: 27.74 %\n",
      "Testing Accuracy: 29.81 %\n",
      "Test Loss:  82.41686081886292\n",
      "Epoch [34/500], Loss: 84.6341, Accuracy: 27.93 %\n",
      "Testing Accuracy: 29.21 %\n",
      "Test Loss:  82.64140212535858\n",
      "Epoch [35/500], Loss: 84.4814, Accuracy: 27.97 %\n",
      "Testing Accuracy: 28.55 %\n",
      "Test Loss:  84.27295410633087\n",
      "Epoch [36/500], Loss: 84.8943, Accuracy: 27.55 %\n",
      "Testing Accuracy: 27.46 %\n",
      "Test Loss:  85.17319464683533\n",
      "Epoch [37/500], Loss: 84.7287, Accuracy: 27.87 %\n",
      "Testing Accuracy: 29.72 %\n",
      "Test Loss:  82.43092167377472\n",
      "Epoch [38/500], Loss: 84.6263, Accuracy: 27.84 %\n",
      "Testing Accuracy: 29.32 %\n",
      "Test Loss:  83.49229216575623\n",
      "Epoch [39/500], Loss: 84.6491, Accuracy: 27.83 %\n",
      "Testing Accuracy: 29.60 %\n",
      "Test Loss:  81.94259977340698\n",
      "Epoch [40/500], Loss: 84.5026, Accuracy: 27.75 %\n",
      "Testing Accuracy: 29.86 %\n",
      "Test Loss:  82.24211132526398\n",
      "Epoch [41/500], Loss: 84.5388, Accuracy: 27.93 %\n",
      "Testing Accuracy: 29.12 %\n",
      "Test Loss:  82.9778401851654\n",
      "Epoch [42/500], Loss: 84.8203, Accuracy: 27.76 %\n",
      "Testing Accuracy: 28.35 %\n",
      "Test Loss:  84.13111889362335\n",
      "Epoch [43/500], Loss: 84.9093, Accuracy: 27.69 %\n",
      "Testing Accuracy: 29.20 %\n",
      "Test Loss:  83.25770616531372\n",
      "Epoch [44/500], Loss: 84.6652, Accuracy: 28.15 %\n",
      "Testing Accuracy: 29.41 %\n",
      "Test Loss:  81.84918022155762\n",
      "Epoch [45/500], Loss: 84.5677, Accuracy: 27.53 %\n",
      "Testing Accuracy: 29.15 %\n",
      "Test Loss:  82.56131279468536\n",
      "Epoch [46/500], Loss: 84.9741, Accuracy: 27.73 %\n",
      "Testing Accuracy: 28.56 %\n",
      "Test Loss:  83.67604851722717\n",
      "Epoch [47/500], Loss: 84.8278, Accuracy: 27.63 %\n",
      "Testing Accuracy: 29.67 %\n",
      "Test Loss:  81.98542749881744\n",
      "Epoch [48/500], Loss: 84.8697, Accuracy: 27.76 %\n",
      "Testing Accuracy: 29.38 %\n",
      "Test Loss:  83.26797008514404\n",
      "Epoch [49/500], Loss: 85.0158, Accuracy: 27.69 %\n",
      "Testing Accuracy: 29.16 %\n",
      "Test Loss:  82.73247134685516\n",
      "Epoch [50/500], Loss: 84.2457, Accuracy: 28.27 %\n",
      "Testing Accuracy: 28.88 %\n",
      "Test Loss:  82.6382052898407\n",
      "Epoch [51/500], Loss: 84.7589, Accuracy: 27.78 %\n",
      "Testing Accuracy: 29.38 %\n",
      "Test Loss:  81.97462677955627\n",
      "Epoch [52/500], Loss: 84.3850, Accuracy: 28.01 %\n",
      "Testing Accuracy: 29.68 %\n",
      "Test Loss:  82.2935700416565\n",
      "Epoch [53/500], Loss: 84.4770, Accuracy: 27.88 %\n",
      "Testing Accuracy: 29.72 %\n",
      "Test Loss:  81.91059923171997\n",
      "Epoch [54/500], Loss: 84.9068, Accuracy: 27.79 %\n",
      "Testing Accuracy: 29.39 %\n",
      "Test Loss:  82.55415630340576\n",
      "Epoch [55/500], Loss: 84.6354, Accuracy: 27.58 %\n",
      "Testing Accuracy: 29.23 %\n",
      "Test Loss:  82.32411932945251\n",
      "Epoch [56/500], Loss: 84.1327, Accuracy: 28.05 %\n",
      "Testing Accuracy: 28.66 %\n",
      "Test Loss:  83.42511034011841\n",
      "Epoch [57/500], Loss: 84.5430, Accuracy: 27.95 %\n",
      "Testing Accuracy: 29.26 %\n",
      "Test Loss:  82.64023017883301\n",
      "Epoch [58/500], Loss: 84.6361, Accuracy: 27.66 %\n",
      "Testing Accuracy: 29.23 %\n",
      "Test Loss:  82.8609105348587\n",
      "Epoch [59/500], Loss: 84.7362, Accuracy: 27.67 %\n",
      "Testing Accuracy: 28.42 %\n",
      "Test Loss:  82.70105481147766\n",
      "Epoch [60/500], Loss: 84.8893, Accuracy: 27.50 %\n",
      "Testing Accuracy: 29.67 %\n",
      "Test Loss:  82.30187273025513\n",
      "Epoch [61/500], Loss: 84.1756, Accuracy: 28.10 %\n",
      "Testing Accuracy: 29.86 %\n",
      "Test Loss:  81.97751212120056\n",
      "Epoch [62/500], Loss: 84.8333, Accuracy: 27.57 %\n",
      "Testing Accuracy: 28.25 %\n",
      "Test Loss:  83.94957840442657\n",
      "Epoch [63/500], Loss: 84.5623, Accuracy: 27.88 %\n",
      "Testing Accuracy: 29.12 %\n",
      "Test Loss:  82.37982416152954\n",
      "Epoch [64/500], Loss: 84.4967, Accuracy: 27.85 %\n",
      "Testing Accuracy: 29.86 %\n",
      "Test Loss:  82.1538416147232\n",
      "Testing Accuracy: 29.86 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        624640   \n",
      "Net/Dropout[dropout]/onnx::Relu   610      \n",
      "Net/Linear[fc2]/onnx::Gemm        3355     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "628,605 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746, 37.04266279223352, 34.024567428345, 29.863954563465857]\n",
      "pruning hidden size:  305\n",
      "with hidden layer:  305\n",
      "removing:  (268, 29, 64)\n",
      "--- 27.80025553703308 seconds ---\n",
      "Epoch [1/500], Loss: 92.0004, Accuracy: 20.80 %\n",
      "Testing Accuracy: 22.34 %\n",
      "Test Loss:  90.0023227930069\n",
      "Epoch [2/500], Loss: 90.5365, Accuracy: 23.62 %\n",
      "Testing Accuracy: 25.02 %\n",
      "Test Loss:  89.0974509716034\n",
      "Epoch [3/500], Loss: 90.0120, Accuracy: 24.70 %\n",
      "Testing Accuracy: 25.20 %\n",
      "Test Loss:  88.69508707523346\n",
      "Epoch [4/500], Loss: 89.8172, Accuracy: 24.81 %\n",
      "Testing Accuracy: 24.56 %\n",
      "Test Loss:  89.5568665266037\n",
      "Epoch [5/500], Loss: 89.6374, Accuracy: 24.99 %\n",
      "Testing Accuracy: 25.12 %\n",
      "Test Loss:  88.47115695476532\n",
      "Epoch [6/500], Loss: 89.4449, Accuracy: 24.96 %\n",
      "Testing Accuracy: 25.32 %\n",
      "Test Loss:  87.9171599149704\n",
      "Epoch [7/500], Loss: 89.1886, Accuracy: 24.88 %\n",
      "Testing Accuracy: 25.51 %\n",
      "Test Loss:  87.7317202091217\n",
      "Epoch [8/500], Loss: 89.0673, Accuracy: 25.07 %\n",
      "Testing Accuracy: 25.29 %\n",
      "Test Loss:  87.67699420452118\n",
      "Epoch [9/500], Loss: 89.2079, Accuracy: 25.00 %\n",
      "Testing Accuracy: 25.68 %\n",
      "Test Loss:  87.60498261451721\n",
      "Epoch [10/500], Loss: 89.4508, Accuracy: 24.72 %\n",
      "Testing Accuracy: 25.62 %\n",
      "Test Loss:  87.5132417678833\n",
      "Epoch [11/500], Loss: 89.2075, Accuracy: 24.78 %\n",
      "Testing Accuracy: 25.23 %\n",
      "Test Loss:  88.34212803840637\n",
      "Epoch [12/500], Loss: 89.5598, Accuracy: 24.66 %\n",
      "Testing Accuracy: 25.58 %\n",
      "Test Loss:  87.17790162563324\n",
      "Epoch [13/500], Loss: 89.2306, Accuracy: 24.85 %\n",
      "Testing Accuracy: 25.84 %\n",
      "Test Loss:  87.05146837234497\n",
      "Epoch [14/500], Loss: 88.7843, Accuracy: 25.01 %\n",
      "Testing Accuracy: 25.45 %\n",
      "Test Loss:  87.17912292480469\n",
      "Epoch [15/500], Loss: 88.8924, Accuracy: 24.96 %\n",
      "Testing Accuracy: 25.86 %\n",
      "Test Loss:  87.00052571296692\n",
      "Epoch [16/500], Loss: 88.7905, Accuracy: 25.09 %\n",
      "Testing Accuracy: 25.78 %\n",
      "Test Loss:  86.81254935264587\n",
      "Epoch [17/500], Loss: 88.7372, Accuracy: 25.07 %\n",
      "Testing Accuracy: 25.53 %\n",
      "Test Loss:  87.03640043735504\n",
      "Epoch [18/500], Loss: 89.0024, Accuracy: 24.86 %\n",
      "Testing Accuracy: 25.65 %\n",
      "Test Loss:  86.81260013580322\n",
      "Epoch [19/500], Loss: 88.6498, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.30 %\n",
      "Test Loss:  87.25496470928192\n",
      "Epoch [20/500], Loss: 88.6580, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.05 %\n",
      "Test Loss:  87.57082986831665\n",
      "Epoch [21/500], Loss: 88.7361, Accuracy: 24.98 %\n",
      "Testing Accuracy: 24.69 %\n",
      "Test Loss:  88.25504100322723\n",
      "Epoch [22/500], Loss: 88.7115, Accuracy: 24.87 %\n",
      "Testing Accuracy: 25.85 %\n",
      "Test Loss:  86.51142525672913\n",
      "Epoch [23/500], Loss: 88.3046, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.72 %\n",
      "Test Loss:  86.58788394927979\n",
      "Epoch [24/500], Loss: 88.5803, Accuracy: 25.04 %\n",
      "Testing Accuracy: 25.55 %\n",
      "Test Loss:  86.6859837770462\n",
      "Epoch [25/500], Loss: 88.3380, Accuracy: 25.09 %\n",
      "Testing Accuracy: 25.59 %\n",
      "Test Loss:  86.63036513328552\n",
      "Epoch [26/500], Loss: 88.3934, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.78 %\n",
      "Test Loss:  86.46090602874756\n",
      "Epoch [27/500], Loss: 88.6244, Accuracy: 24.98 %\n",
      "Testing Accuracy: 25.60 %\n",
      "Test Loss:  86.54624712467194\n",
      "Epoch [28/500], Loss: 88.5414, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.21 %\n",
      "Test Loss:  87.23193347454071\n",
      "Epoch [29/500], Loss: 88.6304, Accuracy: 24.99 %\n",
      "Testing Accuracy: 25.54 %\n",
      "Test Loss:  86.72130846977234\n",
      "Epoch [30/500], Loss: 88.4686, Accuracy: 24.93 %\n",
      "Testing Accuracy: 25.90 %\n",
      "Test Loss:  86.22127497196198\n",
      "Epoch [31/500], Loss: 88.4641, Accuracy: 25.01 %\n",
      "Testing Accuracy: 25.31 %\n",
      "Test Loss:  87.10104811191559\n",
      "Epoch [32/500], Loss: 88.6393, Accuracy: 24.76 %\n",
      "Testing Accuracy: 24.86 %\n",
      "Test Loss:  87.70765721797943\n",
      "Epoch [33/500], Loss: 88.3036, Accuracy: 24.94 %\n",
      "Testing Accuracy: 25.30 %\n",
      "Test Loss:  87.04088008403778\n",
      "Epoch [34/500], Loss: 88.3736, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.66 %\n",
      "Test Loss:  86.36935186386108\n",
      "Epoch [35/500], Loss: 88.3983, Accuracy: 24.95 %\n",
      "Testing Accuracy: 25.56 %\n",
      "Test Loss:  86.48775100708008\n",
      "Epoch [36/500], Loss: 88.5978, Accuracy: 25.03 %\n",
      "Testing Accuracy: 25.90 %\n",
      "Test Loss:  86.0483547449112\n",
      "Epoch [37/500], Loss: 88.3198, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.39 %\n",
      "Test Loss:  86.76127445697784\n",
      "Epoch [38/500], Loss: 88.2420, Accuracy: 25.09 %\n",
      "Testing Accuracy: 25.38 %\n",
      "Test Loss:  86.69293928146362\n",
      "Epoch [39/500], Loss: 88.1453, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.69 %\n",
      "Test Loss:  86.33406257629395\n",
      "Epoch [40/500], Loss: 88.2083, Accuracy: 25.17 %\n",
      "Testing Accuracy: 24.86 %\n",
      "Test Loss:  87.48521304130554\n",
      "Epoch [41/500], Loss: 88.1428, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.51 %\n",
      "Test Loss:  86.66619765758514\n",
      "Epoch [42/500], Loss: 88.5120, Accuracy: 24.89 %\n",
      "Testing Accuracy: 25.91 %\n",
      "Test Loss:  85.93212282657623\n",
      "Epoch [43/500], Loss: 88.0166, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.59 %\n",
      "Test Loss:  86.34011721611023\n",
      "Epoch [44/500], Loss: 88.4503, Accuracy: 25.04 %\n",
      "Testing Accuracy: 25.68 %\n",
      "Test Loss:  86.28502893447876\n",
      "Epoch [45/500], Loss: 88.3208, Accuracy: 24.89 %\n",
      "Testing Accuracy: 25.72 %\n",
      "Test Loss:  86.18512809276581\n",
      "Epoch [46/500], Loss: 88.3282, Accuracy: 25.01 %\n",
      "Testing Accuracy: 25.82 %\n",
      "Test Loss:  85.96644508838654\n",
      "Epoch [47/500], Loss: 88.1718, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.18 %\n",
      "Test Loss:  87.00236749649048\n",
      "Epoch [48/500], Loss: 88.0883, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.93367779254913\n",
      "Epoch [49/500], Loss: 88.2814, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.93 %\n",
      "Test Loss:  85.9642596244812\n",
      "Epoch [50/500], Loss: 88.2680, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.69 %\n",
      "Test Loss:  86.20672059059143\n",
      "Epoch [51/500], Loss: 87.8688, Accuracy: 25.13 %\n",
      "Testing Accuracy: 25.30 %\n",
      "Test Loss:  86.84711945056915\n",
      "Epoch [52/500], Loss: 87.9973, Accuracy: 25.00 %\n",
      "Testing Accuracy: 25.31 %\n",
      "Test Loss:  86.75429892539978\n",
      "Epoch [53/500], Loss: 88.1489, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.40 %\n",
      "Test Loss:  86.64058792591095\n",
      "Epoch [54/500], Loss: 88.4886, Accuracy: 24.95 %\n",
      "Testing Accuracy: 25.16 %\n",
      "Test Loss:  87.02524888515472\n",
      "Epoch [55/500], Loss: 88.3102, Accuracy: 24.90 %\n",
      "Testing Accuracy: 25.80 %\n",
      "Test Loss:  85.96100091934204\n",
      "Epoch [56/500], Loss: 88.4567, Accuracy: 25.00 %\n",
      "Testing Accuracy: 24.77 %\n",
      "Test Loss:  87.4233570098877\n",
      "Epoch [57/500], Loss: 88.1751, Accuracy: 25.00 %\n",
      "Testing Accuracy: 25.60 %\n",
      "Test Loss:  86.20946431159973\n",
      "Epoch [58/500], Loss: 88.1659, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.91 %\n",
      "Test Loss:  85.79330348968506\n",
      "Epoch [59/500], Loss: 87.9148, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.50 %\n",
      "Test Loss:  86.37061500549316\n",
      "Epoch [60/500], Loss: 87.8693, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.93 %\n",
      "Test Loss:  85.8035455942154\n",
      "Epoch [61/500], Loss: 87.8663, Accuracy: 25.11 %\n",
      "Testing Accuracy: 26.07 %\n",
      "Test Loss:  85.65509283542633\n",
      "Epoch [62/500], Loss: 88.0301, Accuracy: 25.02 %\n",
      "Testing Accuracy: 25.53 %\n",
      "Test Loss:  86.27835810184479\n",
      "Epoch [63/500], Loss: 87.9326, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.63 %\n",
      "Test Loss:  86.06888616085052\n",
      "Epoch [64/500], Loss: 87.8154, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.92 %\n",
      "Test Loss:  85.5714476108551\n",
      "Epoch [65/500], Loss: 88.2192, Accuracy: 24.92 %\n",
      "Testing Accuracy: 25.54 %\n",
      "Test Loss:  86.24946784973145\n",
      "Epoch [66/500], Loss: 87.9704, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  85.7221212387085\n",
      "Epoch [67/500], Loss: 87.8800, Accuracy: 25.13 %\n",
      "Testing Accuracy: 25.60 %\n",
      "Test Loss:  86.27292966842651\n",
      "Epoch [68/500], Loss: 88.6051, Accuracy: 24.87 %\n",
      "Testing Accuracy: 24.99 %\n",
      "Test Loss:  87.12428891658783\n",
      "Epoch [69/500], Loss: 88.1956, Accuracy: 24.84 %\n",
      "Testing Accuracy: 25.60 %\n",
      "Test Loss:  86.0327718257904\n",
      "Epoch [70/500], Loss: 87.8165, Accuracy: 25.03 %\n",
      "Testing Accuracy: 26.01 %\n",
      "Test Loss:  85.49893426895142\n",
      "Epoch [71/500], Loss: 88.1547, Accuracy: 24.98 %\n",
      "Testing Accuracy: 26.09 %\n",
      "Test Loss:  85.52153491973877\n",
      "Epoch [72/500], Loss: 87.8931, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.84 %\n",
      "Test Loss:  85.75308609008789\n",
      "Epoch [73/500], Loss: 88.1225, Accuracy: 24.90 %\n",
      "Testing Accuracy: 24.94 %\n",
      "Test Loss:  87.11242353916168\n",
      "Epoch [74/500], Loss: 88.0607, Accuracy: 25.09 %\n",
      "Testing Accuracy: 25.78 %\n",
      "Test Loss:  85.81247162818909\n",
      "Epoch [75/500], Loss: 87.8866, Accuracy: 25.07 %\n",
      "Testing Accuracy: 26.04 %\n",
      "Test Loss:  85.45572566986084\n",
      "Epoch [76/500], Loss: 87.8853, Accuracy: 25.07 %\n",
      "Testing Accuracy: 25.14 %\n",
      "Test Loss:  86.82405531406403\n",
      "Epoch [77/500], Loss: 87.8809, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.91 %\n",
      "Test Loss:  85.6720712184906\n",
      "Epoch [78/500], Loss: 88.1009, Accuracy: 25.01 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  85.53146088123322\n",
      "Epoch [79/500], Loss: 87.9570, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.64128053188324\n",
      "Epoch [80/500], Loss: 88.2349, Accuracy: 24.86 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  85.3527899980545\n",
      "Epoch [81/500], Loss: 88.0489, Accuracy: 24.85 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  85.54469060897827\n",
      "Epoch [82/500], Loss: 87.9204, Accuracy: 25.09 %\n",
      "Testing Accuracy: 25.61 %\n",
      "Test Loss:  86.19698560237885\n",
      "Epoch [83/500], Loss: 87.9511, Accuracy: 25.12 %\n",
      "Testing Accuracy: 26.01 %\n",
      "Test Loss:  85.38167417049408\n",
      "Epoch [84/500], Loss: 87.9867, Accuracy: 25.00 %\n",
      "Testing Accuracy: 25.91 %\n",
      "Test Loss:  85.60657632350922\n",
      "Epoch [85/500], Loss: 87.7032, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.40158581733704\n",
      "Epoch [86/500], Loss: 87.8191, Accuracy: 25.13 %\n",
      "Testing Accuracy: 25.80 %\n",
      "Test Loss:  85.7181293964386\n",
      "Epoch [87/500], Loss: 87.7359, Accuracy: 25.15 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.59020054340363\n",
      "Epoch [88/500], Loss: 87.9144, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  85.34482991695404\n",
      "Epoch [89/500], Loss: 88.2441, Accuracy: 24.93 %\n",
      "Testing Accuracy: 25.98 %\n",
      "Test Loss:  85.26674294471741\n",
      "Epoch [90/500], Loss: 88.0142, Accuracy: 25.00 %\n",
      "Testing Accuracy: 25.73 %\n",
      "Test Loss:  85.78267014026642\n",
      "Epoch [91/500], Loss: 87.7759, Accuracy: 25.09 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  85.29222822189331\n",
      "Epoch [92/500], Loss: 87.8124, Accuracy: 25.16 %\n",
      "Testing Accuracy: 26.07 %\n",
      "Test Loss:  85.23875820636749\n",
      "Epoch [93/500], Loss: 87.7301, Accuracy: 25.18 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  85.26343059539795\n",
      "Epoch [94/500], Loss: 87.7958, Accuracy: 25.21 %\n",
      "Testing Accuracy: 25.76 %\n",
      "Test Loss:  85.76027584075928\n",
      "Epoch [95/500], Loss: 87.6955, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.82 %\n",
      "Test Loss:  85.76258540153503\n",
      "Epoch [96/500], Loss: 87.7853, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  85.43717229366302\n",
      "Epoch [97/500], Loss: 87.8657, Accuracy: 25.07 %\n",
      "Testing Accuracy: 24.98 %\n",
      "Test Loss:  87.04178702831268\n",
      "Epoch [98/500], Loss: 87.8665, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.71 %\n",
      "Test Loss:  85.86934840679169\n",
      "Epoch [99/500], Loss: 87.7121, Accuracy: 25.04 %\n",
      "Testing Accuracy: 25.91 %\n",
      "Test Loss:  85.32903528213501\n",
      "Epoch [100/500], Loss: 88.0225, Accuracy: 24.88 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.3697419166565\n",
      "Epoch [101/500], Loss: 87.5638, Accuracy: 25.36 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  85.26457214355469\n",
      "Epoch [102/500], Loss: 87.5565, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.88 %\n",
      "Test Loss:  85.58057677745819\n",
      "Epoch [103/500], Loss: 87.7709, Accuracy: 24.96 %\n",
      "Testing Accuracy: 26.09 %\n",
      "Test Loss:  85.18508327007294\n",
      "Epoch [104/500], Loss: 87.6312, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.52 %\n",
      "Test Loss:  86.10642170906067\n",
      "Epoch [105/500], Loss: 88.0317, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.05 %\n",
      "Test Loss:  86.9059910774231\n",
      "Epoch [106/500], Loss: 88.0238, Accuracy: 24.81 %\n",
      "Testing Accuracy: 26.13 %\n",
      "Test Loss:  85.18341064453125\n",
      "Epoch [107/500], Loss: 87.6927, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.06 %\n",
      "Test Loss:  86.80634450912476\n",
      "Epoch [108/500], Loss: 87.7870, Accuracy: 25.13 %\n",
      "Testing Accuracy: 25.75 %\n",
      "Test Loss:  85.91267681121826\n",
      "Epoch [109/500], Loss: 87.6071, Accuracy: 25.24 %\n",
      "Testing Accuracy: 25.88 %\n",
      "Test Loss:  85.65436792373657\n",
      "Epoch [110/500], Loss: 87.6703, Accuracy: 25.07 %\n",
      "Testing Accuracy: 25.98 %\n",
      "Test Loss:  85.09840250015259\n",
      "Epoch [111/500], Loss: 87.7677, Accuracy: 25.08 %\n",
      "Testing Accuracy: 25.51 %\n",
      "Test Loss:  86.14270484447479\n",
      "Epoch [112/500], Loss: 87.5336, Accuracy: 25.14 %\n",
      "Testing Accuracy: 26.07 %\n",
      "Test Loss:  85.09974646568298\n",
      "Epoch [113/500], Loss: 87.5527, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.88 %\n",
      "Test Loss:  85.46452009677887\n",
      "Epoch [114/500], Loss: 87.6082, Accuracy: 25.17 %\n",
      "Testing Accuracy: 26.11 %\n",
      "Test Loss:  85.08809459209442\n",
      "Epoch [115/500], Loss: 87.8334, Accuracy: 24.92 %\n",
      "Testing Accuracy: 25.82 %\n",
      "Test Loss:  85.6518942117691\n",
      "Epoch [116/500], Loss: 87.6428, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.58 %\n",
      "Test Loss:  85.99997913837433\n",
      "Epoch [117/500], Loss: 87.6207, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.72 %\n",
      "Test Loss:  85.75294589996338\n",
      "Epoch [118/500], Loss: 87.5454, Accuracy: 25.21 %\n",
      "Testing Accuracy: 26.08 %\n",
      "Test Loss:  85.07219731807709\n",
      "Epoch [119/500], Loss: 87.8594, Accuracy: 24.90 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  85.07273733615875\n",
      "Epoch [120/500], Loss: 87.5119, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.38 %\n",
      "Test Loss:  86.17949795722961\n",
      "Epoch [121/500], Loss: 87.5091, Accuracy: 25.24 %\n",
      "Testing Accuracy: 25.40 %\n",
      "Test Loss:  86.17768263816833\n",
      "Epoch [122/500], Loss: 87.8293, Accuracy: 25.02 %\n",
      "Testing Accuracy: 25.77 %\n",
      "Test Loss:  85.74358892440796\n",
      "Epoch [123/500], Loss: 87.5333, Accuracy: 25.13 %\n",
      "Testing Accuracy: 25.82 %\n",
      "Test Loss:  85.62406325340271\n",
      "Epoch [124/500], Loss: 87.6696, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.37 %\n",
      "Test Loss:  86.19203066825867\n",
      "Epoch [125/500], Loss: 87.4413, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.35 %\n",
      "Test Loss:  86.27607893943787\n",
      "Epoch [126/500], Loss: 87.4857, Accuracy: 25.23 %\n",
      "Testing Accuracy: 25.81 %\n",
      "Test Loss:  85.39082479476929\n",
      "Epoch [127/500], Loss: 87.3959, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.94 %\n",
      "Test Loss:  85.25347530841827\n",
      "Epoch [128/500], Loss: 87.6549, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.17 %\n",
      "Test Loss:  86.57667803764343\n",
      "Epoch [129/500], Loss: 87.5430, Accuracy: 25.20 %\n",
      "Testing Accuracy: 25.16 %\n",
      "Test Loss:  86.49896943569183\n",
      "Epoch [130/500], Loss: 87.4643, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.87 %\n",
      "Test Loss:  85.25636315345764\n",
      "Epoch [131/500], Loss: 87.7200, Accuracy: 25.06 %\n",
      "Testing Accuracy: 24.92 %\n",
      "Test Loss:  86.97630500793457\n",
      "Epoch [132/500], Loss: 87.7026, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.93 %\n",
      "Test Loss:  85.26210129261017\n",
      "Epoch [133/500], Loss: 87.3870, Accuracy: 25.23 %\n",
      "Testing Accuracy: 25.76 %\n",
      "Test Loss:  85.62275004386902\n",
      "Epoch [134/500], Loss: 87.4883, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  85.06942355632782\n",
      "Epoch [135/500], Loss: 87.8015, Accuracy: 24.97 %\n",
      "Testing Accuracy: 25.48 %\n",
      "Test Loss:  86.05097579956055\n",
      "Epoch [136/500], Loss: 88.1773, Accuracy: 24.86 %\n",
      "Testing Accuracy: 25.51 %\n",
      "Test Loss:  86.04477739334106\n",
      "Epoch [137/500], Loss: 87.5375, Accuracy: 25.14 %\n",
      "Testing Accuracy: 26.01 %\n",
      "Test Loss:  85.09901225566864\n",
      "Epoch [138/500], Loss: 87.7259, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.52 %\n",
      "Test Loss:  85.96127915382385\n",
      "Epoch [139/500], Loss: 87.7348, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  85.13015234470367\n",
      "Epoch [140/500], Loss: 87.5211, Accuracy: 25.19 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  85.18731486797333\n",
      "Epoch [141/500], Loss: 87.5245, Accuracy: 25.11 %\n",
      "Testing Accuracy: 26.11 %\n",
      "Test Loss:  84.93938755989075\n",
      "Epoch [142/500], Loss: 87.6654, Accuracy: 25.11 %\n",
      "Testing Accuracy: 25.92 %\n",
      "Test Loss:  85.14142727851868\n",
      "Epoch [143/500], Loss: 87.2330, Accuracy: 25.22 %\n",
      "Testing Accuracy: 26.00 %\n",
      "Test Loss:  85.01925981044769\n",
      "Epoch [144/500], Loss: 87.5508, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  85.0802686214447\n",
      "Epoch [145/500], Loss: 87.6169, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.31 %\n",
      "Test Loss:  85.95844674110413\n",
      "Epoch [146/500], Loss: 87.7935, Accuracy: 25.04 %\n",
      "Testing Accuracy: 26.11 %\n",
      "Test Loss:  84.92694854736328\n",
      "Epoch [147/500], Loss: 87.4293, Accuracy: 25.04 %\n",
      "Testing Accuracy: 25.85 %\n",
      "Test Loss:  85.19992554187775\n",
      "Epoch [148/500], Loss: 87.3558, Accuracy: 25.22 %\n",
      "Testing Accuracy: 25.98 %\n",
      "Test Loss:  84.92644393444061\n",
      "Epoch [149/500], Loss: 87.5685, Accuracy: 25.18 %\n",
      "Testing Accuracy: 25.19 %\n",
      "Test Loss:  86.39201974868774\n",
      "Epoch [150/500], Loss: 87.4526, Accuracy: 25.27 %\n",
      "Testing Accuracy: 25.71 %\n",
      "Test Loss:  85.5406848192215\n",
      "Epoch [151/500], Loss: 87.3369, Accuracy: 25.26 %\n",
      "Testing Accuracy: 25.86 %\n",
      "Test Loss:  85.25644612312317\n",
      "Epoch [152/500], Loss: 87.7897, Accuracy: 24.84 %\n",
      "Testing Accuracy: 25.15 %\n",
      "Test Loss:  86.46212697029114\n",
      "Epoch [153/500], Loss: 88.0359, Accuracy: 24.86 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.92681622505188\n",
      "Epoch [154/500], Loss: 87.2856, Accuracy: 25.22 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.88912189006805\n",
      "Epoch [155/500], Loss: 87.5237, Accuracy: 25.15 %\n",
      "Testing Accuracy: 25.69 %\n",
      "Test Loss:  85.18747329711914\n",
      "Epoch [156/500], Loss: 87.3403, Accuracy: 25.16 %\n",
      "Testing Accuracy: 25.74 %\n",
      "Test Loss:  85.66815268993378\n",
      "Epoch [157/500], Loss: 87.4228, Accuracy: 25.06 %\n",
      "Testing Accuracy: 25.84 %\n",
      "Test Loss:  85.36621356010437\n",
      "Epoch [158/500], Loss: 87.3717, Accuracy: 25.12 %\n",
      "Testing Accuracy: 25.75 %\n",
      "Test Loss:  85.46325516700745\n",
      "Epoch [159/500], Loss: 87.3784, Accuracy: 25.12 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  85.08268690109253\n",
      "Epoch [160/500], Loss: 87.4198, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  84.9532653093338\n",
      "Epoch [161/500], Loss: 87.4083, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.84 %\n",
      "Test Loss:  85.36724317073822\n",
      "Epoch [162/500], Loss: 87.2637, Accuracy: 25.38 %\n",
      "Testing Accuracy: 25.86 %\n",
      "Test Loss:  85.20836055278778\n",
      "Epoch [163/500], Loss: 87.2636, Accuracy: 25.18 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  85.03768765926361\n",
      "Epoch [164/500], Loss: 87.5956, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.84 %\n",
      "Test Loss:  85.18260526657104\n",
      "Epoch [165/500], Loss: 87.2335, Accuracy: 25.12 %\n",
      "Testing Accuracy: 25.90 %\n",
      "Test Loss:  85.02716624736786\n",
      "Epoch [166/500], Loss: 87.3778, Accuracy: 25.14 %\n",
      "Testing Accuracy: 26.03 %\n",
      "Test Loss:  84.91518652439117\n",
      "Epoch [167/500], Loss: 87.1878, Accuracy: 25.24 %\n",
      "Testing Accuracy: 26.17 %\n",
      "Test Loss:  84.81950891017914\n",
      "Epoch [168/500], Loss: 87.2067, Accuracy: 25.16 %\n",
      "Testing Accuracy: 26.04 %\n",
      "Test Loss:  84.82413589954376\n",
      "Epoch [169/500], Loss: 87.2189, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.20 %\n",
      "Test Loss:  86.40194499492645\n",
      "Epoch [170/500], Loss: 87.5173, Accuracy: 25.15 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  84.91717207431793\n",
      "Epoch [171/500], Loss: 87.1879, Accuracy: 25.31 %\n",
      "Testing Accuracy: 25.74 %\n",
      "Test Loss:  85.10345685482025\n",
      "Epoch [172/500], Loss: 87.5813, Accuracy: 25.08 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.02026009559631\n",
      "Epoch [173/500], Loss: 87.4944, Accuracy: 25.04 %\n",
      "Testing Accuracy: 25.38 %\n",
      "Test Loss:  86.10305607318878\n",
      "Epoch [174/500], Loss: 87.4297, Accuracy: 25.06 %\n",
      "Testing Accuracy: 26.10 %\n",
      "Test Loss:  84.85568618774414\n",
      "Epoch [175/500], Loss: 87.0820, Accuracy: 25.27 %\n",
      "Testing Accuracy: 26.04 %\n",
      "Test Loss:  84.80135607719421\n",
      "Epoch [176/500], Loss: 87.2177, Accuracy: 25.29 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.96418797969818\n",
      "Epoch [177/500], Loss: 87.5328, Accuracy: 25.01 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  84.9542248249054\n",
      "Epoch [178/500], Loss: 87.1828, Accuracy: 25.27 %\n",
      "Testing Accuracy: 25.94 %\n",
      "Test Loss:  85.00536918640137\n",
      "Epoch [179/500], Loss: 87.3862, Accuracy: 25.25 %\n",
      "Testing Accuracy: 26.07 %\n",
      "Test Loss:  84.76592373847961\n",
      "Epoch [180/500], Loss: 87.5584, Accuracy: 24.99 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  84.87241852283478\n",
      "Epoch [181/500], Loss: 87.9391, Accuracy: 24.93 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  84.7378545999527\n",
      "Epoch [182/500], Loss: 87.4698, Accuracy: 25.03 %\n",
      "Testing Accuracy: 25.70 %\n",
      "Test Loss:  85.20214068889618\n",
      "Epoch [183/500], Loss: 88.1896, Accuracy: 24.59 %\n",
      "Testing Accuracy: 25.85 %\n",
      "Test Loss:  84.95613169670105\n",
      "Epoch [184/500], Loss: 87.1060, Accuracy: 25.29 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  84.9953305721283\n",
      "Epoch [185/500], Loss: 87.4887, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  84.9357408285141\n",
      "Epoch [186/500], Loss: 87.3191, Accuracy: 25.24 %\n",
      "Testing Accuracy: 25.80 %\n",
      "Test Loss:  85.14587366580963\n",
      "Epoch [187/500], Loss: 87.6492, Accuracy: 25.02 %\n",
      "Testing Accuracy: 25.73 %\n",
      "Test Loss:  85.35451126098633\n",
      "Epoch [188/500], Loss: 87.3994, Accuracy: 25.04 %\n",
      "Testing Accuracy: 26.15 %\n",
      "Test Loss:  84.74552178382874\n",
      "Epoch [189/500], Loss: 87.2508, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.77 %\n",
      "Test Loss:  85.32395267486572\n",
      "Epoch [190/500], Loss: 87.3069, Accuracy: 25.23 %\n",
      "Testing Accuracy: 25.39 %\n",
      "Test Loss:  86.05358505249023\n",
      "Epoch [191/500], Loss: 87.2746, Accuracy: 25.09 %\n",
      "Testing Accuracy: 26.00 %\n",
      "Test Loss:  85.00241446495056\n",
      "Epoch [192/500], Loss: 87.2318, Accuracy: 25.16 %\n",
      "Testing Accuracy: 26.07 %\n",
      "Test Loss:  84.75864100456238\n",
      "Epoch [193/500], Loss: 87.0447, Accuracy: 25.29 %\n",
      "Testing Accuracy: 25.73 %\n",
      "Test Loss:  85.39696741104126\n",
      "Epoch [194/500], Loss: 87.6323, Accuracy: 24.95 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.77696514129639\n",
      "Epoch [195/500], Loss: 87.2378, Accuracy: 25.17 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  84.68918359279633\n",
      "Epoch [196/500], Loss: 87.2440, Accuracy: 25.21 %\n",
      "Testing Accuracy: 25.29 %\n",
      "Test Loss:  86.24971425533295\n",
      "Epoch [197/500], Loss: 87.0741, Accuracy: 25.25 %\n",
      "Testing Accuracy: 25.59 %\n",
      "Test Loss:  85.42788469791412\n",
      "Epoch [198/500], Loss: 87.2464, Accuracy: 25.18 %\n",
      "Testing Accuracy: 25.49 %\n",
      "Test Loss:  85.35237264633179\n",
      "Epoch [199/500], Loss: 87.4195, Accuracy: 25.05 %\n",
      "Testing Accuracy: 25.54 %\n",
      "Test Loss:  85.6522786617279\n",
      "Epoch [200/500], Loss: 87.0711, Accuracy: 25.24 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  84.85365664958954\n",
      "Epoch [201/500], Loss: 87.4589, Accuracy: 24.97 %\n",
      "Testing Accuracy: 25.83 %\n",
      "Test Loss:  85.14568626880646\n",
      "Epoch [202/500], Loss: 87.2879, Accuracy: 25.16 %\n",
      "Testing Accuracy: 26.01 %\n",
      "Test Loss:  84.67075681686401\n",
      "Epoch [203/500], Loss: 87.4154, Accuracy: 24.94 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  84.67663013935089\n",
      "Epoch [204/500], Loss: 87.5369, Accuracy: 24.93 %\n",
      "Testing Accuracy: 26.10 %\n",
      "Test Loss:  84.68400943279266\n",
      "Epoch [205/500], Loss: 87.1840, Accuracy: 25.07 %\n",
      "Testing Accuracy: 25.61 %\n",
      "Test Loss:  85.50871360301971\n",
      "Epoch [206/500], Loss: 87.2058, Accuracy: 25.16 %\n",
      "Testing Accuracy: 26.00 %\n",
      "Test Loss:  84.70431053638458\n",
      "Epoch [207/500], Loss: 87.2211, Accuracy: 25.20 %\n",
      "Testing Accuracy: 26.04 %\n",
      "Test Loss:  84.82244038581848\n",
      "Epoch [208/500], Loss: 87.3437, Accuracy: 25.18 %\n",
      "Testing Accuracy: 25.50 %\n",
      "Test Loss:  85.73890149593353\n",
      "Epoch [209/500], Loss: 87.4700, Accuracy: 25.19 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.6848977804184\n",
      "Epoch [210/500], Loss: 87.1792, Accuracy: 25.21 %\n",
      "Testing Accuracy: 25.00 %\n",
      "Test Loss:  86.77845239639282\n",
      "Epoch [211/500], Loss: 87.7788, Accuracy: 24.81 %\n",
      "Testing Accuracy: 25.75 %\n",
      "Test Loss:  85.31215131282806\n",
      "Epoch [212/500], Loss: 87.0952, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.50 %\n",
      "Test Loss:  85.70663809776306\n",
      "Epoch [213/500], Loss: 87.0082, Accuracy: 25.31 %\n",
      "Testing Accuracy: 26.09 %\n",
      "Test Loss:  84.68326914310455\n",
      "Epoch [214/500], Loss: 87.1826, Accuracy: 25.23 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  84.90356969833374\n",
      "Epoch [215/500], Loss: 87.1508, Accuracy: 25.13 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.70328950881958\n",
      "Epoch [216/500], Loss: 87.0198, Accuracy: 25.28 %\n",
      "Testing Accuracy: 25.80 %\n",
      "Test Loss:  85.05478405952454\n",
      "Epoch [217/500], Loss: 87.6364, Accuracy: 24.82 %\n",
      "Testing Accuracy: 26.06 %\n",
      "Test Loss:  84.5900387763977\n",
      "Epoch [218/500], Loss: 87.1192, Accuracy: 25.24 %\n",
      "Testing Accuracy: 25.84 %\n",
      "Test Loss:  85.05170023441315\n",
      "Epoch [219/500], Loss: 87.0116, Accuracy: 25.29 %\n",
      "Testing Accuracy: 25.46 %\n",
      "Test Loss:  85.57410514354706\n",
      "Epoch [220/500], Loss: 87.7646, Accuracy: 24.90 %\n",
      "Testing Accuracy: 24.88 %\n",
      "Test Loss:  87.14854216575623\n",
      "Epoch [221/500], Loss: 87.4567, Accuracy: 25.08 %\n",
      "Testing Accuracy: 26.02 %\n",
      "Test Loss:  84.73933839797974\n",
      "Epoch [222/500], Loss: 87.4087, Accuracy: 24.98 %\n",
      "Testing Accuracy: 25.66 %\n",
      "Test Loss:  85.33510136604309\n",
      "Epoch [223/500], Loss: 87.3711, Accuracy: 24.98 %\n",
      "Testing Accuracy: 25.43 %\n",
      "Test Loss:  85.89208710193634\n",
      "Epoch [224/500], Loss: 87.2104, Accuracy: 25.10 %\n",
      "Testing Accuracy: 25.90 %\n",
      "Test Loss:  84.68393075466156\n",
      "Epoch [225/500], Loss: 87.0594, Accuracy: 25.20 %\n",
      "Testing Accuracy: 26.02 %\n",
      "Test Loss:  84.6313841342926\n",
      "Epoch [226/500], Loss: 87.3643, Accuracy: 25.02 %\n",
      "Testing Accuracy: 24.00 %\n",
      "Test Loss:  89.32406544685364\n",
      "Epoch [227/500], Loss: 87.9050, Accuracy: 24.83 %\n",
      "Testing Accuracy: 25.12 %\n",
      "Test Loss:  86.55956053733826\n",
      "Epoch [228/500], Loss: 87.0168, Accuracy: 25.21 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  84.71792435646057\n",
      "Epoch [229/500], Loss: 87.0921, Accuracy: 25.30 %\n",
      "Testing Accuracy: 25.69 %\n",
      "Test Loss:  85.22842133045197\n",
      "Epoch [230/500], Loss: 87.2868, Accuracy: 25.14 %\n",
      "Testing Accuracy: 25.97 %\n",
      "Test Loss:  84.7933019399643\n",
      "Epoch [231/500], Loss: 87.2495, Accuracy: 25.14 %\n",
      "Testing Accuracy: 26.04 %\n",
      "Test Loss:  84.60953915119171\n",
      "Epoch [232/500], Loss: 87.4464, Accuracy: 24.86 %\n",
      "Testing Accuracy: 25.95 %\n",
      "Test Loss:  84.62311220169067\n",
      "Epoch [233/500], Loss: 87.0118, Accuracy: 25.25 %\n",
      "Testing Accuracy: 25.99 %\n",
      "Test Loss:  84.72226524353027\n",
      "Epoch [234/500], Loss: 87.1183, Accuracy: 25.17 %\n",
      "Testing Accuracy: 25.92 %\n",
      "Test Loss:  84.61312937736511\n",
      "Epoch [235/500], Loss: 87.1901, Accuracy: 25.23 %\n",
      "Testing Accuracy: 25.89 %\n",
      "Test Loss:  85.02355921268463\n",
      "Epoch [236/500], Loss: 87.1484, Accuracy: 25.13 %\n",
      "Testing Accuracy: 26.05 %\n",
      "Test Loss:  84.60312366485596\n",
      "Epoch [237/500], Loss: 87.1565, Accuracy: 25.21 %\n",
      "Testing Accuracy: 26.11 %\n",
      "Test Loss:  84.6498191356659\n",
      "Testing Accuracy: 26.11 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        618496   \n",
      "Net/Dropout[dropout]/onnx::Relu   604      \n",
      "Net/Linear[fc2]/onnx::Gemm        3322     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "622,422 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746, 37.04266279223352, 34.024567428345, 29.863954563465857, 26.10619469026549]\n",
      "pruning hidden size:  302\n",
      "with hidden layer:  302\n",
      "removing:  (174, 111, 22)\n",
      "--- 28.39085340499878 seconds ---\n",
      "Epoch [1/500], Loss: 99.2332, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  98.12640023231506\n",
      "Epoch [2/500], Loss: 97.9838, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  97.04371404647827\n",
      "Epoch [3/500], Loss: 97.1074, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  96.51625227928162\n",
      "Epoch [4/500], Loss: 96.7010, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  96.18993186950684\n",
      "Epoch [5/500], Loss: 96.4185, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  96.02390098571777\n",
      "Epoch [6/500], Loss: 96.3102, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.9017117023468\n",
      "Epoch [7/500], Loss: 96.1976, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.83099508285522\n",
      "Epoch [8/500], Loss: 96.1546, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.80795383453369\n",
      "Epoch [9/500], Loss: 96.1158, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.77888369560242\n",
      "Epoch [10/500], Loss: 96.0576, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.7656672000885\n",
      "Epoch [11/500], Loss: 96.0748, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.7503650188446\n",
      "Epoch [12/500], Loss: 96.0715, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.75536608695984\n",
      "Epoch [13/500], Loss: 96.0551, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.74389481544495\n",
      "Epoch [14/500], Loss: 96.0311, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.7350685596466\n",
      "Epoch [15/500], Loss: 96.0492, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.7467930316925\n",
      "Epoch [16/500], Loss: 96.0319, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.74635672569275\n",
      "Epoch [17/500], Loss: 96.0553, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.73610353469849\n",
      "Epoch [18/500], Loss: 96.0496, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.74688816070557\n",
      "Epoch [19/500], Loss: 96.0185, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.74909257888794\n",
      "Epoch [20/500], Loss: 96.0223, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.7196033000946\n",
      "Epoch [21/500], Loss: 96.0013, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.74752402305603\n",
      "Epoch [22/500], Loss: 96.0444, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.71292209625244\n",
      "Epoch [23/500], Loss: 96.0789, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.74300217628479\n",
      "Epoch [24/500], Loss: 96.0311, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.72251510620117\n",
      "Epoch [25/500], Loss: 96.0200, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.70461010932922\n",
      "Epoch [26/500], Loss: 96.0059, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.65040016174316\n",
      "Epoch [27/500], Loss: 95.9411, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.60813283920288\n",
      "Epoch [28/500], Loss: 95.8370, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.41926956176758\n",
      "Epoch [29/500], Loss: 95.6203, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  95.06574153900146\n",
      "Epoch [30/500], Loss: 95.2398, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  94.53068733215332\n",
      "Epoch [31/500], Loss: 94.7084, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  93.92690134048462\n",
      "Epoch [32/500], Loss: 94.1489, Accuracy: 21.16 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  93.29084086418152\n",
      "Epoch [33/500], Loss: 93.7155, Accuracy: 21.19 %\n",
      "Testing Accuracy: 21.78 %\n",
      "Test Loss:  92.72280263900757\n",
      "Epoch [34/500], Loss: 93.2936, Accuracy: 21.37 %\n",
      "Testing Accuracy: 21.86 %\n",
      "Test Loss:  92.39438581466675\n",
      "Epoch [35/500], Loss: 93.0049, Accuracy: 21.26 %\n",
      "Testing Accuracy: 21.95 %\n",
      "Test Loss:  91.69710803031921\n",
      "Epoch [36/500], Loss: 92.7178, Accuracy: 21.25 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  91.74094605445862\n",
      "Epoch [37/500], Loss: 92.5913, Accuracy: 21.12 %\n",
      "Testing Accuracy: 21.91 %\n",
      "Test Loss:  91.23000812530518\n",
      "Epoch [38/500], Loss: 92.3951, Accuracy: 21.26 %\n",
      "Testing Accuracy: 21.95 %\n",
      "Test Loss:  90.75407230854034\n",
      "Epoch [39/500], Loss: 92.2248, Accuracy: 21.15 %\n",
      "Testing Accuracy: 22.03 %\n",
      "Test Loss:  90.53927206993103\n",
      "Epoch [40/500], Loss: 92.2918, Accuracy: 21.21 %\n",
      "Testing Accuracy: 21.94 %\n",
      "Test Loss:  90.56031060218811\n",
      "Epoch [41/500], Loss: 92.0757, Accuracy: 21.20 %\n",
      "Testing Accuracy: 21.96 %\n",
      "Test Loss:  90.37261140346527\n",
      "Epoch [42/500], Loss: 92.0236, Accuracy: 21.30 %\n",
      "Testing Accuracy: 21.90 %\n",
      "Test Loss:  90.31205070018768\n",
      "Epoch [43/500], Loss: 92.0591, Accuracy: 21.10 %\n",
      "Testing Accuracy: 21.77 %\n",
      "Test Loss:  90.0122948884964\n",
      "Epoch [44/500], Loss: 91.8788, Accuracy: 21.22 %\n",
      "Testing Accuracy: 21.93 %\n",
      "Test Loss:  90.05612075328827\n",
      "Epoch [45/500], Loss: 91.9037, Accuracy: 21.10 %\n",
      "Testing Accuracy: 21.89 %\n",
      "Test Loss:  89.80377280712128\n",
      "Epoch [46/500], Loss: 91.7525, Accuracy: 21.18 %\n",
      "Testing Accuracy: 22.02 %\n",
      "Test Loss:  89.84318590164185\n",
      "Epoch [47/500], Loss: 91.7188, Accuracy: 21.33 %\n",
      "Testing Accuracy: 22.02 %\n",
      "Test Loss:  89.64732348918915\n",
      "Epoch [48/500], Loss: 91.7476, Accuracy: 21.18 %\n",
      "Testing Accuracy: 21.56 %\n",
      "Test Loss:  89.55165338516235\n",
      "Epoch [49/500], Loss: 91.6263, Accuracy: 21.24 %\n",
      "Testing Accuracy: 22.05 %\n",
      "Test Loss:  89.49516928195953\n",
      "Epoch [50/500], Loss: 91.7844, Accuracy: 21.13 %\n",
      "Testing Accuracy: 22.10 %\n",
      "Test Loss:  89.44830644130707\n",
      "Epoch [51/500], Loss: 91.6967, Accuracy: 21.22 %\n",
      "Testing Accuracy: 21.88 %\n",
      "Test Loss:  89.41412472724915\n",
      "Epoch [52/500], Loss: 91.5734, Accuracy: 21.24 %\n",
      "Testing Accuracy: 21.54 %\n",
      "Test Loss:  89.64060473442078\n",
      "Epoch [53/500], Loss: 91.5658, Accuracy: 21.15 %\n",
      "Testing Accuracy: 21.87 %\n",
      "Test Loss:  89.30586326122284\n",
      "Epoch [54/500], Loss: 91.4966, Accuracy: 21.24 %\n",
      "Testing Accuracy: 22.00 %\n",
      "Test Loss:  89.26813173294067\n",
      "Epoch [55/500], Loss: 91.4372, Accuracy: 21.06 %\n",
      "Testing Accuracy: 21.87 %\n",
      "Test Loss:  89.24200594425201\n",
      "Epoch [56/500], Loss: 91.5651, Accuracy: 21.12 %\n",
      "Testing Accuracy: 22.04 %\n",
      "Test Loss:  89.35434293746948\n",
      "Epoch [57/500], Loss: 91.5502, Accuracy: 21.18 %\n",
      "Testing Accuracy: 21.77 %\n",
      "Test Loss:  89.10698783397675\n",
      "Epoch [58/500], Loss: 91.4955, Accuracy: 21.18 %\n",
      "Testing Accuracy: 21.86 %\n",
      "Test Loss:  89.17283463478088\n",
      "Epoch [59/500], Loss: 91.4938, Accuracy: 21.05 %\n",
      "Testing Accuracy: 21.91 %\n",
      "Test Loss:  89.16813719272614\n",
      "Epoch [60/500], Loss: 91.4347, Accuracy: 21.05 %\n",
      "Testing Accuracy: 21.58 %\n",
      "Test Loss:  89.04372918605804\n",
      "Epoch [61/500], Loss: 91.3402, Accuracy: 21.20 %\n",
      "Testing Accuracy: 22.02 %\n",
      "Test Loss:  89.18048107624054\n",
      "Epoch [62/500], Loss: 91.2697, Accuracy: 21.24 %\n",
      "Testing Accuracy: 21.57 %\n",
      "Test Loss:  88.93627846240997\n",
      "Epoch [63/500], Loss: 91.3388, Accuracy: 21.17 %\n",
      "Testing Accuracy: 22.06 %\n",
      "Test Loss:  89.97355270385742\n",
      "Epoch [64/500], Loss: 91.3320, Accuracy: 20.89 %\n",
      "Testing Accuracy: 25.17 %\n",
      "Test Loss:  88.89826548099518\n",
      "Epoch [65/500], Loss: 91.2815, Accuracy: 21.08 %\n",
      "Testing Accuracy: 24.84 %\n",
      "Test Loss:  88.91720294952393\n",
      "Epoch [66/500], Loss: 91.4611, Accuracy: 21.00 %\n",
      "Testing Accuracy: 21.59 %\n",
      "Test Loss:  89.61828374862671\n",
      "Epoch [67/500], Loss: 91.1530, Accuracy: 21.04 %\n",
      "Testing Accuracy: 25.08 %\n",
      "Test Loss:  89.01558530330658\n",
      "Epoch [68/500], Loss: 91.1859, Accuracy: 20.95 %\n",
      "Testing Accuracy: 25.45 %\n",
      "Test Loss:  88.81544351577759\n",
      "Epoch [69/500], Loss: 91.2211, Accuracy: 21.10 %\n",
      "Testing Accuracy: 24.99 %\n",
      "Test Loss:  88.78701603412628\n",
      "Epoch [70/500], Loss: 91.2180, Accuracy: 21.09 %\n",
      "Testing Accuracy: 25.09 %\n",
      "Test Loss:  89.39478921890259\n",
      "Epoch [71/500], Loss: 91.1397, Accuracy: 21.12 %\n",
      "Testing Accuracy: 25.05 %\n",
      "Test Loss:  88.65321886539459\n",
      "Epoch [72/500], Loss: 91.2372, Accuracy: 20.86 %\n",
      "Testing Accuracy: 25.12 %\n",
      "Test Loss:  88.77478086948395\n",
      "Epoch [73/500], Loss: 91.0856, Accuracy: 21.04 %\n",
      "Testing Accuracy: 25.39 %\n",
      "Test Loss:  88.66908860206604\n",
      "Epoch [74/500], Loss: 91.1666, Accuracy: 20.84 %\n",
      "Testing Accuracy: 25.56 %\n",
      "Test Loss:  88.90246295928955\n",
      "Epoch [75/500], Loss: 91.0300, Accuracy: 21.12 %\n",
      "Testing Accuracy: 25.03 %\n",
      "Test Loss:  89.73836624622345\n",
      "Epoch [76/500], Loss: 91.1796, Accuracy: 21.07 %\n",
      "Testing Accuracy: 24.63 %\n",
      "Test Loss:  88.53740859031677\n",
      "Epoch [77/500], Loss: 91.0862, Accuracy: 21.13 %\n",
      "Testing Accuracy: 25.54 %\n",
      "Test Loss:  88.95109248161316\n",
      "Epoch [78/500], Loss: 91.0010, Accuracy: 21.13 %\n",
      "Testing Accuracy: 25.39 %\n",
      "Test Loss:  88.51845252513885\n",
      "Epoch [79/500], Loss: 90.8997, Accuracy: 21.00 %\n",
      "Testing Accuracy: 25.59 %\n",
      "Test Loss:  88.64122295379639\n",
      "Epoch [80/500], Loss: 90.8601, Accuracy: 21.18 %\n",
      "Testing Accuracy: 25.41 %\n",
      "Test Loss:  89.08490145206451\n",
      "Epoch [81/500], Loss: 91.0404, Accuracy: 21.17 %\n",
      "Testing Accuracy: 24.32 %\n",
      "Test Loss:  89.08302354812622\n",
      "Epoch [82/500], Loss: 90.9888, Accuracy: 21.14 %\n",
      "Testing Accuracy: 24.94 %\n",
      "Test Loss:  88.30406928062439\n",
      "Epoch [83/500], Loss: 90.8771, Accuracy: 21.44 %\n",
      "Testing Accuracy: 25.06 %\n",
      "Test Loss:  88.34478914737701\n",
      "Epoch [84/500], Loss: 90.7928, Accuracy: 21.20 %\n",
      "Testing Accuracy: 25.31 %\n",
      "Test Loss:  88.60047590732574\n",
      "Epoch [85/500], Loss: 90.8373, Accuracy: 21.08 %\n",
      "Testing Accuracy: 24.98 %\n",
      "Test Loss:  88.25805652141571\n",
      "Epoch [86/500], Loss: 90.8634, Accuracy: 21.07 %\n",
      "Testing Accuracy: 25.62 %\n",
      "Test Loss:  88.3028393983841\n",
      "Epoch [87/500], Loss: 90.8887, Accuracy: 21.09 %\n",
      "Testing Accuracy: 25.18 %\n",
      "Test Loss:  88.19760489463806\n",
      "Epoch [88/500], Loss: 90.8261, Accuracy: 21.22 %\n",
      "Testing Accuracy: 25.45 %\n",
      "Test Loss:  88.69167709350586\n",
      "Epoch [89/500], Loss: 90.7233, Accuracy: 21.09 %\n",
      "Testing Accuracy: 25.56 %\n",
      "Test Loss:  88.13544809818268\n",
      "Epoch [90/500], Loss: 90.9309, Accuracy: 20.87 %\n",
      "Testing Accuracy: 25.50 %\n",
      "Test Loss:  88.97732663154602\n",
      "Epoch [91/500], Loss: 90.5823, Accuracy: 21.30 %\n",
      "Testing Accuracy: 25.20 %\n",
      "Test Loss:  88.09989023208618\n",
      "Epoch [92/500], Loss: 90.6867, Accuracy: 21.27 %\n",
      "Testing Accuracy: 25.56 %\n",
      "Test Loss:  88.3909832239151\n",
      "Epoch [93/500], Loss: 90.7905, Accuracy: 20.99 %\n",
      "Testing Accuracy: 25.28 %\n",
      "Test Loss:  88.11765968799591\n",
      "Epoch [94/500], Loss: 90.6818, Accuracy: 21.20 %\n",
      "Testing Accuracy: 25.44 %\n",
      "Test Loss:  88.65017795562744\n",
      "Epoch [95/500], Loss: 90.7110, Accuracy: 21.19 %\n",
      "Testing Accuracy: 25.75 %\n",
      "Test Loss:  88.61117899417877\n",
      "Epoch [96/500], Loss: 90.7705, Accuracy: 20.99 %\n",
      "Testing Accuracy: 25.71 %\n",
      "Test Loss:  88.71526980400085\n",
      "Epoch [97/500], Loss: 90.5976, Accuracy: 21.18 %\n",
      "Testing Accuracy: 25.31 %\n",
      "Test Loss:  88.01411426067352\n",
      "Epoch [98/500], Loss: 90.9504, Accuracy: 21.23 %\n",
      "Testing Accuracy: 24.92 %\n",
      "Test Loss:  87.89432072639465\n",
      "Epoch [99/500], Loss: 90.8405, Accuracy: 21.27 %\n",
      "Testing Accuracy: 25.49 %\n",
      "Test Loss:  88.18147373199463\n",
      "Epoch [100/500], Loss: 90.4637, Accuracy: 21.07 %\n",
      "Testing Accuracy: 25.08 %\n",
      "Test Loss:  87.8428384065628\n",
      "Epoch [101/500], Loss: 90.6130, Accuracy: 21.05 %\n",
      "Testing Accuracy: 25.45 %\n",
      "Test Loss:  87.93944799900055\n",
      "Epoch [102/500], Loss: 90.5306, Accuracy: 21.20 %\n",
      "Testing Accuracy: 25.27 %\n",
      "Test Loss:  89.17254996299744\n",
      "Epoch [103/500], Loss: 90.8483, Accuracy: 21.14 %\n",
      "Testing Accuracy: 25.56 %\n",
      "Test Loss:  88.49092447757721\n",
      "Epoch [104/500], Loss: 90.7135, Accuracy: 21.34 %\n",
      "Testing Accuracy: 25.54 %\n",
      "Test Loss:  88.37553012371063\n",
      "Epoch [105/500], Loss: 90.5710, Accuracy: 21.14 %\n",
      "Testing Accuracy: 24.13 %\n",
      "Test Loss:  88.43765795230865\n",
      "Epoch [106/500], Loss: 90.3573, Accuracy: 21.36 %\n",
      "Testing Accuracy: 24.73 %\n",
      "Test Loss:  87.77994346618652\n",
      "Epoch [107/500], Loss: 90.3838, Accuracy: 21.39 %\n",
      "Testing Accuracy: 25.60 %\n",
      "Test Loss:  88.23006284236908\n",
      "Epoch [108/500], Loss: 90.3253, Accuracy: 21.35 %\n",
      "Testing Accuracy: 24.71 %\n",
      "Test Loss:  87.8183434009552\n",
      "Epoch [109/500], Loss: 90.5234, Accuracy: 21.07 %\n",
      "Testing Accuracy: 25.29 %\n",
      "Test Loss:  87.63083589076996\n",
      "Epoch [110/500], Loss: 90.5450, Accuracy: 21.16 %\n",
      "Testing Accuracy: 24.94 %\n",
      "Test Loss:  87.6766551733017\n",
      "Epoch [111/500], Loss: 90.3830, Accuracy: 21.29 %\n",
      "Testing Accuracy: 25.58 %\n",
      "Test Loss:  88.79323267936707\n",
      "Epoch [112/500], Loss: 90.3976, Accuracy: 21.22 %\n",
      "Testing Accuracy: 24.81 %\n",
      "Test Loss:  89.6920838356018\n",
      "Epoch [113/500], Loss: 90.9004, Accuracy: 21.00 %\n",
      "Testing Accuracy: 25.31 %\n",
      "Test Loss:  88.50756311416626\n",
      "Epoch [114/500], Loss: 90.2087, Accuracy: 21.46 %\n",
      "Testing Accuracy: 25.58 %\n",
      "Test Loss:  87.9231927394867\n",
      "Epoch [115/500], Loss: 90.1418, Accuracy: 21.43 %\n",
      "Testing Accuracy: 25.80 %\n",
      "Test Loss:  87.92653334140778\n",
      "Epoch [116/500], Loss: 90.1851, Accuracy: 21.33 %\n",
      "Testing Accuracy: 25.19 %\n",
      "Test Loss:  87.4798264503479\n",
      "Epoch [117/500], Loss: 90.1612, Accuracy: 21.55 %\n",
      "Testing Accuracy: 25.34 %\n",
      "Test Loss:  87.41550242900848\n",
      "Epoch [118/500], Loss: 90.2036, Accuracy: 21.35 %\n",
      "Testing Accuracy: 25.62 %\n",
      "Test Loss:  87.97378277778625\n",
      "Epoch [119/500], Loss: 90.7347, Accuracy: 21.23 %\n",
      "Testing Accuracy: 25.17 %\n",
      "Test Loss:  87.35015666484833\n",
      "Epoch [120/500], Loss: 90.2550, Accuracy: 21.20 %\n",
      "Testing Accuracy: 25.94 %\n",
      "Test Loss:  87.55458676815033\n",
      "Epoch [121/500], Loss: 90.0309, Accuracy: 21.55 %\n",
      "Testing Accuracy: 25.46 %\n",
      "Test Loss:  87.38903939723969\n",
      "Epoch [122/500], Loss: 90.0149, Accuracy: 21.63 %\n",
      "Testing Accuracy: 25.64 %\n",
      "Test Loss:  87.48380398750305\n",
      "Epoch [123/500], Loss: 90.2790, Accuracy: 21.39 %\n",
      "Testing Accuracy: 25.49 %\n",
      "Test Loss:  87.35508942604065\n",
      "Epoch [124/500], Loss: 90.0252, Accuracy: 21.47 %\n",
      "Testing Accuracy: 25.63 %\n",
      "Test Loss:  88.49972355365753\n",
      "Epoch [125/500], Loss: 90.2765, Accuracy: 21.42 %\n",
      "Testing Accuracy: 25.67 %\n",
      "Test Loss:  87.4536806344986\n",
      "Epoch [126/500], Loss: 90.0952, Accuracy: 21.27 %\n",
      "Testing Accuracy: 25.27 %\n",
      "Test Loss:  89.12861502170563\n",
      "Epoch [127/500], Loss: 90.3875, Accuracy: 21.41 %\n",
      "Testing Accuracy: 25.58 %\n",
      "Test Loss:  87.98806250095367\n",
      "Epoch [128/500], Loss: 90.0203, Accuracy: 21.33 %\n",
      "Testing Accuracy: 24.99 %\n",
      "Test Loss:  87.32784461975098\n",
      "Epoch [129/500], Loss: 90.1060, Accuracy: 21.44 %\n",
      "Testing Accuracy: 25.51 %\n",
      "Test Loss:  87.13095510005951\n",
      "Epoch [130/500], Loss: 90.7981, Accuracy: 21.13 %\n",
      "Testing Accuracy: 24.88 %\n",
      "Test Loss:  89.27613294124603\n",
      "Epoch [131/500], Loss: 90.2221, Accuracy: 21.46 %\n",
      "Testing Accuracy: 24.75 %\n",
      "Test Loss:  89.64359736442566\n",
      "Epoch [132/500], Loss: 90.2025, Accuracy: 21.36 %\n",
      "Testing Accuracy: 25.13 %\n",
      "Test Loss:  89.22532975673676\n",
      "Epoch [133/500], Loss: 90.0665, Accuracy: 21.57 %\n",
      "Testing Accuracy: 25.53 %\n",
      "Test Loss:  87.9666041135788\n",
      "Epoch [134/500], Loss: 89.9376, Accuracy: 21.48 %\n",
      "Testing Accuracy: 25.58 %\n",
      "Test Loss:  87.85493695735931\n",
      "Epoch [135/500], Loss: 90.0855, Accuracy: 21.54 %\n",
      "Testing Accuracy: 25.64 %\n",
      "Test Loss:  87.7403175830841\n",
      "Epoch [136/500], Loss: 89.8381, Accuracy: 21.62 %\n",
      "Testing Accuracy: 25.98 %\n",
      "Test Loss:  87.35601103305817\n",
      "Epoch [137/500], Loss: 89.8736, Accuracy: 21.75 %\n",
      "Testing Accuracy: 25.47 %\n",
      "Test Loss:  86.96891069412231\n",
      "Epoch [138/500], Loss: 89.9922, Accuracy: 21.54 %\n",
      "Testing Accuracy: 24.06 %\n",
      "Test Loss:  90.40403079986572\n",
      "Epoch [139/500], Loss: 90.0580, Accuracy: 21.38 %\n",
      "Testing Accuracy: 25.27 %\n",
      "Test Loss:  86.97130286693573\n",
      "Epoch [140/500], Loss: 90.0644, Accuracy: 21.38 %\n",
      "Testing Accuracy: 25.45 %\n",
      "Test Loss:  86.92896938323975\n",
      "Epoch [141/500], Loss: 90.0565, Accuracy: 21.23 %\n",
      "Testing Accuracy: 24.80 %\n",
      "Test Loss:  89.42113029956818\n",
      "Epoch [142/500], Loss: 90.2559, Accuracy: 21.48 %\n",
      "Testing Accuracy: 25.26 %\n",
      "Test Loss:  88.69172036647797\n",
      "Epoch [143/500], Loss: 89.9973, Accuracy: 21.33 %\n",
      "Testing Accuracy: 25.47 %\n",
      "Test Loss:  88.39207577705383\n",
      "Epoch [144/500], Loss: 90.0302, Accuracy: 21.40 %\n",
      "Testing Accuracy: 25.78 %\n",
      "Test Loss:  87.24078440666199\n",
      "Epoch [145/500], Loss: 90.0455, Accuracy: 21.52 %\n",
      "Testing Accuracy: 25.64 %\n",
      "Test Loss:  87.56830286979675\n",
      "Epoch [146/500], Loss: 90.0606, Accuracy: 21.49 %\n",
      "Testing Accuracy: 24.63 %\n",
      "Test Loss:  87.35148060321808\n",
      "Epoch [147/500], Loss: 90.0112, Accuracy: 21.27 %\n",
      "Testing Accuracy: 25.21 %\n",
      "Test Loss:  86.94346296787262\n",
      "Epoch [148/500], Loss: 89.7628, Accuracy: 21.52 %\n",
      "Testing Accuracy: 25.45 %\n",
      "Test Loss:  86.85525870323181\n",
      "Epoch [149/500], Loss: 89.6540, Accuracy: 21.66 %\n",
      "Testing Accuracy: 25.53 %\n",
      "Test Loss:  88.26870381832123\n",
      "Epoch [150/500], Loss: 89.7538, Accuracy: 21.65 %\n",
      "Testing Accuracy: 25.08 %\n",
      "Test Loss:  89.05855000019073\n",
      "Epoch [151/500], Loss: 90.1066, Accuracy: 21.50 %\n",
      "Testing Accuracy: 25.12 %\n",
      "Test Loss:  86.9992743730545\n",
      "Epoch [152/500], Loss: 90.0304, Accuracy: 21.49 %\n",
      "Testing Accuracy: 25.74 %\n",
      "Test Loss:  87.27322769165039\n",
      "Epoch [153/500], Loss: 89.5231, Accuracy: 21.67 %\n",
      "Testing Accuracy: 24.48 %\n",
      "Test Loss:  87.26151180267334\n",
      "Epoch [154/500], Loss: 89.7662, Accuracy: 21.64 %\n",
      "Testing Accuracy: 25.79 %\n",
      "Test Loss:  86.51367902755737\n",
      "Epoch [155/500], Loss: 89.4791, Accuracy: 22.14 %\n",
      "Testing Accuracy: 25.34 %\n",
      "Test Loss:  86.88038742542267\n",
      "Epoch [156/500], Loss: 90.3057, Accuracy: 22.46 %\n",
      "Testing Accuracy: 27.06 %\n",
      "Test Loss:  86.39624011516571\n",
      "Epoch [157/500], Loss: 89.4139, Accuracy: 23.80 %\n",
      "Testing Accuracy: 27.16 %\n",
      "Test Loss:  87.59953272342682\n",
      "Epoch [158/500], Loss: 88.8223, Accuracy: 24.32 %\n",
      "Testing Accuracy: 27.79 %\n",
      "Test Loss:  86.69005537033081\n",
      "Epoch [159/500], Loss: 88.3577, Accuracy: 24.50 %\n",
      "Testing Accuracy: 28.61 %\n",
      "Test Loss:  86.21649992465973\n",
      "Epoch [160/500], Loss: 88.6947, Accuracy: 24.14 %\n",
      "Testing Accuracy: 28.69 %\n",
      "Test Loss:  85.43619847297668\n",
      "Epoch [161/500], Loss: 88.2872, Accuracy: 24.79 %\n",
      "Testing Accuracy: 28.69 %\n",
      "Test Loss:  85.36644351482391\n",
      "Epoch [162/500], Loss: 88.1869, Accuracy: 24.76 %\n",
      "Testing Accuracy: 28.68 %\n",
      "Test Loss:  84.9433958530426\n",
      "Epoch [163/500], Loss: 88.2279, Accuracy: 24.88 %\n",
      "Testing Accuracy: 28.93 %\n",
      "Test Loss:  85.11922574043274\n",
      "Epoch [164/500], Loss: 87.9533, Accuracy: 25.20 %\n",
      "Testing Accuracy: 29.20 %\n",
      "Test Loss:  84.63992321491241\n",
      "Epoch [165/500], Loss: 88.5401, Accuracy: 24.91 %\n",
      "Testing Accuracy: 28.36 %\n",
      "Test Loss:  85.49582898616791\n",
      "Epoch [166/500], Loss: 88.2492, Accuracy: 25.12 %\n",
      "Testing Accuracy: 27.11 %\n",
      "Test Loss:  88.55549240112305\n",
      "Epoch [167/500], Loss: 88.1361, Accuracy: 25.17 %\n",
      "Testing Accuracy: 30.12 %\n",
      "Test Loss:  84.49362623691559\n",
      "Epoch [168/500], Loss: 87.9433, Accuracy: 25.45 %\n",
      "Testing Accuracy: 30.35 %\n",
      "Test Loss:  84.51618254184723\n",
      "Epoch [169/500], Loss: 87.4525, Accuracy: 26.03 %\n",
      "Testing Accuracy: 30.00 %\n",
      "Test Loss:  84.1965537071228\n",
      "Epoch [170/500], Loss: 87.4502, Accuracy: 25.99 %\n",
      "Testing Accuracy: 29.92 %\n",
      "Test Loss:  86.01156306266785\n",
      "Epoch [171/500], Loss: 87.4422, Accuracy: 25.79 %\n",
      "Testing Accuracy: 29.65 %\n",
      "Test Loss:  84.07679629325867\n",
      "Epoch [172/500], Loss: 87.1669, Accuracy: 26.20 %\n",
      "Testing Accuracy: 29.25 %\n",
      "Test Loss:  87.07734417915344\n",
      "Epoch [173/500], Loss: 87.5840, Accuracy: 26.09 %\n",
      "Testing Accuracy: 30.30 %\n",
      "Test Loss:  83.82058954238892\n",
      "Epoch [174/500], Loss: 86.7979, Accuracy: 26.58 %\n",
      "Testing Accuracy: 28.91 %\n",
      "Test Loss:  85.05713939666748\n",
      "Epoch [175/500], Loss: 87.2482, Accuracy: 26.39 %\n",
      "Testing Accuracy: 30.78 %\n",
      "Test Loss:  83.25921821594238\n",
      "Epoch [176/500], Loss: 87.9315, Accuracy: 25.99 %\n",
      "Testing Accuracy: 29.90 %\n",
      "Test Loss:  84.36041378974915\n",
      "Epoch [177/500], Loss: 86.8902, Accuracy: 26.62 %\n",
      "Testing Accuracy: 30.95 %\n",
      "Test Loss:  82.78411602973938\n",
      "Epoch [178/500], Loss: 86.3215, Accuracy: 27.00 %\n",
      "Testing Accuracy: 30.25 %\n",
      "Test Loss:  84.75481152534485\n",
      "Epoch [179/500], Loss: 87.5416, Accuracy: 26.85 %\n",
      "Testing Accuracy: 29.84 %\n",
      "Test Loss:  83.4742591381073\n",
      "Epoch [180/500], Loss: 85.9148, Accuracy: 27.73 %\n",
      "Testing Accuracy: 31.25 %\n",
      "Test Loss:  82.40778636932373\n",
      "Epoch [181/500], Loss: 85.7593, Accuracy: 27.73 %\n",
      "Testing Accuracy: 31.36 %\n",
      "Test Loss:  82.30162024497986\n",
      "Epoch [182/500], Loss: 86.0338, Accuracy: 27.71 %\n",
      "Testing Accuracy: 31.11 %\n",
      "Test Loss:  82.97644364833832\n",
      "Epoch [183/500], Loss: 85.3459, Accuracy: 27.86 %\n",
      "Testing Accuracy: 30.29 %\n",
      "Test Loss:  82.57866775989532\n",
      "Epoch [184/500], Loss: 85.7706, Accuracy: 27.90 %\n",
      "Testing Accuracy: 30.72 %\n",
      "Test Loss:  82.14880514144897\n",
      "Epoch [185/500], Loss: 85.3106, Accuracy: 27.94 %\n",
      "Testing Accuracy: 30.94 %\n",
      "Test Loss:  82.4804116487503\n",
      "Epoch [186/500], Loss: 85.0245, Accuracy: 28.25 %\n",
      "Testing Accuracy: 31.25 %\n",
      "Test Loss:  81.6858880519867\n",
      "Epoch [187/500], Loss: 84.8643, Accuracy: 28.18 %\n",
      "Testing Accuracy: 30.51 %\n",
      "Test Loss:  84.6598230600357\n",
      "Epoch [188/500], Loss: 85.5170, Accuracy: 28.06 %\n",
      "Testing Accuracy: 30.72 %\n",
      "Test Loss:  81.865229845047\n",
      "Epoch [189/500], Loss: 85.4414, Accuracy: 28.08 %\n",
      "Testing Accuracy: 30.88 %\n",
      "Test Loss:  82.70233941078186\n",
      "Epoch [190/500], Loss: 84.6566, Accuracy: 28.37 %\n",
      "Testing Accuracy: 29.93 %\n",
      "Test Loss:  82.55880439281464\n",
      "Epoch [191/500], Loss: 84.7165, Accuracy: 28.33 %\n",
      "Testing Accuracy: 30.97 %\n",
      "Test Loss:  81.41463482379913\n",
      "Epoch [192/500], Loss: 84.6013, Accuracy: 28.65 %\n",
      "Testing Accuracy: 31.53 %\n",
      "Test Loss:  81.22743451595306\n",
      "Epoch [193/500], Loss: 84.4289, Accuracy: 28.65 %\n",
      "Testing Accuracy: 31.14 %\n",
      "Test Loss:  81.5017808675766\n",
      "Epoch [194/500], Loss: 84.5561, Accuracy: 28.82 %\n",
      "Testing Accuracy: 31.20 %\n",
      "Test Loss:  81.46218001842499\n",
      "Epoch [195/500], Loss: 84.9736, Accuracy: 28.49 %\n",
      "Testing Accuracy: 29.69 %\n",
      "Test Loss:  83.4832513332367\n",
      "Epoch [196/500], Loss: 84.8760, Accuracy: 28.45 %\n",
      "Testing Accuracy: 30.33 %\n",
      "Test Loss:  84.86832368373871\n",
      "Epoch [197/500], Loss: 84.8892, Accuracy: 28.34 %\n",
      "Testing Accuracy: 30.91 %\n",
      "Test Loss:  81.42536473274231\n",
      "Epoch [198/500], Loss: 84.5125, Accuracy: 28.71 %\n",
      "Testing Accuracy: 31.01 %\n",
      "Test Loss:  81.43659210205078\n",
      "Epoch [199/500], Loss: 84.6422, Accuracy: 28.65 %\n",
      "Testing Accuracy: 31.66 %\n",
      "Test Loss:  81.20080840587616\n",
      "Epoch [200/500], Loss: 84.3212, Accuracy: 28.66 %\n",
      "Testing Accuracy: 29.44 %\n",
      "Test Loss:  81.98379778862\n",
      "Epoch [201/500], Loss: 84.1816, Accuracy: 28.86 %\n",
      "Testing Accuracy: 30.53 %\n",
      "Test Loss:  82.14018094539642\n",
      "Epoch [202/500], Loss: 84.3875, Accuracy: 28.65 %\n",
      "Testing Accuracy: 31.44 %\n",
      "Test Loss:  81.97432708740234\n",
      "Epoch [203/500], Loss: 84.4941, Accuracy: 28.62 %\n",
      "Testing Accuracy: 31.09 %\n",
      "Test Loss:  81.13418662548065\n",
      "Epoch [204/500], Loss: 84.0457, Accuracy: 29.00 %\n",
      "Testing Accuracy: 30.95 %\n",
      "Test Loss:  81.56655442714691\n",
      "Epoch [205/500], Loss: 84.1673, Accuracy: 28.76 %\n",
      "Testing Accuracy: 30.53 %\n",
      "Test Loss:  82.34887611865997\n",
      "Epoch [206/500], Loss: 84.2071, Accuracy: 28.43 %\n",
      "Testing Accuracy: 31.57 %\n",
      "Test Loss:  81.77198624610901\n",
      "Epoch [207/500], Loss: 84.1845, Accuracy: 28.82 %\n",
      "Testing Accuracy: 30.87 %\n",
      "Test Loss:  81.328808426857\n",
      "Epoch [208/500], Loss: 84.0933, Accuracy: 28.97 %\n",
      "Testing Accuracy: 30.50 %\n",
      "Test Loss:  81.9697870016098\n",
      "Epoch [209/500], Loss: 84.0463, Accuracy: 29.01 %\n",
      "Testing Accuracy: 31.22 %\n",
      "Test Loss:  83.26089155673981\n",
      "Epoch [210/500], Loss: 84.3069, Accuracy: 28.92 %\n",
      "Testing Accuracy: 31.83 %\n",
      "Test Loss:  81.1630607843399\n",
      "Epoch [211/500], Loss: 84.3958, Accuracy: 28.79 %\n",
      "Testing Accuracy: 28.82 %\n",
      "Test Loss:  81.97576224803925\n",
      "Epoch [212/500], Loss: 83.9178, Accuracy: 29.10 %\n",
      "Testing Accuracy: 31.73 %\n",
      "Test Loss:  80.48188197612762\n",
      "Epoch [213/500], Loss: 84.0397, Accuracy: 29.13 %\n",
      "Testing Accuracy: 32.04 %\n",
      "Test Loss:  80.50283360481262\n",
      "Epoch [214/500], Loss: 83.8008, Accuracy: 29.26 %\n",
      "Testing Accuracy: 31.90 %\n",
      "Test Loss:  80.2944016456604\n",
      "Epoch [215/500], Loss: 84.0297, Accuracy: 28.92 %\n",
      "Testing Accuracy: 31.76 %\n",
      "Test Loss:  80.76413083076477\n",
      "Epoch [216/500], Loss: 83.7337, Accuracy: 29.18 %\n",
      "Testing Accuracy: 32.02 %\n",
      "Test Loss:  80.43111491203308\n",
      "Epoch [217/500], Loss: 83.6621, Accuracy: 29.23 %\n",
      "Testing Accuracy: 31.68 %\n",
      "Test Loss:  81.99020683765411\n",
      "Epoch [218/500], Loss: 83.6782, Accuracy: 29.25 %\n",
      "Testing Accuracy: 31.28 %\n",
      "Test Loss:  81.26555407047272\n",
      "Epoch [219/500], Loss: 83.9695, Accuracy: 29.17 %\n",
      "Testing Accuracy: 32.03 %\n",
      "Test Loss:  80.66136395931244\n",
      "Epoch [220/500], Loss: 83.7598, Accuracy: 29.42 %\n",
      "Testing Accuracy: 32.29 %\n",
      "Test Loss:  80.47365939617157\n",
      "Epoch [221/500], Loss: 83.3378, Accuracy: 29.61 %\n",
      "Testing Accuracy: 31.33 %\n",
      "Test Loss:  81.55223500728607\n",
      "Epoch [222/500], Loss: 83.3719, Accuracy: 29.49 %\n",
      "Testing Accuracy: 32.10 %\n",
      "Test Loss:  81.20904564857483\n",
      "Epoch [223/500], Loss: 83.8010, Accuracy: 29.47 %\n",
      "Testing Accuracy: 32.21 %\n",
      "Test Loss:  80.68271827697754\n",
      "Epoch [224/500], Loss: 84.4659, Accuracy: 28.74 %\n",
      "Testing Accuracy: 29.80 %\n",
      "Test Loss:  82.36559844017029\n",
      "Epoch [225/500], Loss: 83.9046, Accuracy: 28.95 %\n",
      "Testing Accuracy: 32.16 %\n",
      "Test Loss:  81.24305713176727\n",
      "Epoch [226/500], Loss: 83.4331, Accuracy: 29.50 %\n",
      "Testing Accuracy: 30.24 %\n",
      "Test Loss:  81.80685818195343\n",
      "Epoch [227/500], Loss: 83.3378, Accuracy: 29.43 %\n",
      "Testing Accuracy: 32.32 %\n",
      "Test Loss:  80.2244313955307\n",
      "Epoch [228/500], Loss: 83.2012, Accuracy: 29.72 %\n",
      "Testing Accuracy: 31.87 %\n",
      "Test Loss:  80.50399374961853\n",
      "Epoch [229/500], Loss: 83.4782, Accuracy: 29.40 %\n",
      "Testing Accuracy: 32.53 %\n",
      "Test Loss:  81.18357181549072\n",
      "Epoch [230/500], Loss: 83.2324, Accuracy: 29.66 %\n",
      "Testing Accuracy: 32.43 %\n",
      "Test Loss:  80.17152035236359\n",
      "Epoch [231/500], Loss: 83.3758, Accuracy: 29.42 %\n",
      "Testing Accuracy: 32.07 %\n",
      "Test Loss:  80.11836898326874\n",
      "Epoch [232/500], Loss: 83.6987, Accuracy: 29.24 %\n",
      "Testing Accuracy: 30.54 %\n",
      "Test Loss:  82.1062080860138\n",
      "Epoch [233/500], Loss: 83.4200, Accuracy: 29.80 %\n",
      "Testing Accuracy: 31.93 %\n",
      "Test Loss:  80.41352033615112\n",
      "Epoch [234/500], Loss: 83.5034, Accuracy: 28.98 %\n",
      "Testing Accuracy: 31.38 %\n",
      "Test Loss:  80.33729231357574\n",
      "Epoch [235/500], Loss: 83.2915, Accuracy: 29.84 %\n",
      "Testing Accuracy: 30.23 %\n",
      "Test Loss:  83.4666599035263\n",
      "Epoch [236/500], Loss: 83.4814, Accuracy: 29.64 %\n",
      "Testing Accuracy: 32.41 %\n",
      "Test Loss:  80.61689078807831\n",
      "Epoch [237/500], Loss: 83.3318, Accuracy: 29.55 %\n",
      "Testing Accuracy: 31.63 %\n",
      "Test Loss:  80.54222309589386\n",
      "Epoch [238/500], Loss: 82.9703, Accuracy: 29.92 %\n",
      "Testing Accuracy: 32.35 %\n",
      "Test Loss:  81.3148502111435\n",
      "Epoch [239/500], Loss: 83.3550, Accuracy: 29.26 %\n",
      "Testing Accuracy: 32.14 %\n",
      "Test Loss:  80.34082925319672\n",
      "Epoch [240/500], Loss: 83.2474, Accuracy: 29.60 %\n",
      "Testing Accuracy: 32.93 %\n",
      "Test Loss:  79.98152339458466\n",
      "Epoch [241/500], Loss: 83.1461, Accuracy: 29.45 %\n",
      "Testing Accuracy: 32.63 %\n",
      "Test Loss:  82.43655610084534\n",
      "Epoch [242/500], Loss: 83.5456, Accuracy: 29.14 %\n",
      "Testing Accuracy: 31.85 %\n",
      "Test Loss:  81.09667754173279\n",
      "Epoch [243/500], Loss: 82.8979, Accuracy: 29.55 %\n",
      "Testing Accuracy: 31.96 %\n",
      "Test Loss:  82.1936423778534\n",
      "Epoch [244/500], Loss: 83.2069, Accuracy: 29.39 %\n",
      "Testing Accuracy: 33.05 %\n",
      "Test Loss:  80.0463570356369\n",
      "Epoch [245/500], Loss: 82.8517, Accuracy: 29.86 %\n",
      "Testing Accuracy: 32.96 %\n",
      "Test Loss:  79.9130870103836\n",
      "Epoch [246/500], Loss: 82.9733, Accuracy: 29.78 %\n",
      "Testing Accuracy: 33.15 %\n",
      "Test Loss:  80.33312094211578\n",
      "Epoch [247/500], Loss: 82.8665, Accuracy: 29.62 %\n",
      "Testing Accuracy: 33.23 %\n",
      "Test Loss:  79.5940432548523\n",
      "Epoch [248/500], Loss: 82.8360, Accuracy: 29.78 %\n",
      "Testing Accuracy: 32.06 %\n",
      "Test Loss:  80.56004512310028\n",
      "Epoch [249/500], Loss: 83.1025, Accuracy: 29.38 %\n",
      "Testing Accuracy: 32.87 %\n",
      "Test Loss:  79.91983222961426\n",
      "Epoch [250/500], Loss: 82.9141, Accuracy: 29.46 %\n",
      "Testing Accuracy: 33.53 %\n",
      "Test Loss:  80.20927262306213\n",
      "Epoch [251/500], Loss: 82.7478, Accuracy: 29.59 %\n",
      "Testing Accuracy: 31.40 %\n",
      "Test Loss:  83.4764209985733\n",
      "Epoch [252/500], Loss: 83.4213, Accuracy: 29.37 %\n",
      "Testing Accuracy: 32.45 %\n",
      "Test Loss:  83.31979250907898\n",
      "Epoch [253/500], Loss: 83.1960, Accuracy: 29.89 %\n",
      "Testing Accuracy: 31.05 %\n",
      "Test Loss:  80.18040561676025\n",
      "Epoch [254/500], Loss: 83.2174, Accuracy: 29.83 %\n",
      "Testing Accuracy: 33.54 %\n",
      "Test Loss:  79.90979254245758\n",
      "Epoch [255/500], Loss: 82.7096, Accuracy: 29.82 %\n",
      "Testing Accuracy: 31.85 %\n",
      "Test Loss:  80.04920554161072\n",
      "Epoch [256/500], Loss: 82.5875, Accuracy: 30.03 %\n",
      "Testing Accuracy: 33.56 %\n",
      "Test Loss:  79.52053642272949\n",
      "Epoch [257/500], Loss: 82.8765, Accuracy: 29.78 %\n",
      "Testing Accuracy: 32.83 %\n",
      "Test Loss:  79.55831944942474\n",
      "Epoch [258/500], Loss: 83.3486, Accuracy: 29.68 %\n",
      "Testing Accuracy: 31.85 %\n",
      "Test Loss:  81.44084286689758\n",
      "Epoch [259/500], Loss: 83.1807, Accuracy: 29.76 %\n",
      "Testing Accuracy: 33.67 %\n",
      "Test Loss:  79.54992163181305\n",
      "Epoch [260/500], Loss: 82.5215, Accuracy: 30.16 %\n",
      "Testing Accuracy: 33.23 %\n",
      "Test Loss:  79.35853922367096\n",
      "Epoch [261/500], Loss: 82.6887, Accuracy: 29.73 %\n",
      "Testing Accuracy: 33.46 %\n",
      "Test Loss:  79.27108061313629\n",
      "Epoch [262/500], Loss: 82.5443, Accuracy: 30.15 %\n",
      "Testing Accuracy: 32.37 %\n",
      "Test Loss:  81.54477596282959\n",
      "Epoch [263/500], Loss: 83.0310, Accuracy: 29.85 %\n",
      "Testing Accuracy: 32.25 %\n",
      "Test Loss:  79.8181961774826\n",
      "Epoch [264/500], Loss: 82.5289, Accuracy: 30.17 %\n",
      "Testing Accuracy: 33.16 %\n",
      "Test Loss:  79.75501871109009\n",
      "Epoch [265/500], Loss: 82.9862, Accuracy: 29.40 %\n",
      "Testing Accuracy: 33.69 %\n",
      "Test Loss:  79.41491532325745\n",
      "Epoch [266/500], Loss: 82.6167, Accuracy: 29.61 %\n",
      "Testing Accuracy: 33.05 %\n",
      "Test Loss:  79.83734822273254\n",
      "Epoch [267/500], Loss: 82.8639, Accuracy: 30.05 %\n",
      "Testing Accuracy: 33.17 %\n",
      "Test Loss:  79.47508263587952\n",
      "Epoch [268/500], Loss: 82.5392, Accuracy: 30.08 %\n",
      "Testing Accuracy: 33.71 %\n",
      "Test Loss:  79.50146305561066\n",
      "Epoch [269/500], Loss: 82.9859, Accuracy: 29.99 %\n",
      "Testing Accuracy: 31.07 %\n",
      "Test Loss:  81.08797788619995\n",
      "Epoch [270/500], Loss: 83.0349, Accuracy: 29.90 %\n",
      "Testing Accuracy: 32.49 %\n",
      "Test Loss:  79.6210069656372\n",
      "Epoch [271/500], Loss: 82.4170, Accuracy: 30.05 %\n",
      "Testing Accuracy: 33.41 %\n",
      "Test Loss:  79.48104894161224\n",
      "Epoch [272/500], Loss: 82.4704, Accuracy: 30.31 %\n",
      "Testing Accuracy: 33.74 %\n",
      "Test Loss:  80.64000070095062\n",
      "Epoch [273/500], Loss: 82.2556, Accuracy: 30.33 %\n",
      "Testing Accuracy: 33.71 %\n",
      "Test Loss:  80.20761859416962\n",
      "Epoch [274/500], Loss: 82.2331, Accuracy: 30.58 %\n",
      "Testing Accuracy: 33.83 %\n",
      "Test Loss:  79.85806560516357\n",
      "Epoch [275/500], Loss: 82.2219, Accuracy: 30.10 %\n",
      "Testing Accuracy: 32.82 %\n",
      "Test Loss:  80.55973434448242\n",
      "Epoch [276/500], Loss: 82.8280, Accuracy: 30.03 %\n",
      "Testing Accuracy: 33.67 %\n",
      "Test Loss:  81.19640958309174\n",
      "Epoch [277/500], Loss: 83.0965, Accuracy: 29.62 %\n",
      "Testing Accuracy: 31.48 %\n",
      "Test Loss:  80.95229887962341\n",
      "Epoch [278/500], Loss: 82.3225, Accuracy: 30.35 %\n",
      "Testing Accuracy: 33.89 %\n",
      "Test Loss:  79.35906994342804\n",
      "Epoch [279/500], Loss: 82.3012, Accuracy: 30.20 %\n",
      "Testing Accuracy: 33.65 %\n",
      "Test Loss:  79.75055587291718\n",
      "Epoch [280/500], Loss: 82.2160, Accuracy: 30.42 %\n",
      "Testing Accuracy: 32.89 %\n",
      "Test Loss:  79.89561581611633\n",
      "Epoch [281/500], Loss: 82.3283, Accuracy: 30.40 %\n",
      "Testing Accuracy: 33.76 %\n",
      "Test Loss:  79.98450100421906\n",
      "Testing Accuracy: 33.76 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        612352   \n",
      "Net/Dropout[dropout]/onnx::Relu   598      \n",
      "Net/Linear[fc2]/onnx::Gemm        3289     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "616,239 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746, 37.04266279223352, 34.024567428345, 29.863954563465857, 26.10619469026549, 33.7604015321622]\n",
      "pruning hidden size:  299\n",
      "with hidden layer:  299\n",
      "removing:  (270, 72, 62)\n",
      "--- 27.71276617050171 seconds ---\n",
      "Epoch [1/500], Loss: 87.4826, Accuracy: 28.39 %\n",
      "Testing Accuracy: 32.23 %\n",
      "Test Loss:  84.82887089252472\n",
      "Epoch [2/500], Loss: 86.4850, Accuracy: 29.57 %\n",
      "Testing Accuracy: 32.33 %\n",
      "Test Loss:  83.3828558921814\n",
      "Epoch [3/500], Loss: 86.0585, Accuracy: 29.57 %\n",
      "Testing Accuracy: 30.12 %\n",
      "Test Loss:  84.66761076450348\n",
      "Epoch [4/500], Loss: 85.7505, Accuracy: 29.63 %\n",
      "Testing Accuracy: 29.75 %\n",
      "Test Loss:  84.43997061252594\n",
      "Epoch [5/500], Loss: 85.7872, Accuracy: 29.31 %\n",
      "Testing Accuracy: 28.97 %\n",
      "Test Loss:  84.68372297286987\n",
      "Epoch [6/500], Loss: 85.9290, Accuracy: 29.39 %\n",
      "Testing Accuracy: 31.43 %\n",
      "Test Loss:  83.10271346569061\n",
      "Epoch [7/500], Loss: 85.6258, Accuracy: 29.07 %\n",
      "Testing Accuracy: 30.95 %\n",
      "Test Loss:  83.28222513198853\n",
      "Epoch [8/500], Loss: 85.3563, Accuracy: 29.67 %\n",
      "Testing Accuracy: 28.94 %\n",
      "Test Loss:  84.58449625968933\n",
      "Epoch [9/500], Loss: 85.6018, Accuracy: 29.36 %\n",
      "Testing Accuracy: 30.56 %\n",
      "Test Loss:  83.58134615421295\n",
      "Epoch [10/500], Loss: 85.8459, Accuracy: 29.05 %\n",
      "Testing Accuracy: 31.18 %\n",
      "Test Loss:  82.97184491157532\n",
      "Epoch [11/500], Loss: 85.3344, Accuracy: 29.60 %\n",
      "Testing Accuracy: 32.16 %\n",
      "Test Loss:  81.99428129196167\n",
      "Epoch [12/500], Loss: 85.4749, Accuracy: 29.55 %\n",
      "Testing Accuracy: 32.36 %\n",
      "Test Loss:  82.02793765068054\n",
      "Epoch [13/500], Loss: 85.1493, Accuracy: 29.84 %\n",
      "Testing Accuracy: 30.86 %\n",
      "Test Loss:  83.25986874103546\n",
      "Epoch [14/500], Loss: 85.1035, Accuracy: 29.53 %\n",
      "Testing Accuracy: 31.56 %\n",
      "Test Loss:  82.59574806690216\n",
      "Epoch [15/500], Loss: 84.9602, Accuracy: 29.86 %\n",
      "Testing Accuracy: 31.89 %\n",
      "Test Loss:  81.96269023418427\n",
      "Epoch [16/500], Loss: 85.0728, Accuracy: 29.67 %\n",
      "Testing Accuracy: 31.80 %\n",
      "Test Loss:  81.86858546733856\n",
      "Epoch [17/500], Loss: 84.9047, Accuracy: 29.84 %\n",
      "Testing Accuracy: 30.97 %\n",
      "Test Loss:  82.78850483894348\n",
      "Epoch [18/500], Loss: 84.9592, Accuracy: 29.96 %\n",
      "Testing Accuracy: 32.16 %\n",
      "Test Loss:  81.88728260993958\n",
      "Epoch [19/500], Loss: 84.7979, Accuracy: 30.08 %\n",
      "Testing Accuracy: 31.73 %\n",
      "Test Loss:  82.6255863904953\n",
      "Epoch [20/500], Loss: 85.2849, Accuracy: 29.42 %\n",
      "Testing Accuracy: 31.46 %\n",
      "Test Loss:  83.06826066970825\n",
      "Epoch [21/500], Loss: 84.8236, Accuracy: 29.77 %\n",
      "Testing Accuracy: 32.42 %\n",
      "Test Loss:  81.93565714359283\n",
      "Epoch [22/500], Loss: 84.8953, Accuracy: 30.26 %\n",
      "Testing Accuracy: 32.11 %\n",
      "Test Loss:  82.10221982002258\n",
      "Epoch [23/500], Loss: 84.6514, Accuracy: 30.14 %\n",
      "Testing Accuracy: 32.51 %\n",
      "Test Loss:  81.63341331481934\n",
      "Epoch [24/500], Loss: 84.7095, Accuracy: 30.00 %\n",
      "Testing Accuracy: 32.05 %\n",
      "Test Loss:  81.475341796875\n",
      "Epoch [25/500], Loss: 84.9556, Accuracy: 29.99 %\n",
      "Testing Accuracy: 32.58 %\n",
      "Test Loss:  81.4747838973999\n",
      "Epoch [26/500], Loss: 84.7789, Accuracy: 30.10 %\n",
      "Testing Accuracy: 32.44 %\n",
      "Test Loss:  81.9357236623764\n",
      "Epoch [27/500], Loss: 84.7299, Accuracy: 30.15 %\n",
      "Testing Accuracy: 30.08 %\n",
      "Test Loss:  85.24174082279205\n",
      "Epoch [28/500], Loss: 84.9267, Accuracy: 30.06 %\n",
      "Testing Accuracy: 32.14 %\n",
      "Test Loss:  81.36442923545837\n",
      "Epoch [29/500], Loss: 84.9272, Accuracy: 29.53 %\n",
      "Testing Accuracy: 32.46 %\n",
      "Test Loss:  81.80911946296692\n",
      "Epoch [30/500], Loss: 84.7608, Accuracy: 30.29 %\n",
      "Testing Accuracy: 32.70 %\n",
      "Test Loss:  81.2877870798111\n",
      "Epoch [31/500], Loss: 84.7535, Accuracy: 29.68 %\n",
      "Testing Accuracy: 32.50 %\n",
      "Test Loss:  81.33340287208557\n",
      "Epoch [32/500], Loss: 85.4729, Accuracy: 29.42 %\n",
      "Testing Accuracy: 31.77 %\n",
      "Test Loss:  82.47693192958832\n",
      "Epoch [33/500], Loss: 84.5146, Accuracy: 30.05 %\n",
      "Testing Accuracy: 29.44 %\n",
      "Test Loss:  83.6239002943039\n",
      "Epoch [34/500], Loss: 84.7022, Accuracy: 29.99 %\n",
      "Testing Accuracy: 31.13 %\n",
      "Test Loss:  82.21619439125061\n",
      "Epoch [35/500], Loss: 85.0084, Accuracy: 29.55 %\n",
      "Testing Accuracy: 32.45 %\n",
      "Test Loss:  81.93443763256073\n",
      "Epoch [36/500], Loss: 84.7946, Accuracy: 29.96 %\n",
      "Testing Accuracy: 32.47 %\n",
      "Test Loss:  81.24646818637848\n",
      "Epoch [37/500], Loss: 84.3890, Accuracy: 30.37 %\n",
      "Testing Accuracy: 29.08 %\n",
      "Test Loss:  83.12694454193115\n",
      "Epoch [38/500], Loss: 84.6847, Accuracy: 29.94 %\n",
      "Testing Accuracy: 31.32 %\n",
      "Test Loss:  82.40743482112885\n",
      "Epoch [39/500], Loss: 84.3314, Accuracy: 30.14 %\n",
      "Testing Accuracy: 31.76 %\n",
      "Test Loss:  81.31794762611389\n",
      "Epoch [40/500], Loss: 84.7601, Accuracy: 29.86 %\n",
      "Testing Accuracy: 31.21 %\n",
      "Test Loss:  82.42700064182281\n",
      "Epoch [41/500], Loss: 84.9110, Accuracy: 29.65 %\n",
      "Testing Accuracy: 32.62 %\n",
      "Test Loss:  81.05976355075836\n",
      "Epoch [42/500], Loss: 84.5944, Accuracy: 29.97 %\n",
      "Testing Accuracy: 31.11 %\n",
      "Test Loss:  82.61770355701447\n",
      "Epoch [43/500], Loss: 84.4173, Accuracy: 30.14 %\n",
      "Testing Accuracy: 32.64 %\n",
      "Test Loss:  80.98943281173706\n",
      "Epoch [44/500], Loss: 84.6264, Accuracy: 29.70 %\n",
      "Testing Accuracy: 30.51 %\n",
      "Test Loss:  82.97065579891205\n",
      "Epoch [45/500], Loss: 84.7813, Accuracy: 29.95 %\n",
      "Testing Accuracy: 32.43 %\n",
      "Test Loss:  81.94762802124023\n",
      "Epoch [46/500], Loss: 85.1906, Accuracy: 29.48 %\n",
      "Testing Accuracy: 30.78 %\n",
      "Test Loss:  82.9597259759903\n",
      "Epoch [47/500], Loss: 84.4527, Accuracy: 30.02 %\n",
      "Testing Accuracy: 29.13 %\n",
      "Test Loss:  83.69638323783875\n",
      "Epoch [48/500], Loss: 85.1466, Accuracy: 29.26 %\n",
      "Testing Accuracy: 29.86 %\n",
      "Test Loss:  83.05985033512115\n",
      "Epoch [49/500], Loss: 84.8610, Accuracy: 29.53 %\n",
      "Testing Accuracy: 31.25 %\n",
      "Test Loss:  82.09778487682343\n",
      "Epoch [50/500], Loss: 84.2255, Accuracy: 30.05 %\n",
      "Testing Accuracy: 30.61 %\n",
      "Test Loss:  84.29737615585327\n",
      "Epoch [51/500], Loss: 84.4884, Accuracy: 30.00 %\n",
      "Testing Accuracy: 32.10 %\n",
      "Test Loss:  82.15936517715454\n",
      "Epoch [52/500], Loss: 84.9858, Accuracy: 29.85 %\n",
      "Testing Accuracy: 32.51 %\n",
      "Test Loss:  81.51900923252106\n",
      "Epoch [53/500], Loss: 84.7638, Accuracy: 29.98 %\n",
      "Testing Accuracy: 32.33 %\n",
      "Test Loss:  81.46900713443756\n",
      "Epoch [54/500], Loss: 84.3378, Accuracy: 30.43 %\n",
      "Testing Accuracy: 31.46 %\n",
      "Test Loss:  82.2495664358139\n",
      "Epoch [55/500], Loss: 84.5135, Accuracy: 30.18 %\n",
      "Testing Accuracy: 31.80 %\n",
      "Test Loss:  81.90590214729309\n",
      "Epoch [56/500], Loss: 84.0536, Accuracy: 30.33 %\n",
      "Testing Accuracy: 31.87 %\n",
      "Test Loss:  82.29227006435394\n",
      "Epoch [57/500], Loss: 84.1774, Accuracy: 30.46 %\n",
      "Testing Accuracy: 31.07 %\n",
      "Test Loss:  83.83884859085083\n",
      "Epoch [58/500], Loss: 84.9617, Accuracy: 29.81 %\n",
      "Testing Accuracy: 33.07 %\n",
      "Test Loss:  80.85603976249695\n",
      "Epoch [59/500], Loss: 84.3267, Accuracy: 30.14 %\n",
      "Testing Accuracy: 30.73 %\n",
      "Test Loss:  82.18322932720184\n",
      "Epoch [60/500], Loss: 84.4880, Accuracy: 30.41 %\n",
      "Testing Accuracy: 33.38 %\n",
      "Test Loss:  80.72733163833618\n",
      "Epoch [61/500], Loss: 84.2198, Accuracy: 30.74 %\n",
      "Testing Accuracy: 29.07 %\n",
      "Test Loss:  83.26232206821442\n",
      "Epoch [62/500], Loss: 83.9018, Accuracy: 31.37 %\n",
      "Testing Accuracy: 31.82 %\n",
      "Test Loss:  83.51925981044769\n",
      "Epoch [63/500], Loss: 83.7704, Accuracy: 31.38 %\n",
      "Testing Accuracy: 34.06 %\n",
      "Test Loss:  80.67644655704498\n",
      "Epoch [64/500], Loss: 83.1013, Accuracy: 31.95 %\n",
      "Testing Accuracy: 34.28 %\n",
      "Test Loss:  81.01789569854736\n",
      "Epoch [65/500], Loss: 83.2295, Accuracy: 32.02 %\n",
      "Testing Accuracy: 34.44 %\n",
      "Test Loss:  80.1579658985138\n",
      "Epoch [66/500], Loss: 83.7930, Accuracy: 31.33 %\n",
      "Testing Accuracy: 34.86 %\n",
      "Test Loss:  79.74557626247406\n",
      "Epoch [67/500], Loss: 83.4200, Accuracy: 31.87 %\n",
      "Testing Accuracy: 34.85 %\n",
      "Test Loss:  79.75396406650543\n",
      "Epoch [68/500], Loss: 82.8195, Accuracy: 32.42 %\n",
      "Testing Accuracy: 35.05 %\n",
      "Test Loss:  79.61322712898254\n",
      "Epoch [69/500], Loss: 83.0284, Accuracy: 32.21 %\n",
      "Testing Accuracy: 30.28 %\n",
      "Test Loss:  82.81672489643097\n",
      "Epoch [70/500], Loss: 82.9699, Accuracy: 32.35 %\n",
      "Testing Accuracy: 34.64 %\n",
      "Test Loss:  79.86187148094177\n",
      "Epoch [71/500], Loss: 82.5860, Accuracy: 32.75 %\n",
      "Testing Accuracy: 35.17 %\n",
      "Test Loss:  79.30931997299194\n",
      "Epoch [72/500], Loss: 82.8060, Accuracy: 32.51 %\n",
      "Testing Accuracy: 33.37 %\n",
      "Test Loss:  80.68334412574768\n",
      "Epoch [73/500], Loss: 82.6273, Accuracy: 32.71 %\n",
      "Testing Accuracy: 34.29 %\n",
      "Test Loss:  79.765988945961\n",
      "Epoch [74/500], Loss: 82.8356, Accuracy: 32.26 %\n",
      "Testing Accuracy: 34.76 %\n",
      "Test Loss:  79.86773908138275\n",
      "Epoch [75/500], Loss: 82.7047, Accuracy: 32.52 %\n",
      "Testing Accuracy: 32.45 %\n",
      "Test Loss:  81.08435940742493\n",
      "Epoch [76/500], Loss: 83.2633, Accuracy: 32.23 %\n",
      "Testing Accuracy: 33.38 %\n",
      "Test Loss:  80.61117231845856\n",
      "Epoch [77/500], Loss: 83.2146, Accuracy: 32.13 %\n",
      "Testing Accuracy: 32.30 %\n",
      "Test Loss:  81.62880671024323\n",
      "Epoch [78/500], Loss: 83.6073, Accuracy: 32.06 %\n",
      "Testing Accuracy: 34.45 %\n",
      "Test Loss:  80.2766547203064\n",
      "Epoch [79/500], Loss: 82.8275, Accuracy: 32.55 %\n",
      "Testing Accuracy: 31.36 %\n",
      "Test Loss:  82.26811623573303\n",
      "Epoch [80/500], Loss: 83.6000, Accuracy: 31.74 %\n",
      "Testing Accuracy: 33.67 %\n",
      "Test Loss:  81.26980376243591\n",
      "Epoch [81/500], Loss: 82.8232, Accuracy: 32.41 %\n",
      "Testing Accuracy: 35.77 %\n",
      "Test Loss:  79.02161908149719\n",
      "Epoch [82/500], Loss: 82.4433, Accuracy: 33.05 %\n",
      "Testing Accuracy: 34.98 %\n",
      "Test Loss:  79.60496735572815\n",
      "Epoch [83/500], Loss: 82.7617, Accuracy: 32.46 %\n",
      "Testing Accuracy: 35.66 %\n",
      "Test Loss:  79.16431939601898\n",
      "Epoch [84/500], Loss: 82.6783, Accuracy: 32.42 %\n",
      "Testing Accuracy: 34.83 %\n",
      "Test Loss:  79.47346436977386\n",
      "Epoch [85/500], Loss: 82.8669, Accuracy: 32.51 %\n",
      "Testing Accuracy: 33.67 %\n",
      "Test Loss:  79.79854416847229\n",
      "Epoch [86/500], Loss: 82.5863, Accuracy: 32.68 %\n",
      "Testing Accuracy: 36.05 %\n",
      "Test Loss:  78.74388027191162\n",
      "Epoch [87/500], Loss: 82.2044, Accuracy: 32.81 %\n",
      "Testing Accuracy: 33.79 %\n",
      "Test Loss:  80.22968566417694\n",
      "Epoch [88/500], Loss: 82.4174, Accuracy: 32.67 %\n",
      "Testing Accuracy: 35.46 %\n",
      "Test Loss:  78.76380944252014\n",
      "Epoch [89/500], Loss: 82.4676, Accuracy: 32.62 %\n",
      "Testing Accuracy: 34.79 %\n",
      "Test Loss:  79.13979363441467\n",
      "Epoch [90/500], Loss: 82.9314, Accuracy: 32.14 %\n",
      "Testing Accuracy: 35.17 %\n",
      "Test Loss:  79.32358574867249\n",
      "Epoch [91/500], Loss: 81.9905, Accuracy: 32.95 %\n",
      "Testing Accuracy: 36.19 %\n",
      "Test Loss:  78.59260654449463\n",
      "Epoch [92/500], Loss: 81.9606, Accuracy: 33.09 %\n",
      "Testing Accuracy: 35.72 %\n",
      "Test Loss:  78.84579741954803\n",
      "Epoch [93/500], Loss: 82.1960, Accuracy: 32.75 %\n",
      "Testing Accuracy: 31.70 %\n",
      "Test Loss:  81.00594234466553\n",
      "Epoch [94/500], Loss: 82.5904, Accuracy: 32.54 %\n",
      "Testing Accuracy: 35.64 %\n",
      "Test Loss:  78.85176765918732\n",
      "Epoch [95/500], Loss: 82.0720, Accuracy: 32.97 %\n",
      "Testing Accuracy: 35.77 %\n",
      "Test Loss:  79.12652850151062\n",
      "Epoch [96/500], Loss: 82.6066, Accuracy: 32.73 %\n",
      "Testing Accuracy: 33.14 %\n",
      "Test Loss:  80.69428753852844\n",
      "Epoch [97/500], Loss: 81.9734, Accuracy: 32.82 %\n",
      "Testing Accuracy: 35.20 %\n",
      "Test Loss:  79.82769513130188\n",
      "Epoch [98/500], Loss: 81.5536, Accuracy: 33.28 %\n",
      "Testing Accuracy: 35.85 %\n",
      "Test Loss:  78.89633023738861\n",
      "Epoch [99/500], Loss: 81.6838, Accuracy: 33.14 %\n",
      "Testing Accuracy: 36.01 %\n",
      "Test Loss:  78.7460435628891\n",
      "Epoch [100/500], Loss: 81.6147, Accuracy: 33.30 %\n",
      "Testing Accuracy: 35.97 %\n",
      "Test Loss:  78.86465048789978\n",
      "Epoch [101/500], Loss: 81.4823, Accuracy: 33.52 %\n",
      "Testing Accuracy: 35.64 %\n",
      "Test Loss:  78.64261186122894\n",
      "Epoch [102/500], Loss: 85.1748, Accuracy: 29.30 %\n",
      "Testing Accuracy: 27.72 %\n",
      "Test Loss:  84.67982804775238\n",
      "Epoch [103/500], Loss: 90.7893, Accuracy: 24.38 %\n",
      "Testing Accuracy: 28.48 %\n",
      "Test Loss:  83.48820555210114\n",
      "Epoch [104/500], Loss: 85.8410, Accuracy: 27.08 %\n",
      "Testing Accuracy: 28.66 %\n",
      "Test Loss:  83.21038234233856\n",
      "Epoch [105/500], Loss: 85.5217, Accuracy: 27.57 %\n",
      "Testing Accuracy: 30.82 %\n",
      "Test Loss:  82.76236867904663\n",
      "Epoch [106/500], Loss: 83.0279, Accuracy: 31.24 %\n",
      "Testing Accuracy: 35.02 %\n",
      "Test Loss:  79.9361754655838\n",
      "Epoch [107/500], Loss: 82.2544, Accuracy: 32.92 %\n",
      "Testing Accuracy: 35.97 %\n",
      "Test Loss:  78.83526813983917\n",
      "Epoch [108/500], Loss: 81.9403, Accuracy: 33.14 %\n",
      "Testing Accuracy: 35.13 %\n",
      "Test Loss:  80.43914258480072\n",
      "Epoch [109/500], Loss: 81.9458, Accuracy: 33.12 %\n",
      "Testing Accuracy: 36.10 %\n",
      "Test Loss:  78.6791285276413\n",
      "Epoch [110/500], Loss: 82.0854, Accuracy: 32.97 %\n",
      "Testing Accuracy: 34.61 %\n",
      "Test Loss:  79.10402405261993\n",
      "Epoch [111/500], Loss: 81.9982, Accuracy: 33.00 %\n",
      "Testing Accuracy: 36.07 %\n",
      "Test Loss:  78.67739617824554\n",
      "Testing Accuracy: 36.07 %\n",
      "FX is unsupported on your pytorch version, falling back to JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\pthflops\\__init__.py:33: DeprecationWarning: Call to deprecated function count_ops_jit (JIT mode is deprecated, please update to pytorch 1.8.0 or newer and use FX.).\n",
      "  *args)\n",
      "C:\\Users\\guest11\\Anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:446: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  training_mode + \", as specified by the export mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                         OPS      \n",
      "--------------------------------  -------  \n",
      "Net/Linear[fc1]/onnx::Gemm        606208   \n",
      "Net/Dropout[dropout]/onnx::Relu   592      \n",
      "Net/Linear[fc2]/onnx::Gemm        3256     \n",
      "-------------------------------   ------   \n",
      "Input size: (1, 2048)\n",
      "610,056 FLOPs or approx. 0.00 GFLOPs\n",
      "test accuracies:  [47.54325716549994, 47.77440232465989, 46.03751155725796, 47.47061154404966, 46.42055210672302, 45.16576409985471, 46.25544842160877, 46.12336547351737, 46.103553031303655, 45.89882446176199, 44.74309866596222, 45.16576409985471, 45.42332584863294, 45.30445119535068, 45.271430458327835, 46.04411570466253, 45.42332584863294, 45.93844934618941, 45.11293092061815, 44.86857746664906, 45.56861709153348, 44.87518161405363, 45.21859727909127, 45.39690925901466, 45.48936732267864, 45.007264562145025, 44.894994056267336, 44.6836613393211, 45.65447100779289, 44.72328622374852, 44.47232862237485, 45.145951657641, 44.8817857614582, 44.439307885352, 44.27420420023775, 44.38647470611544, 43.72605996565843, 44.12230880993263, 44.19495443138291, 43.422269185048215, 43.74587240787214, 43.61378945978075, 43.99022586184124, 43.303394531765946, 43.34301941619337, 43.23074891031568, 43.02602034077401, 43.63360190199445, 42.92035398230089, 43.29679038436138, 42.0353982300885, 41.37498348963149, 39.10976092986396, 40.952318055739006, 35.90674943864747, 38.06630563994188, 37.22757891956148, 40.64192312772421, 37.71628582749967, 38.88521991810857, 39.94188350283978, 39.57205124818386, 40.126799630167746, 37.04266279223352, 34.024567428345, 29.863954563465857, 26.10619469026549, 33.7604015321622, 36.065248976357154]\n",
      "pruning hidden size:  296\n",
      "with hidden layer:  296\n",
      "removing:  (260, 295, 289)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7b1a5f42c7a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#     prune_three(neural_net, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mprune_four\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-98b4be5d42e2>\u001b[0m in \u001b[0;36mprune_four\u001b[1;34m(neural_net, hidden_size)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmallest_angles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mneurons_to_keep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmallest_angles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mneurons_to_keep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmallest_angles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mneurons_to_keep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmallest_angles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mneurons_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# input_size = 2048\n",
    "# hidden_size = 500\n",
    "# num_classes = 11\n",
    "# num_epochs = 500\n",
    "# batch_size = 1024\n",
    "# learning_rate = 0.01\n",
    "# patience = 20\n",
    "# dropout = 0.2\n",
    "\n",
    "# Iterative pruning process, train a base model, prune, fine-tune, \n",
    "# repeat steps 2 and 3 until there are no more suitable neurons to prune\n",
    "# store data for visualisation and analysis \n",
    "test_accs = []\n",
    "hidden_neurons = []\n",
    "flops = []\n",
    "\n",
    "neural_net = Net(input_size, hidden_size, num_classes, dropout).to(device)\n",
    "optimizer = torch.optim.SGD(neural_net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "hidden_neurons.append(hidden_size)\n",
    "test_accs.append(train(neural_net, optimizer))\n",
    "# Calculate the number of FLOPs via 1 example evaluation on the model\n",
    "inp = torch.rand(1,2048).to(device)\n",
    "flops.append(count_ops(neural_net, inp))\n",
    "# while we have neurons to prune\n",
    "while hidden_size > 2:\n",
    "    # Modify the following two lines to use different pruning methods\n",
    "    # either: batch_prune_two_sim, prune_one_sim, or prune_two_sim_comp\n",
    "    batch_prune_two_sim(neural_net, hidden_size)\n",
    "    # 1 for single similarity neuron pruning otherwise 2\n",
    "    hidden_size -= 2\n",
    "    hidden_neurons.append(hidden_size)\n",
    "    optimizer = torch.optim.SGD(neural_net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    test_accs.append(train(neural_net, optimizer))\n",
    "    \n",
    "    flops.append(count_ops(neural_net, inp))\n",
    "    print(\"test accuracies: \", test_accs)\n",
    "print(hidden_neurons)\n",
    "print(test_accs)\n",
    "plt.figure()\n",
    "plt.plot(hidden_neurons)\n",
    "plt.plot(test_accs)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously run data, added here for visualisation purposes\n",
    "\n",
    "\n",
    "test_accs = [44.15532954695549, 45.81297054550257, 45.37709681680095, 44.98745211993131, 44.604411570466254, 44.544974243825116, 41.86369039756968, 44.32703737947431, 44.58459912825254, 41.79764892352397, 44.87518161405363, 40.43719455818254, 44.23457931581033, 42.755250297186635, 44.20816272619204, 44.96103553031304, 41.573107911768595, 43.059041077796856, 42.365605600317, 42.57033416985867, 41.20327565711266, 41.7910447761194, 40.32492405230485, 39.77017567032096, 41.26931713115837, 41.40800422665434, 41.758024039096554, 36.283185840707965, 40.82683925505217, 41.41460837405891, 40.24567428345001, 41.196671509708096, 41.5268788799366, 40.46361114780082, 40.85985999207502, 39.12296922467309, 39.64469686963413, 40.78721437062475, 40.13340377757231, 40.384361378945975, 38.35688812574297, 40.42398626337339, 39.65790516444327, 40.49663188482367, 40.357944789327696, 39.823008849557525, 40.06075815612205, 40.126799630167746, 38.317263241315544, 39.836217144366664, 39.00409457139084, 40.0475498613129, 38.89182406551314, 38.66067890635319, 38.488971073834364, 38.69369964337604, 38.80597014925373, 38.97767798177255, 39.472989037115305, 37.511557257958, 38.82578259146744, 39.129573372077665, 37.72949412230881, 38.171971998415, 37.92101439704134, 38.145555408796724, 36.26337339849425, 38.58142913749835, 38.304054946506405, 36.91057984414212, 38.25782591467441, 39.01730286619997, 38.08611808215559, 38.204992735437855, 38.64086646413948, 38.60124157971206, 36.309602430326244, 37.86818121780478, 37.06907938185181, 37.84836877559108, 37.63703605864483, 35.127460044908204, 36.441685378417645, 37.38607845727117, 38.317263241315544, 36.88416325452384, 38.27103420948356, 35.774666490556065, 37.478536520935144, 38.08611808215559, 37.34645357284375, 36.989829612996964, 37.465328226126005, 37.670056795667676, 37.83516048078193, 36.92378813895127, 37.854972922995636, 37.452119931316865, 37.16153744551578, 36.818121780478144, 37.03605864482895, 37.709681680095095, 37.108704266279226, 37.075683529256374, 36.369039756967375, 37.25399550917977, 35.1935015189539, 36.43508123101308, 37.26059965658434, 36.758684453837006, 36.62660150574561, 37.656848500858544, 37.207766477347775, 37.656848500858544, 36.963413023378685, 37.51816140536256, 38.304054946506405, 36.20393607185312, 36.68603883238674, 35.61616695284639, 35.87372870162462, 36.514330999867916, 35.32558446704531, 34.07740060758156, 34.26892088231409, 34.374587240787214, 35.16708492933562, 35.028397833839655, 33.60190199445252, 31.87821952185973, 36.61999735834104, 34.62554484216088, 36.05864482895258, 34.823669264297976, 35.31898031964074, 29.929996037511557, 29.540351340641923, 23.127724210804384, 23.827763835688813, 23.761722361643113, 24.699511293092062, 22.975828820499274, 24.686302998282923, 22.678642187293622, 24.49478272355039, 24.36269977545899, 21.62858274996698, 21.61537445515784, 21.549332981112137, 21.82010302469951, 22.48712191256109, 24.924052304847443, 23.78813895126139, 24.5145951657641, 24.924052304847443, 24.197596090344735, 23.966450931184784, 24.666490556069213, 24.138158763703608, 23.52397305507859, 22.467309470347377, 23.365473517368905, 23.768326509047682, 24.4485536917184, 22.58618412362964, 23.391890106987187, 22.48051776515652, 24.29005415400872, 21.549332981112137, 21.54272883370757, 21.54272883370757, 21.549332981112137, 21.54272883370757, 21.522916391493858, 21.522916391493858, 21.522916391493858, 21.522916391493858, 21.51631224408929, 21.51631224408929, 21.51631224408929, 21.51631224408929, 21.51631224408929, 21.50970809668472, 21.51631224408929, 21.522916391493858, 21.51631224408929, 21.522916391493858, 21.522916391493858, 21.522916391493858, 21.522916391493858, 21.522916391493858, 21.51631224408929, 21.51631224408929, 21.51631224408929, 21.529520538898428, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "fig = plt.figure()\n",
    "plt.plot(test_accs)\n",
    "\n",
    "plt.suptitle('Accuracy of Similar and Complementary pruning method')\n",
    "plt.xlabel('Prunes')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "fig.savefig('similarandcomp.png', dpi=300, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs_small_only = [45.62145027077004, 45.225201426495836, 46.796988508783514, 43.54774798573504, 43.55435213313961, 44.58459912825254, 42.88072909787346, 43.94399683000925, 44.71668207634394, 44.67045304451195, 44.776119402985074, 44.51195350680227, 42.16087703077533, 43.521331396116764, 41.005151234975564, 43.158103288865405, 42.464667811385546, 42.57033416985867, 41.52027473253203, 44.07607977810065, 43.072249372605995, 41.23629639413552, 40.397569673755115, 40.59569409589222, 38.984282129177124, 41.903315281997095, 40.879672434288736, 41.21648395192181, 40.77400607581561, 38.81917844406287, 39.98811253467177, 40.84004754986131, 39.9154669132215, 40.50984017963281, 38.898428212917715, 38.218201030246995, 40.47021529520539, 40.63531898031964, 40.648527275128785, 41.077796856425834, 40.19284110421344, 40.98533879276186, 39.7173424910844, 39.28146876238278, 40.42398626337339, 39.882446176198656, 38.96446968696342, 39.836217144366664, 38.60784572711663, 40.84004754986131, 40.19284110421344, 40.1598203671906, 39.18240655131422, 39.88905032360322, 39.71073834367983, 39.24844802535993, 39.52582221635187, 39.96169594505349, 41.051380266807556, 38.64086646413948, 36.804913485669, 37.670056795667676, 37.425703341698586, 39.61828028001585, 36.97001717078325, 37.947430986659626, 38.56822084268921, 40.30511161009114, 37.280412098798045, 37.095495971470086, 39.45317659490159, 39.03711530841368, 39.189010698718796, 38.488971073834364, 37.86818121780478, 39.06353189803197, 39.7173424910844, 38.80597014925373, 38.91824065513143, 39.40034341566504, 37.8417646281865, 37.37287016246202, 38.04649319772817, 38.93144894994056, 36.17751948223484, 39.968300092458065, 39.34090608902391, 39.142781666886805, 38.594637432307486, 36.77189274864615, 37.45872407872144, 38.42292959978867, 38.11253467177387, 38.50878351604808, 37.934222691850486, 37.96063928146876, 39.48619733192445, 37.71628582749967, 38.27103420948356, 38.70690793818518, 39.103156782459386, 38.17857614581958, 38.119138819178445, 37.81534803856822, 38.2842425042927, 37.438911636507726, 38.99088627658169, 37.353057720248316, 38.39651301017039, 38.03988905032361, 38.19838858803328, 38.244617619865274, 36.62660150574561, 37.425703341698586, 36.97001717078325, 36.85114251750099, 36.77849689605072, 38.65407475894862, 37.062475234447234, 38.713512085589755, 37.72288997490424, 36.600184916127326, 37.914410249636774, 38.449346189406945, 38.06630563994188, 37.26059965658434, 36.369039756967375, 37.075683529256374, 36.283185840707965, 35.35200105666358, 37.41909919429401, 36.67943468498217, 36.23035266147141, 36.963413023378685, 36.91718399154669, 36.56056003169991, 37.34645357284375, 37.38607845727117, 37.0030379078061, 36.184123629639416, 36.223748514066834, 33.60190199445252, 37.27380795139347, 37.92761854444591, 36.28978998811254, 37.306828688416324, 35.60956280544182, 36.39545634658566, 36.804913485669, 35.71522916391494, 36.8709549597147, 35.5699379210144, 35.86712455422005, 35.55672962620526, 36.28978998811254, 35.13406419231278, 36.098269713380006, 35.87372870162462, 35.70862501651037, 35.45106326773214, 35.51050059437327, 35.47087570994585, 36.23695680887597, 36.26337339849425, 36.58037247391362, 36.197331924448555, 37.06907938185181, 35.58314621582353, 34.55289922071061, 33.27169462422401, 34.850085853916255, 35.39823008849557, 35.36520935147273, 35.47087570994585, 35.9001452912429, 35.99920750231146, 35.25293884559503, 33.23206973979659, 32.19521859727909, 33.04054946506406, 34.01135913353586, 33.07357020208691, 33.839651301017035, 33.364152687887994, 34.57271166292431, 36.04543653414344, 34.13683793422269, 33.11319508651433, 32.78298771628583, 32.57825914674415, 33.740589089948486, 33.14621582353718, 33.324527803460576, 32.99432043323207, 33.46981904636112, 34.189671113459255, 34.737815348038566, 34.94914806498481, 35.53691718399155, 34.87650244353454, 34.12362963941355, 34.47364945185576, 34.361378945978075, 33.925505217276445, 34.62554484216088, 34.83027341170255, 32.888654074758946, 33.95192180689473, 33.31131950865143, 33.4301941619337, 33.99154669132215, 33.41698586712455, 33.265090476819445, 33.60190199445252, 34.301941619336944, 34.4868577466649, 33.839651301017035, 33.88588033284903, 33.01413287544578, 32.55184255712587, 32.11596882842425, 32.57825914674415, 32.88204992735438, 31.376304319112403, 28.767666094307227, 28.985602958658035, 25.64390437194558, 24.996697926297713, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_accs_small_only)\n",
    "plt.suptitle('Accuracy of Similar pruning method')\n",
    "plt.xlabel('Prunes')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "fig.savefig('justsimilar.png', dpi=300, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs_3 = [43.4024567428345, 45.04688944657245, 45.65447100779289, 46.12336547351737, 45.35068022718267, 43.158103288865405, 43.924184387795535, 44.74309866596222, 43.996830009245805, 43.44208162726192, 42.54391758024039, 44.42609959054286, 41.08440100383041, 42.827895918636905, 43.607185312376174, 42.0353982300885, 43.48831065909391, 43.23074891031568, 39.80980055474838, 39.869237881389516, 42.12785629375247, 40.5560692114648, 42.45145951657641, 41.7910447761194, 40.291903315282, 42.19389776779818, 42.6297714964998, 41.01175538238013, 41.22969224673095, 40.483423590014525, 41.07119270902126, 40.69475630696077, 39.585259542993, 40.43719455818254, 39.71073834367983, 39.869237881389516, 40.648527275128785, 39.75036322810725, 38.11253467177387, 39.94848765024435, 38.70030379078061, 39.591863690397574, 40.32492405230485, 39.116365077268526, 39.71073834367983, 38.64747061154405, 37.478536520935144, 38.2842425042927, 37.755910711927086, 38.158763703605864, 39.18240655131422, 39.26165632016907, 40.12019548276318, 38.70030379078061, 39.50600977413816, 38.33707568352926, 40.27209087306829, 38.81917844406287, 38.70030379078061, 38.25122176726985, 38.27103420948356, 38.88521991810857, 38.82578259146744, 37.63043191124026, 38.70690793818518, 38.62765816933034, 37.2011623299432, 39.677717606656984, 38.33707568352926, 38.23801347246071, 36.137894597807424, 37.9078061022322, 37.537973847576275, 38.449346189406945, 39.189010698718796, 37.92761854444591, 37.24739136177519, 38.7465328226126, 38.99088627658169, 39.72394663848897, 37.65024435345397, 38.488971073834364, 37.108704266279226, 38.38330471536125, 38.21159688284242, 36.963413023378685, 38.19838858803328, 38.944657244749706, 39.3871351208559, 37.438911636507726, 38.059701492537314, 39.30788535200105, 37.67666094307225, 36.98322546559239, 37.59080702681284, 38.350283978338396, 37.41909919429401, 37.50495311055343, 38.49557522123894, 38.44274204200238, 39.274864614978206, 35.31898031964074, 36.88416325452384, 37.914410249636774, 36.25676925108968, 36.01241579712059, 35.16708492933562, 35.41143838330471, 37.564390437194554, 36.98322546559239, 36.89076740192841, 36.454893673226785, 38.317263241315544, 36.71245542200502, 36.976621318187824, 37.392682604675734, 35.00858539162594, 36.07185312376172, 36.10487386078457, 34.275525029718665, 36.54735173689077, 35.94637432307489, 35.979395060097744, 36.82472592788271, 37.03605864482895, 35.919957733456606, 37.22757891956148, 36.89076740192841, 35.59635451063268, 36.34922731475366, 36.098269713380006, 37.379474309866595, 36.44828952582221, 35.0614185708625, 35.39823008849557, 35.48408400475498, 36.11147800818914, 35.62277110025096, 36.74547615902787, 34.94254391758024, 35.834103817197196, 35.13406419231278, 35.85391625941091, 35.345396909259016, 35.173689076740196, 35.444459120327565, 36.296394135517104, 35.71522916391494, 35.60956280544182, 35.90674943864747, 36.00581164971602, 35.85391625941091, 36.40206049399023, 35.33879276185444, 35.5699379210144, 35.79447893276978, 34.35477479857351, 34.85669000132083, 34.493461894069476, 34.9293356227711, 35.07462686567165, 33.278298771628585, 32.8094043059041, 31.779157310791177, 35.345396909259016, 35.503896446968696, 32.598071588957865, 25.45238409721305, 26.05336151102893, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998, 21.536124686302998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_accs_3)\n",
    "plt.suptitle('Accuracy of Batch pruning via Similarity')\n",
    "plt.xlabel('Prunes')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "fig.savefig('batchprune2.png', dpi=300, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "inflection_sim_comp = 0\n",
    "for index, value in enumerate(test_accs):\n",
    "    if value < 30:\n",
    "        print(index)\n",
    "        inflection_sim_comp = index\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "inflection_sim = 0\n",
    "for index, value in enumerate(test_accs_small_only):\n",
    "    if value < 30:\n",
    "        print(index)\n",
    "        inflection_sim = index\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "inflection_2_sim = 0\n",
    "for index, value in enumerate(test_accs_3):\n",
    "    if value < 30:\n",
    "        print(index)\n",
    "        inflection_2_sim = index\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_2 = [(1030500, [['Net/Linear[fc1]/onnx::Gemm', 1024000], ['Net/ReLU[relu]/onnx::Relu', 1000], ['Net/Linear[fc2]/onnx::Gemm', 5500]]), (1026378, [['Net/Linear[fc1]/onnx::Gemm', 1019904], ['Net/ReLU[relu]/onnx::Relu', 996], ['Net/Linear[fc2]/onnx::Gemm', 5478]]), (1022256, [['Net/Linear[fc1]/onnx::Gemm', 1015808], ['Net/ReLU[relu]/onnx::Relu', 992], ['Net/Linear[fc2]/onnx::Gemm', 5456]]), (1018134, [['Net/Linear[fc1]/onnx::Gemm', 1011712], ['Net/ReLU[relu]/onnx::Relu', 988], ['Net/Linear[fc2]/onnx::Gemm', 5434]]), (1014012, [['Net/Linear[fc1]/onnx::Gemm', 1007616], ['Net/ReLU[relu]/onnx::Relu', 984], ['Net/Linear[fc2]/onnx::Gemm', 5412]]), (1009890, [['Net/Linear[fc1]/onnx::Gemm', 1003520], ['Net/ReLU[relu]/onnx::Relu', 980], ['Net/Linear[fc2]/onnx::Gemm', 5390]]), (1005768, [['Net/Linear[fc1]/onnx::Gemm', 999424], ['Net/ReLU[relu]/onnx::Relu', 976], ['Net/Linear[fc2]/onnx::Gemm', 5368]]), (1001646, [['Net/Linear[fc1]/onnx::Gemm', 995328], ['Net/ReLU[relu]/onnx::Relu', 972], ['Net/Linear[fc2]/onnx::Gemm', 5346]]), (997524, [['Net/Linear[fc1]/onnx::Gemm', 991232], ['Net/ReLU[relu]/onnx::Relu', 968], ['Net/Linear[fc2]/onnx::Gemm', 5324]]), (993402, [['Net/Linear[fc1]/onnx::Gemm', 987136], ['Net/ReLU[relu]/onnx::Relu', 964], ['Net/Linear[fc2]/onnx::Gemm', 5302]]), (989280, [['Net/Linear[fc1]/onnx::Gemm', 983040], ['Net/ReLU[relu]/onnx::Relu', 960], ['Net/Linear[fc2]/onnx::Gemm', 5280]]), (985158, [['Net/Linear[fc1]/onnx::Gemm', 978944], ['Net/ReLU[relu]/onnx::Relu', 956], ['Net/Linear[fc2]/onnx::Gemm', 5258]]), (981036, [['Net/Linear[fc1]/onnx::Gemm', 974848], ['Net/ReLU[relu]/onnx::Relu', 952], ['Net/Linear[fc2]/onnx::Gemm', 5236]]), (976914, [['Net/Linear[fc1]/onnx::Gemm', 970752], ['Net/ReLU[relu]/onnx::Relu', 948], ['Net/Linear[fc2]/onnx::Gemm', 5214]]), (972792, [['Net/Linear[fc1]/onnx::Gemm', 966656], ['Net/ReLU[relu]/onnx::Relu', 944], ['Net/Linear[fc2]/onnx::Gemm', 5192]]), (968670, [['Net/Linear[fc1]/onnx::Gemm', 962560], ['Net/ReLU[relu]/onnx::Relu', 940], ['Net/Linear[fc2]/onnx::Gemm', 5170]]), (964548, [['Net/Linear[fc1]/onnx::Gemm', 958464], ['Net/ReLU[relu]/onnx::Relu', 936], ['Net/Linear[fc2]/onnx::Gemm', 5148]]), (960426, [['Net/Linear[fc1]/onnx::Gemm', 954368], ['Net/ReLU[relu]/onnx::Relu', 932], ['Net/Linear[fc2]/onnx::Gemm', 5126]]), (956304, [['Net/Linear[fc1]/onnx::Gemm', 950272], ['Net/ReLU[relu]/onnx::Relu', 928], ['Net/Linear[fc2]/onnx::Gemm', 5104]]), (952182, [['Net/Linear[fc1]/onnx::Gemm', 946176], ['Net/ReLU[relu]/onnx::Relu', 924], ['Net/Linear[fc2]/onnx::Gemm', 5082]]), (948060, [['Net/Linear[fc1]/onnx::Gemm', 942080], ['Net/ReLU[relu]/onnx::Relu', 920], ['Net/Linear[fc2]/onnx::Gemm', 5060]]), (943938, [['Net/Linear[fc1]/onnx::Gemm', 937984], ['Net/ReLU[relu]/onnx::Relu', 916], ['Net/Linear[fc2]/onnx::Gemm', 5038]]), (939816, [['Net/Linear[fc1]/onnx::Gemm', 933888], ['Net/ReLU[relu]/onnx::Relu', 912], ['Net/Linear[fc2]/onnx::Gemm', 5016]]), (935694, [['Net/Linear[fc1]/onnx::Gemm', 929792], ['Net/ReLU[relu]/onnx::Relu', 908], ['Net/Linear[fc2]/onnx::Gemm', 4994]]), (931572, [['Net/Linear[fc1]/onnx::Gemm', 925696], ['Net/ReLU[relu]/onnx::Relu', 904], ['Net/Linear[fc2]/onnx::Gemm', 4972]]), (927450, [['Net/Linear[fc1]/onnx::Gemm', 921600], ['Net/ReLU[relu]/onnx::Relu', 900], ['Net/Linear[fc2]/onnx::Gemm', 4950]]), (923328, [['Net/Linear[fc1]/onnx::Gemm', 917504], ['Net/ReLU[relu]/onnx::Relu', 896], ['Net/Linear[fc2]/onnx::Gemm', 4928]]), (919206, [['Net/Linear[fc1]/onnx::Gemm', 913408], ['Net/ReLU[relu]/onnx::Relu', 892], ['Net/Linear[fc2]/onnx::Gemm', 4906]]), (915084, [['Net/Linear[fc1]/onnx::Gemm', 909312], ['Net/ReLU[relu]/onnx::Relu', 888], ['Net/Linear[fc2]/onnx::Gemm', 4884]]), (910962, [['Net/Linear[fc1]/onnx::Gemm', 905216], ['Net/ReLU[relu]/onnx::Relu', 884], ['Net/Linear[fc2]/onnx::Gemm', 4862]]), (906840, [['Net/Linear[fc1]/onnx::Gemm', 901120], ['Net/ReLU[relu]/onnx::Relu', 880], ['Net/Linear[fc2]/onnx::Gemm', 4840]]), (902718, [['Net/Linear[fc1]/onnx::Gemm', 897024], ['Net/ReLU[relu]/onnx::Relu', 876], ['Net/Linear[fc2]/onnx::Gemm', 4818]]), (898596, [['Net/Linear[fc1]/onnx::Gemm', 892928], ['Net/ReLU[relu]/onnx::Relu', 872], ['Net/Linear[fc2]/onnx::Gemm', 4796]]), (894474, [['Net/Linear[fc1]/onnx::Gemm', 888832], ['Net/ReLU[relu]/onnx::Relu', 868], ['Net/Linear[fc2]/onnx::Gemm', 4774]]), (890352, [['Net/Linear[fc1]/onnx::Gemm', 884736], ['Net/ReLU[relu]/onnx::Relu', 864], ['Net/Linear[fc2]/onnx::Gemm', 4752]]), (886230, [['Net/Linear[fc1]/onnx::Gemm', 880640], ['Net/ReLU[relu]/onnx::Relu', 860], ['Net/Linear[fc2]/onnx::Gemm', 4730]]), (882108, [['Net/Linear[fc1]/onnx::Gemm', 876544], ['Net/ReLU[relu]/onnx::Relu', 856], ['Net/Linear[fc2]/onnx::Gemm', 4708]]), (877986, [['Net/Linear[fc1]/onnx::Gemm', 872448], ['Net/ReLU[relu]/onnx::Relu', 852], ['Net/Linear[fc2]/onnx::Gemm', 4686]]), (873864, [['Net/Linear[fc1]/onnx::Gemm', 868352], ['Net/ReLU[relu]/onnx::Relu', 848], ['Net/Linear[fc2]/onnx::Gemm', 4664]]), (869742, [['Net/Linear[fc1]/onnx::Gemm', 864256], ['Net/ReLU[relu]/onnx::Relu', 844], ['Net/Linear[fc2]/onnx::Gemm', 4642]]), (865620, [['Net/Linear[fc1]/onnx::Gemm', 860160], ['Net/ReLU[relu]/onnx::Relu', 840], ['Net/Linear[fc2]/onnx::Gemm', 4620]]), (861498, [['Net/Linear[fc1]/onnx::Gemm', 856064], ['Net/ReLU[relu]/onnx::Relu', 836], ['Net/Linear[fc2]/onnx::Gemm', 4598]]), (857376, [['Net/Linear[fc1]/onnx::Gemm', 851968], ['Net/ReLU[relu]/onnx::Relu', 832], ['Net/Linear[fc2]/onnx::Gemm', 4576]]), (853254, [['Net/Linear[fc1]/onnx::Gemm', 847872], ['Net/ReLU[relu]/onnx::Relu', 828], ['Net/Linear[fc2]/onnx::Gemm', 4554]]), (849132, [['Net/Linear[fc1]/onnx::Gemm', 843776], ['Net/ReLU[relu]/onnx::Relu', 824], ['Net/Linear[fc2]/onnx::Gemm', 4532]]), (845010, [['Net/Linear[fc1]/onnx::Gemm', 839680], ['Net/ReLU[relu]/onnx::Relu', 820], ['Net/Linear[fc2]/onnx::Gemm', 4510]]), (840888, [['Net/Linear[fc1]/onnx::Gemm', 835584], ['Net/ReLU[relu]/onnx::Relu', 816], ['Net/Linear[fc2]/onnx::Gemm', 4488]]), (836766, [['Net/Linear[fc1]/onnx::Gemm', 831488], ['Net/ReLU[relu]/onnx::Relu', 812], ['Net/Linear[fc2]/onnx::Gemm', 4466]]), (832644, [['Net/Linear[fc1]/onnx::Gemm', 827392], ['Net/ReLU[relu]/onnx::Relu', 808], ['Net/Linear[fc2]/onnx::Gemm', 4444]]), (828522, [['Net/Linear[fc1]/onnx::Gemm', 823296], ['Net/ReLU[relu]/onnx::Relu', 804], ['Net/Linear[fc2]/onnx::Gemm', 4422]]), (824400, [['Net/Linear[fc1]/onnx::Gemm', 819200], ['Net/ReLU[relu]/onnx::Relu', 800], ['Net/Linear[fc2]/onnx::Gemm', 4400]]), (820278, [['Net/Linear[fc1]/onnx::Gemm', 815104], ['Net/ReLU[relu]/onnx::Relu', 796], ['Net/Linear[fc2]/onnx::Gemm', 4378]]), (816156, [['Net/Linear[fc1]/onnx::Gemm', 811008], ['Net/ReLU[relu]/onnx::Relu', 792], ['Net/Linear[fc2]/onnx::Gemm', 4356]]), (812034, [['Net/Linear[fc1]/onnx::Gemm', 806912], ['Net/ReLU[relu]/onnx::Relu', 788], ['Net/Linear[fc2]/onnx::Gemm', 4334]]), (807912, [['Net/Linear[fc1]/onnx::Gemm', 802816], ['Net/ReLU[relu]/onnx::Relu', 784], ['Net/Linear[fc2]/onnx::Gemm', 4312]]), (803790, [['Net/Linear[fc1]/onnx::Gemm', 798720], ['Net/ReLU[relu]/onnx::Relu', 780], ['Net/Linear[fc2]/onnx::Gemm', 4290]]), (799668, [['Net/Linear[fc1]/onnx::Gemm', 794624], ['Net/ReLU[relu]/onnx::Relu', 776], ['Net/Linear[fc2]/onnx::Gemm', 4268]]), (795546, [['Net/Linear[fc1]/onnx::Gemm', 790528], ['Net/ReLU[relu]/onnx::Relu', 772], ['Net/Linear[fc2]/onnx::Gemm', 4246]]), (791424, [['Net/Linear[fc1]/onnx::Gemm', 786432], ['Net/ReLU[relu]/onnx::Relu', 768], ['Net/Linear[fc2]/onnx::Gemm', 4224]]), (787302, [['Net/Linear[fc1]/onnx::Gemm', 782336], ['Net/ReLU[relu]/onnx::Relu', 764], ['Net/Linear[fc2]/onnx::Gemm', 4202]]), (783180, [['Net/Linear[fc1]/onnx::Gemm', 778240], ['Net/ReLU[relu]/onnx::Relu', 760], ['Net/Linear[fc2]/onnx::Gemm', 4180]]), (779058, [['Net/Linear[fc1]/onnx::Gemm', 774144], ['Net/ReLU[relu]/onnx::Relu', 756], ['Net/Linear[fc2]/onnx::Gemm', 4158]]), (774936, [['Net/Linear[fc1]/onnx::Gemm', 770048], ['Net/ReLU[relu]/onnx::Relu', 752], ['Net/Linear[fc2]/onnx::Gemm', 4136]]), (770814, [['Net/Linear[fc1]/onnx::Gemm', 765952], ['Net/ReLU[relu]/onnx::Relu', 748], ['Net/Linear[fc2]/onnx::Gemm', 4114]]), (766692, [['Net/Linear[fc1]/onnx::Gemm', 761856], ['Net/ReLU[relu]/onnx::Relu', 744], ['Net/Linear[fc2]/onnx::Gemm', 4092]]), (762570, [['Net/Linear[fc1]/onnx::Gemm', 757760], ['Net/ReLU[relu]/onnx::Relu', 740], ['Net/Linear[fc2]/onnx::Gemm', 4070]]), (758448, [['Net/Linear[fc1]/onnx::Gemm', 753664], ['Net/ReLU[relu]/onnx::Relu', 736], ['Net/Linear[fc2]/onnx::Gemm', 4048]]), (754326, [['Net/Linear[fc1]/onnx::Gemm', 749568], ['Net/ReLU[relu]/onnx::Relu', 732], ['Net/Linear[fc2]/onnx::Gemm', 4026]]), (750204, [['Net/Linear[fc1]/onnx::Gemm', 745472], ['Net/ReLU[relu]/onnx::Relu', 728], ['Net/Linear[fc2]/onnx::Gemm', 4004]]), (746082, [['Net/Linear[fc1]/onnx::Gemm', 741376], ['Net/ReLU[relu]/onnx::Relu', 724], ['Net/Linear[fc2]/onnx::Gemm', 3982]]), (741960, [['Net/Linear[fc1]/onnx::Gemm', 737280], ['Net/ReLU[relu]/onnx::Relu', 720], ['Net/Linear[fc2]/onnx::Gemm', 3960]]), (737838, [['Net/Linear[fc1]/onnx::Gemm', 733184], ['Net/ReLU[relu]/onnx::Relu', 716], ['Net/Linear[fc2]/onnx::Gemm', 3938]]), (733716, [['Net/Linear[fc1]/onnx::Gemm', 729088], ['Net/ReLU[relu]/onnx::Relu', 712], ['Net/Linear[fc2]/onnx::Gemm', 3916]]), (729594, [['Net/Linear[fc1]/onnx::Gemm', 724992], ['Net/ReLU[relu]/onnx::Relu', 708], ['Net/Linear[fc2]/onnx::Gemm', 3894]]), (725472, [['Net/Linear[fc1]/onnx::Gemm', 720896], ['Net/ReLU[relu]/onnx::Relu', 704], ['Net/Linear[fc2]/onnx::Gemm', 3872]]), (721350, [['Net/Linear[fc1]/onnx::Gemm', 716800], ['Net/ReLU[relu]/onnx::Relu', 700], ['Net/Linear[fc2]/onnx::Gemm', 3850]]), (717228, [['Net/Linear[fc1]/onnx::Gemm', 712704], ['Net/ReLU[relu]/onnx::Relu', 696], ['Net/Linear[fc2]/onnx::Gemm', 3828]]), (713106, [['Net/Linear[fc1]/onnx::Gemm', 708608], ['Net/ReLU[relu]/onnx::Relu', 692], ['Net/Linear[fc2]/onnx::Gemm', 3806]]), (708984, [['Net/Linear[fc1]/onnx::Gemm', 704512], ['Net/ReLU[relu]/onnx::Relu', 688], ['Net/Linear[fc2]/onnx::Gemm', 3784]]), (704862, [['Net/Linear[fc1]/onnx::Gemm', 700416], ['Net/ReLU[relu]/onnx::Relu', 684], ['Net/Linear[fc2]/onnx::Gemm', 3762]]), (700740, [['Net/Linear[fc1]/onnx::Gemm', 696320], ['Net/ReLU[relu]/onnx::Relu', 680], ['Net/Linear[fc2]/onnx::Gemm', 3740]]), (696618, [['Net/Linear[fc1]/onnx::Gemm', 692224], ['Net/ReLU[relu]/onnx::Relu', 676], ['Net/Linear[fc2]/onnx::Gemm', 3718]]), (692496, [['Net/Linear[fc1]/onnx::Gemm', 688128], ['Net/ReLU[relu]/onnx::Relu', 672], ['Net/Linear[fc2]/onnx::Gemm', 3696]]), (688374, [['Net/Linear[fc1]/onnx::Gemm', 684032], ['Net/ReLU[relu]/onnx::Relu', 668], ['Net/Linear[fc2]/onnx::Gemm', 3674]]), (684252, [['Net/Linear[fc1]/onnx::Gemm', 679936], ['Net/ReLU[relu]/onnx::Relu', 664], ['Net/Linear[fc2]/onnx::Gemm', 3652]]), (680130, [['Net/Linear[fc1]/onnx::Gemm', 675840], ['Net/ReLU[relu]/onnx::Relu', 660], ['Net/Linear[fc2]/onnx::Gemm', 3630]]), (676008, [['Net/Linear[fc1]/onnx::Gemm', 671744], ['Net/ReLU[relu]/onnx::Relu', 656], ['Net/Linear[fc2]/onnx::Gemm', 3608]]), (671886, [['Net/Linear[fc1]/onnx::Gemm', 667648], ['Net/ReLU[relu]/onnx::Relu', 652], ['Net/Linear[fc2]/onnx::Gemm', 3586]]), (667764, [['Net/Linear[fc1]/onnx::Gemm', 663552], ['Net/ReLU[relu]/onnx::Relu', 648], ['Net/Linear[fc2]/onnx::Gemm', 3564]]), (663642, [['Net/Linear[fc1]/onnx::Gemm', 659456], ['Net/ReLU[relu]/onnx::Relu', 644], ['Net/Linear[fc2]/onnx::Gemm', 3542]]), (659520, [['Net/Linear[fc1]/onnx::Gemm', 655360], ['Net/ReLU[relu]/onnx::Relu', 640], ['Net/Linear[fc2]/onnx::Gemm', 3520]]), (655398, [['Net/Linear[fc1]/onnx::Gemm', 651264], ['Net/ReLU[relu]/onnx::Relu', 636], ['Net/Linear[fc2]/onnx::Gemm', 3498]]), (651276, [['Net/Linear[fc1]/onnx::Gemm', 647168], ['Net/ReLU[relu]/onnx::Relu', 632], ['Net/Linear[fc2]/onnx::Gemm', 3476]]), (647154, [['Net/Linear[fc1]/onnx::Gemm', 643072], ['Net/ReLU[relu]/onnx::Relu', 628], ['Net/Linear[fc2]/onnx::Gemm', 3454]]), (643032, [['Net/Linear[fc1]/onnx::Gemm', 638976], ['Net/ReLU[relu]/onnx::Relu', 624], ['Net/Linear[fc2]/onnx::Gemm', 3432]]), (638910, [['Net/Linear[fc1]/onnx::Gemm', 634880], ['Net/ReLU[relu]/onnx::Relu', 620], ['Net/Linear[fc2]/onnx::Gemm', 3410]]), (634788, [['Net/Linear[fc1]/onnx::Gemm', 630784], ['Net/ReLU[relu]/onnx::Relu', 616], ['Net/Linear[fc2]/onnx::Gemm', 3388]]), (630666, [['Net/Linear[fc1]/onnx::Gemm', 626688], ['Net/ReLU[relu]/onnx::Relu', 612], ['Net/Linear[fc2]/onnx::Gemm', 3366]]), (626544, [['Net/Linear[fc1]/onnx::Gemm', 622592], ['Net/ReLU[relu]/onnx::Relu', 608], ['Net/Linear[fc2]/onnx::Gemm', 3344]]), (622422, [['Net/Linear[fc1]/onnx::Gemm', 618496], ['Net/ReLU[relu]/onnx::Relu', 604], ['Net/Linear[fc2]/onnx::Gemm', 3322]]), (618300, [['Net/Linear[fc1]/onnx::Gemm', 614400], ['Net/ReLU[relu]/onnx::Relu', 600], ['Net/Linear[fc2]/onnx::Gemm', 3300]]), (614178, [['Net/Linear[fc1]/onnx::Gemm', 610304], ['Net/ReLU[relu]/onnx::Relu', 596], ['Net/Linear[fc2]/onnx::Gemm', 3278]]), (610056, [['Net/Linear[fc1]/onnx::Gemm', 606208], ['Net/ReLU[relu]/onnx::Relu', 592], ['Net/Linear[fc2]/onnx::Gemm', 3256]]), (605934, [['Net/Linear[fc1]/onnx::Gemm', 602112], ['Net/ReLU[relu]/onnx::Relu', 588], ['Net/Linear[fc2]/onnx::Gemm', 3234]]), (601812, [['Net/Linear[fc1]/onnx::Gemm', 598016], ['Net/ReLU[relu]/onnx::Relu', 584], ['Net/Linear[fc2]/onnx::Gemm', 3212]]), (597690, [['Net/Linear[fc1]/onnx::Gemm', 593920], ['Net/ReLU[relu]/onnx::Relu', 580], ['Net/Linear[fc2]/onnx::Gemm', 3190]]), (593568, [['Net/Linear[fc1]/onnx::Gemm', 589824], ['Net/ReLU[relu]/onnx::Relu', 576], ['Net/Linear[fc2]/onnx::Gemm', 3168]]), (589446, [['Net/Linear[fc1]/onnx::Gemm', 585728], ['Net/ReLU[relu]/onnx::Relu', 572], ['Net/Linear[fc2]/onnx::Gemm', 3146]]), (585324, [['Net/Linear[fc1]/onnx::Gemm', 581632], ['Net/ReLU[relu]/onnx::Relu', 568], ['Net/Linear[fc2]/onnx::Gemm', 3124]]), (581202, [['Net/Linear[fc1]/onnx::Gemm', 577536], ['Net/ReLU[relu]/onnx::Relu', 564], ['Net/Linear[fc2]/onnx::Gemm', 3102]]), (577080, [['Net/Linear[fc1]/onnx::Gemm', 573440], ['Net/ReLU[relu]/onnx::Relu', 560], ['Net/Linear[fc2]/onnx::Gemm', 3080]]), (572958, [['Net/Linear[fc1]/onnx::Gemm', 569344], ['Net/ReLU[relu]/onnx::Relu', 556], ['Net/Linear[fc2]/onnx::Gemm', 3058]]), (568836, [['Net/Linear[fc1]/onnx::Gemm', 565248], ['Net/ReLU[relu]/onnx::Relu', 552], ['Net/Linear[fc2]/onnx::Gemm', 3036]]), (564714, [['Net/Linear[fc1]/onnx::Gemm', 561152], ['Net/ReLU[relu]/onnx::Relu', 548], ['Net/Linear[fc2]/onnx::Gemm', 3014]]), (560592, [['Net/Linear[fc1]/onnx::Gemm', 557056], ['Net/ReLU[relu]/onnx::Relu', 544], ['Net/Linear[fc2]/onnx::Gemm', 2992]]), (556470, [['Net/Linear[fc1]/onnx::Gemm', 552960], ['Net/ReLU[relu]/onnx::Relu', 540], ['Net/Linear[fc2]/onnx::Gemm', 2970]]), (552348, [['Net/Linear[fc1]/onnx::Gemm', 548864], ['Net/ReLU[relu]/onnx::Relu', 536], ['Net/Linear[fc2]/onnx::Gemm', 2948]]), (548226, [['Net/Linear[fc1]/onnx::Gemm', 544768], ['Net/ReLU[relu]/onnx::Relu', 532], ['Net/Linear[fc2]/onnx::Gemm', 2926]]), (544104, [['Net/Linear[fc1]/onnx::Gemm', 540672], ['Net/ReLU[relu]/onnx::Relu', 528], ['Net/Linear[fc2]/onnx::Gemm', 2904]]), (539982, [['Net/Linear[fc1]/onnx::Gemm', 536576], ['Net/ReLU[relu]/onnx::Relu', 524], ['Net/Linear[fc2]/onnx::Gemm', 2882]]), (535860, [['Net/Linear[fc1]/onnx::Gemm', 532480], ['Net/ReLU[relu]/onnx::Relu', 520], ['Net/Linear[fc2]/onnx::Gemm', 2860]]), (531738, [['Net/Linear[fc1]/onnx::Gemm', 528384], ['Net/ReLU[relu]/onnx::Relu', 516], ['Net/Linear[fc2]/onnx::Gemm', 2838]]), (527616, [['Net/Linear[fc1]/onnx::Gemm', 524288], ['Net/ReLU[relu]/onnx::Relu', 512], ['Net/Linear[fc2]/onnx::Gemm', 2816]]), (523494, [['Net/Linear[fc1]/onnx::Gemm', 520192], ['Net/ReLU[relu]/onnx::Relu', 508], ['Net/Linear[fc2]/onnx::Gemm', 2794]]), (519372, [['Net/Linear[fc1]/onnx::Gemm', 516096], ['Net/ReLU[relu]/onnx::Relu', 504], ['Net/Linear[fc2]/onnx::Gemm', 2772]]), (515250, [['Net/Linear[fc1]/onnx::Gemm', 512000], ['Net/ReLU[relu]/onnx::Relu', 500], ['Net/Linear[fc2]/onnx::Gemm', 2750]]), (511128, [['Net/Linear[fc1]/onnx::Gemm', 507904], ['Net/ReLU[relu]/onnx::Relu', 496], ['Net/Linear[fc2]/onnx::Gemm', 2728]]), (507006, [['Net/Linear[fc1]/onnx::Gemm', 503808], ['Net/ReLU[relu]/onnx::Relu', 492], ['Net/Linear[fc2]/onnx::Gemm', 2706]]), (502884, [['Net/Linear[fc1]/onnx::Gemm', 499712], ['Net/ReLU[relu]/onnx::Relu', 488], ['Net/Linear[fc2]/onnx::Gemm', 2684]]), (498762, [['Net/Linear[fc1]/onnx::Gemm', 495616], ['Net/ReLU[relu]/onnx::Relu', 484], ['Net/Linear[fc2]/onnx::Gemm', 2662]]), (494640, [['Net/Linear[fc1]/onnx::Gemm', 491520], ['Net/ReLU[relu]/onnx::Relu', 480], ['Net/Linear[fc2]/onnx::Gemm', 2640]]), (490518, [['Net/Linear[fc1]/onnx::Gemm', 487424], ['Net/ReLU[relu]/onnx::Relu', 476], ['Net/Linear[fc2]/onnx::Gemm', 2618]]), (486396, [['Net/Linear[fc1]/onnx::Gemm', 483328], ['Net/ReLU[relu]/onnx::Relu', 472], ['Net/Linear[fc2]/onnx::Gemm', 2596]]), (482274, [['Net/Linear[fc1]/onnx::Gemm', 479232], ['Net/ReLU[relu]/onnx::Relu', 468], ['Net/Linear[fc2]/onnx::Gemm', 2574]]), (478152, [['Net/Linear[fc1]/onnx::Gemm', 475136], ['Net/ReLU[relu]/onnx::Relu', 464], ['Net/Linear[fc2]/onnx::Gemm', 2552]]), (474030, [['Net/Linear[fc1]/onnx::Gemm', 471040], ['Net/ReLU[relu]/onnx::Relu', 460], ['Net/Linear[fc2]/onnx::Gemm', 2530]]), (469908, [['Net/Linear[fc1]/onnx::Gemm', 466944], ['Net/ReLU[relu]/onnx::Relu', 456], ['Net/Linear[fc2]/onnx::Gemm', 2508]]), (465786, [['Net/Linear[fc1]/onnx::Gemm', 462848], ['Net/ReLU[relu]/onnx::Relu', 452], ['Net/Linear[fc2]/onnx::Gemm', 2486]]), (461664, [['Net/Linear[fc1]/onnx::Gemm', 458752], ['Net/ReLU[relu]/onnx::Relu', 448], ['Net/Linear[fc2]/onnx::Gemm', 2464]]), (457542, [['Net/Linear[fc1]/onnx::Gemm', 454656], ['Net/ReLU[relu]/onnx::Relu', 444], ['Net/Linear[fc2]/onnx::Gemm', 2442]]), (453420, [['Net/Linear[fc1]/onnx::Gemm', 450560], ['Net/ReLU[relu]/onnx::Relu', 440], ['Net/Linear[fc2]/onnx::Gemm', 2420]]), (449298, [['Net/Linear[fc1]/onnx::Gemm', 446464], ['Net/ReLU[relu]/onnx::Relu', 436], ['Net/Linear[fc2]/onnx::Gemm', 2398]]), (445176, [['Net/Linear[fc1]/onnx::Gemm', 442368], ['Net/ReLU[relu]/onnx::Relu', 432], ['Net/Linear[fc2]/onnx::Gemm', 2376]]), (441054, [['Net/Linear[fc1]/onnx::Gemm', 438272], ['Net/ReLU[relu]/onnx::Relu', 428], ['Net/Linear[fc2]/onnx::Gemm', 2354]]), (436932, [['Net/Linear[fc1]/onnx::Gemm', 434176], ['Net/ReLU[relu]/onnx::Relu', 424], ['Net/Linear[fc2]/onnx::Gemm', 2332]]), (432810, [['Net/Linear[fc1]/onnx::Gemm', 430080], ['Net/ReLU[relu]/onnx::Relu', 420], ['Net/Linear[fc2]/onnx::Gemm', 2310]]), (428688, [['Net/Linear[fc1]/onnx::Gemm', 425984], ['Net/ReLU[relu]/onnx::Relu', 416], ['Net/Linear[fc2]/onnx::Gemm', 2288]]), (424566, [['Net/Linear[fc1]/onnx::Gemm', 421888], ['Net/ReLU[relu]/onnx::Relu', 412], ['Net/Linear[fc2]/onnx::Gemm', 2266]]), (420444, [['Net/Linear[fc1]/onnx::Gemm', 417792], ['Net/ReLU[relu]/onnx::Relu', 408], ['Net/Linear[fc2]/onnx::Gemm', 2244]]), (416322, [['Net/Linear[fc1]/onnx::Gemm', 413696], ['Net/ReLU[relu]/onnx::Relu', 404], ['Net/Linear[fc2]/onnx::Gemm', 2222]]), (412200, [['Net/Linear[fc1]/onnx::Gemm', 409600], ['Net/ReLU[relu]/onnx::Relu', 400], ['Net/Linear[fc2]/onnx::Gemm', 2200]]), (408078, [['Net/Linear[fc1]/onnx::Gemm', 405504], ['Net/ReLU[relu]/onnx::Relu', 396], ['Net/Linear[fc2]/onnx::Gemm', 2178]]), (403956, [['Net/Linear[fc1]/onnx::Gemm', 401408], ['Net/ReLU[relu]/onnx::Relu', 392], ['Net/Linear[fc2]/onnx::Gemm', 2156]]), (399834, [['Net/Linear[fc1]/onnx::Gemm', 397312], ['Net/ReLU[relu]/onnx::Relu', 388], ['Net/Linear[fc2]/onnx::Gemm', 2134]]), (395712, [['Net/Linear[fc1]/onnx::Gemm', 393216], ['Net/ReLU[relu]/onnx::Relu', 384], ['Net/Linear[fc2]/onnx::Gemm', 2112]]), (391590, [['Net/Linear[fc1]/onnx::Gemm', 389120], ['Net/ReLU[relu]/onnx::Relu', 380], ['Net/Linear[fc2]/onnx::Gemm', 2090]]), (387468, [['Net/Linear[fc1]/onnx::Gemm', 385024], ['Net/ReLU[relu]/onnx::Relu', 376], ['Net/Linear[fc2]/onnx::Gemm', 2068]]), (383346, [['Net/Linear[fc1]/onnx::Gemm', 380928], ['Net/ReLU[relu]/onnx::Relu', 372], ['Net/Linear[fc2]/onnx::Gemm', 2046]]), (379224, [['Net/Linear[fc1]/onnx::Gemm', 376832], ['Net/ReLU[relu]/onnx::Relu', 368], ['Net/Linear[fc2]/onnx::Gemm', 2024]]), (375102, [['Net/Linear[fc1]/onnx::Gemm', 372736], ['Net/ReLU[relu]/onnx::Relu', 364], ['Net/Linear[fc2]/onnx::Gemm', 2002]]), (370980, [['Net/Linear[fc1]/onnx::Gemm', 368640], ['Net/ReLU[relu]/onnx::Relu', 360], ['Net/Linear[fc2]/onnx::Gemm', 1980]]), (366858, [['Net/Linear[fc1]/onnx::Gemm', 364544], ['Net/ReLU[relu]/onnx::Relu', 356], ['Net/Linear[fc2]/onnx::Gemm', 1958]]), (362736, [['Net/Linear[fc1]/onnx::Gemm', 360448], ['Net/ReLU[relu]/onnx::Relu', 352], ['Net/Linear[fc2]/onnx::Gemm', 1936]]), (358614, [['Net/Linear[fc1]/onnx::Gemm', 356352], ['Net/ReLU[relu]/onnx::Relu', 348], ['Net/Linear[fc2]/onnx::Gemm', 1914]]), (354492, [['Net/Linear[fc1]/onnx::Gemm', 352256], ['Net/ReLU[relu]/onnx::Relu', 344], ['Net/Linear[fc2]/onnx::Gemm', 1892]]), (350370, [['Net/Linear[fc1]/onnx::Gemm', 348160], ['Net/ReLU[relu]/onnx::Relu', 340], ['Net/Linear[fc2]/onnx::Gemm', 1870]]), (346248, [['Net/Linear[fc1]/onnx::Gemm', 344064], ['Net/ReLU[relu]/onnx::Relu', 336], ['Net/Linear[fc2]/onnx::Gemm', 1848]]), (342126, [['Net/Linear[fc1]/onnx::Gemm', 339968], ['Net/ReLU[relu]/onnx::Relu', 332], ['Net/Linear[fc2]/onnx::Gemm', 1826]]), (338004, [['Net/Linear[fc1]/onnx::Gemm', 335872], ['Net/ReLU[relu]/onnx::Relu', 328], ['Net/Linear[fc2]/onnx::Gemm', 1804]]), (333882, [['Net/Linear[fc1]/onnx::Gemm', 331776], ['Net/ReLU[relu]/onnx::Relu', 324], ['Net/Linear[fc2]/onnx::Gemm', 1782]]), (329760, [['Net/Linear[fc1]/onnx::Gemm', 327680], ['Net/ReLU[relu]/onnx::Relu', 320], ['Net/Linear[fc2]/onnx::Gemm', 1760]]), (325638, [['Net/Linear[fc1]/onnx::Gemm', 323584], ['Net/ReLU[relu]/onnx::Relu', 316], ['Net/Linear[fc2]/onnx::Gemm', 1738]]), (321516, [['Net/Linear[fc1]/onnx::Gemm', 319488], ['Net/ReLU[relu]/onnx::Relu', 312], ['Net/Linear[fc2]/onnx::Gemm', 1716]]), (317394, [['Net/Linear[fc1]/onnx::Gemm', 315392], ['Net/ReLU[relu]/onnx::Relu', 308], ['Net/Linear[fc2]/onnx::Gemm', 1694]]), (313272, [['Net/Linear[fc1]/onnx::Gemm', 311296], ['Net/ReLU[relu]/onnx::Relu', 304], ['Net/Linear[fc2]/onnx::Gemm', 1672]]), (309150, [['Net/Linear[fc1]/onnx::Gemm', 307200], ['Net/ReLU[relu]/onnx::Relu', 300], ['Net/Linear[fc2]/onnx::Gemm', 1650]]), (305028, [['Net/Linear[fc1]/onnx::Gemm', 303104], ['Net/ReLU[relu]/onnx::Relu', 296], ['Net/Linear[fc2]/onnx::Gemm', 1628]]), (300906, [['Net/Linear[fc1]/onnx::Gemm', 299008], ['Net/ReLU[relu]/onnx::Relu', 292], ['Net/Linear[fc2]/onnx::Gemm', 1606]]), (296784, [['Net/Linear[fc1]/onnx::Gemm', 294912], ['Net/ReLU[relu]/onnx::Relu', 288], ['Net/Linear[fc2]/onnx::Gemm', 1584]]), (292662, [['Net/Linear[fc1]/onnx::Gemm', 290816], ['Net/ReLU[relu]/onnx::Relu', 284], ['Net/Linear[fc2]/onnx::Gemm', 1562]]), (288540, [['Net/Linear[fc1]/onnx::Gemm', 286720], ['Net/ReLU[relu]/onnx::Relu', 280], ['Net/Linear[fc2]/onnx::Gemm', 1540]]), (284418, [['Net/Linear[fc1]/onnx::Gemm', 282624], ['Net/ReLU[relu]/onnx::Relu', 276], ['Net/Linear[fc2]/onnx::Gemm', 1518]]), (280296, [['Net/Linear[fc1]/onnx::Gemm', 278528], ['Net/ReLU[relu]/onnx::Relu', 272], ['Net/Linear[fc2]/onnx::Gemm', 1496]]), (276174, [['Net/Linear[fc1]/onnx::Gemm', 274432], ['Net/ReLU[relu]/onnx::Relu', 268], ['Net/Linear[fc2]/onnx::Gemm', 1474]]), (272052, [['Net/Linear[fc1]/onnx::Gemm', 270336], ['Net/ReLU[relu]/onnx::Relu', 264], ['Net/Linear[fc2]/onnx::Gemm', 1452]]), (267930, [['Net/Linear[fc1]/onnx::Gemm', 266240], ['Net/ReLU[relu]/onnx::Relu', 260], ['Net/Linear[fc2]/onnx::Gemm', 1430]]), (263808, [['Net/Linear[fc1]/onnx::Gemm', 262144], ['Net/ReLU[relu]/onnx::Relu', 256], ['Net/Linear[fc2]/onnx::Gemm', 1408]]), (259686, [['Net/Linear[fc1]/onnx::Gemm', 258048], ['Net/ReLU[relu]/onnx::Relu', 252], ['Net/Linear[fc2]/onnx::Gemm', 1386]]), (255564, [['Net/Linear[fc1]/onnx::Gemm', 253952], ['Net/ReLU[relu]/onnx::Relu', 248], ['Net/Linear[fc2]/onnx::Gemm', 1364]]), (251442, [['Net/Linear[fc1]/onnx::Gemm', 249856], ['Net/ReLU[relu]/onnx::Relu', 244], ['Net/Linear[fc2]/onnx::Gemm', 1342]]), (247320, [['Net/Linear[fc1]/onnx::Gemm', 245760], ['Net/ReLU[relu]/onnx::Relu', 240], ['Net/Linear[fc2]/onnx::Gemm', 1320]]), (243198, [['Net/Linear[fc1]/onnx::Gemm', 241664], ['Net/ReLU[relu]/onnx::Relu', 236], ['Net/Linear[fc2]/onnx::Gemm', 1298]]), (239076, [['Net/Linear[fc1]/onnx::Gemm', 237568], ['Net/ReLU[relu]/onnx::Relu', 232], ['Net/Linear[fc2]/onnx::Gemm', 1276]]), (234954, [['Net/Linear[fc1]/onnx::Gemm', 233472], ['Net/ReLU[relu]/onnx::Relu', 228], ['Net/Linear[fc2]/onnx::Gemm', 1254]]), (230832, [['Net/Linear[fc1]/onnx::Gemm', 229376], ['Net/ReLU[relu]/onnx::Relu', 224], ['Net/Linear[fc2]/onnx::Gemm', 1232]]), (226710, [['Net/Linear[fc1]/onnx::Gemm', 225280], ['Net/ReLU[relu]/onnx::Relu', 220], ['Net/Linear[fc2]/onnx::Gemm', 1210]]), (222588, [['Net/Linear[fc1]/onnx::Gemm', 221184], ['Net/ReLU[relu]/onnx::Relu', 216], ['Net/Linear[fc2]/onnx::Gemm', 1188]]), (218466, [['Net/Linear[fc1]/onnx::Gemm', 217088], ['Net/ReLU[relu]/onnx::Relu', 212], ['Net/Linear[fc2]/onnx::Gemm', 1166]]), (214344, [['Net/Linear[fc1]/onnx::Gemm', 212992], ['Net/ReLU[relu]/onnx::Relu', 208], ['Net/Linear[fc2]/onnx::Gemm', 1144]]), (210222, [['Net/Linear[fc1]/onnx::Gemm', 208896], ['Net/ReLU[relu]/onnx::Relu', 204], ['Net/Linear[fc2]/onnx::Gemm', 1122]]), (206100, [['Net/Linear[fc1]/onnx::Gemm', 204800], ['Net/ReLU[relu]/onnx::Relu', 200], ['Net/Linear[fc2]/onnx::Gemm', 1100]]), (201978, [['Net/Linear[fc1]/onnx::Gemm', 200704], ['Net/ReLU[relu]/onnx::Relu', 196], ['Net/Linear[fc2]/onnx::Gemm', 1078]]), (197856, [['Net/Linear[fc1]/onnx::Gemm', 196608], ['Net/ReLU[relu]/onnx::Relu', 192], ['Net/Linear[fc2]/onnx::Gemm', 1056]]), (193734, [['Net/Linear[fc1]/onnx::Gemm', 192512], ['Net/ReLU[relu]/onnx::Relu', 188], ['Net/Linear[fc2]/onnx::Gemm', 1034]]), (189612, [['Net/Linear[fc1]/onnx::Gemm', 188416], ['Net/ReLU[relu]/onnx::Relu', 184], ['Net/Linear[fc2]/onnx::Gemm', 1012]]), (185490, [['Net/Linear[fc1]/onnx::Gemm', 184320], ['Net/ReLU[relu]/onnx::Relu', 180], ['Net/Linear[fc2]/onnx::Gemm', 990]]), (181368, [['Net/Linear[fc1]/onnx::Gemm', 180224], ['Net/ReLU[relu]/onnx::Relu', 176], ['Net/Linear[fc2]/onnx::Gemm', 968]]), (177246, [['Net/Linear[fc1]/onnx::Gemm', 176128], ['Net/ReLU[relu]/onnx::Relu', 172], ['Net/Linear[fc2]/onnx::Gemm', 946]]), (173124, [['Net/Linear[fc1]/onnx::Gemm', 172032], ['Net/ReLU[relu]/onnx::Relu', 168], ['Net/Linear[fc2]/onnx::Gemm', 924]]), (169002, [['Net/Linear[fc1]/onnx::Gemm', 167936], ['Net/ReLU[relu]/onnx::Relu', 164], ['Net/Linear[fc2]/onnx::Gemm', 902]]), (164880, [['Net/Linear[fc1]/onnx::Gemm', 163840], ['Net/ReLU[relu]/onnx::Relu', 160], ['Net/Linear[fc2]/onnx::Gemm', 880]]), (160758, [['Net/Linear[fc1]/onnx::Gemm', 159744], ['Net/ReLU[relu]/onnx::Relu', 156], ['Net/Linear[fc2]/onnx::Gemm', 858]]), (156636, [['Net/Linear[fc1]/onnx::Gemm', 155648], ['Net/ReLU[relu]/onnx::Relu', 152], ['Net/Linear[fc2]/onnx::Gemm', 836]]), (152514, [['Net/Linear[fc1]/onnx::Gemm', 151552], ['Net/ReLU[relu]/onnx::Relu', 148], ['Net/Linear[fc2]/onnx::Gemm', 814]]), (148392, [['Net/Linear[fc1]/onnx::Gemm', 147456], ['Net/ReLU[relu]/onnx::Relu', 144], ['Net/Linear[fc2]/onnx::Gemm', 792]]), (144270, [['Net/Linear[fc1]/onnx::Gemm', 143360], ['Net/ReLU[relu]/onnx::Relu', 140], ['Net/Linear[fc2]/onnx::Gemm', 770]]), (140148, [['Net/Linear[fc1]/onnx::Gemm', 139264], ['Net/ReLU[relu]/onnx::Relu', 136], ['Net/Linear[fc2]/onnx::Gemm', 748]]), (136026, [['Net/Linear[fc1]/onnx::Gemm', 135168], ['Net/ReLU[relu]/onnx::Relu', 132], ['Net/Linear[fc2]/onnx::Gemm', 726]]), (131904, [['Net/Linear[fc1]/onnx::Gemm', 131072], ['Net/ReLU[relu]/onnx::Relu', 128], ['Net/Linear[fc2]/onnx::Gemm', 704]]), (127782, [['Net/Linear[fc1]/onnx::Gemm', 126976], ['Net/ReLU[relu]/onnx::Relu', 124], ['Net/Linear[fc2]/onnx::Gemm', 682]]), (123660, [['Net/Linear[fc1]/onnx::Gemm', 122880], ['Net/ReLU[relu]/onnx::Relu', 120], ['Net/Linear[fc2]/onnx::Gemm', 660]]), (119538, [['Net/Linear[fc1]/onnx::Gemm', 118784], ['Net/ReLU[relu]/onnx::Relu', 116], ['Net/Linear[fc2]/onnx::Gemm', 638]]), (115416, [['Net/Linear[fc1]/onnx::Gemm', 114688], ['Net/ReLU[relu]/onnx::Relu', 112], ['Net/Linear[fc2]/onnx::Gemm', 616]]), (111294, [['Net/Linear[fc1]/onnx::Gemm', 110592], ['Net/ReLU[relu]/onnx::Relu', 108], ['Net/Linear[fc2]/onnx::Gemm', 594]]), (107172, [['Net/Linear[fc1]/onnx::Gemm', 106496], ['Net/ReLU[relu]/onnx::Relu', 104], ['Net/Linear[fc2]/onnx::Gemm', 572]]), (103050, [['Net/Linear[fc1]/onnx::Gemm', 102400], ['Net/ReLU[relu]/onnx::Relu', 100], ['Net/Linear[fc2]/onnx::Gemm', 550]]), (98928, [['Net/Linear[fc1]/onnx::Gemm', 98304], ['Net/ReLU[relu]/onnx::Relu', 96], ['Net/Linear[fc2]/onnx::Gemm', 528]]), (94806, [['Net/Linear[fc1]/onnx::Gemm', 94208], ['Net/ReLU[relu]/onnx::Relu', 92], ['Net/Linear[fc2]/onnx::Gemm', 506]]), (90684, [['Net/Linear[fc1]/onnx::Gemm', 90112], ['Net/ReLU[relu]/onnx::Relu', 88], ['Net/Linear[fc2]/onnx::Gemm', 484]]), (86562, [['Net/Linear[fc1]/onnx::Gemm', 86016], ['Net/ReLU[relu]/onnx::Relu', 84], ['Net/Linear[fc2]/onnx::Gemm', 462]]), (82440, [['Net/Linear[fc1]/onnx::Gemm', 81920], ['Net/ReLU[relu]/onnx::Relu', 80], ['Net/Linear[fc2]/onnx::Gemm', 440]]), (78318, [['Net/Linear[fc1]/onnx::Gemm', 77824], ['Net/ReLU[relu]/onnx::Relu', 76], ['Net/Linear[fc2]/onnx::Gemm', 418]]), (74196, [['Net/Linear[fc1]/onnx::Gemm', 73728], ['Net/ReLU[relu]/onnx::Relu', 72], ['Net/Linear[fc2]/onnx::Gemm', 396]]), (70074, [['Net/Linear[fc1]/onnx::Gemm', 69632], ['Net/ReLU[relu]/onnx::Relu', 68], ['Net/Linear[fc2]/onnx::Gemm', 374]]), (65952, [['Net/Linear[fc1]/onnx::Gemm', 65536], ['Net/ReLU[relu]/onnx::Relu', 64], ['Net/Linear[fc2]/onnx::Gemm', 352]]), (61830, [['Net/Linear[fc1]/onnx::Gemm', 61440], ['Net/ReLU[relu]/onnx::Relu', 60], ['Net/Linear[fc2]/onnx::Gemm', 330]]), (57708, [['Net/Linear[fc1]/onnx::Gemm', 57344], ['Net/ReLU[relu]/onnx::Relu', 56], ['Net/Linear[fc2]/onnx::Gemm', 308]]), (53586, [['Net/Linear[fc1]/onnx::Gemm', 53248], ['Net/ReLU[relu]/onnx::Relu', 52], ['Net/Linear[fc2]/onnx::Gemm', 286]]), (49464, [['Net/Linear[fc1]/onnx::Gemm', 49152], ['Net/ReLU[relu]/onnx::Relu', 48], ['Net/Linear[fc2]/onnx::Gemm', 264]]), (45342, [['Net/Linear[fc1]/onnx::Gemm', 45056], ['Net/ReLU[relu]/onnx::Relu', 44], ['Net/Linear[fc2]/onnx::Gemm', 242]]), (41220, [['Net/Linear[fc1]/onnx::Gemm', 40960], ['Net/ReLU[relu]/onnx::Relu', 40], ['Net/Linear[fc2]/onnx::Gemm', 220]]), (37098, [['Net/Linear[fc1]/onnx::Gemm', 36864], ['Net/ReLU[relu]/onnx::Relu', 36], ['Net/Linear[fc2]/onnx::Gemm', 198]]), (32976, [['Net/Linear[fc1]/onnx::Gemm', 32768], ['Net/ReLU[relu]/onnx::Relu', 32], ['Net/Linear[fc2]/onnx::Gemm', 176]]), (28854, [['Net/Linear[fc1]/onnx::Gemm', 28672], ['Net/ReLU[relu]/onnx::Relu', 28], ['Net/Linear[fc2]/onnx::Gemm', 154]]), (24732, [['Net/Linear[fc1]/onnx::Gemm', 24576], ['Net/ReLU[relu]/onnx::Relu', 24], ['Net/Linear[fc2]/onnx::Gemm', 132]]), (20610, [['Net/Linear[fc1]/onnx::Gemm', 20480], ['Net/ReLU[relu]/onnx::Relu', 20], ['Net/Linear[fc2]/onnx::Gemm', 110]]), (16488, [['Net/Linear[fc1]/onnx::Gemm', 16384], ['Net/ReLU[relu]/onnx::Relu', 16], ['Net/Linear[fc2]/onnx::Gemm', 88]]), (12366, [['Net/Linear[fc1]/onnx::Gemm', 12288], ['Net/ReLU[relu]/onnx::Relu', 12], ['Net/Linear[fc2]/onnx::Gemm', 66]]), (8244, [['Net/Linear[fc1]/onnx::Gemm', 8192], ['Net/ReLU[relu]/onnx::Relu', 8], ['Net/Linear[fc2]/onnx::Gemm', 44]]), (4122, [['Net/Linear[fc1]/onnx::Gemm', 4096], ['Net/ReLU[relu]/onnx::Relu', 4], ['Net/Linear[fc2]/onnx::Gemm', 22]])]\n",
    "flops_3 = [(1030500, [['Net/Linear[fc1]/onnx::Gemm', 1024000], ['Net/ReLU[relu]/onnx::Relu', 1000], ['Net/Linear[fc2]/onnx::Gemm', 5500]]), (1026378, [['Net/Linear[fc1]/onnx::Gemm', 1019904], ['Net/ReLU[relu]/onnx::Relu', 996], ['Net/Linear[fc2]/onnx::Gemm', 5478]]), (1022256, [['Net/Linear[fc1]/onnx::Gemm', 1015808], ['Net/ReLU[relu]/onnx::Relu', 992], ['Net/Linear[fc2]/onnx::Gemm', 5456]]), (1018134, [['Net/Linear[fc1]/onnx::Gemm', 1011712], ['Net/ReLU[relu]/onnx::Relu', 988], ['Net/Linear[fc2]/onnx::Gemm', 5434]]), (1014012, [['Net/Linear[fc1]/onnx::Gemm', 1007616], ['Net/ReLU[relu]/onnx::Relu', 984], ['Net/Linear[fc2]/onnx::Gemm', 5412]]), (1009890, [['Net/Linear[fc1]/onnx::Gemm', 1003520], ['Net/ReLU[relu]/onnx::Relu', 980], ['Net/Linear[fc2]/onnx::Gemm', 5390]]), (1005768, [['Net/Linear[fc1]/onnx::Gemm', 999424], ['Net/ReLU[relu]/onnx::Relu', 976], ['Net/Linear[fc2]/onnx::Gemm', 5368]]), (1001646, [['Net/Linear[fc1]/onnx::Gemm', 995328], ['Net/ReLU[relu]/onnx::Relu', 972], ['Net/Linear[fc2]/onnx::Gemm', 5346]]), (997524, [['Net/Linear[fc1]/onnx::Gemm', 991232], ['Net/ReLU[relu]/onnx::Relu', 968], ['Net/Linear[fc2]/onnx::Gemm', 5324]]), (993402, [['Net/Linear[fc1]/onnx::Gemm', 987136], ['Net/ReLU[relu]/onnx::Relu', 964], ['Net/Linear[fc2]/onnx::Gemm', 5302]]), (989280, [['Net/Linear[fc1]/onnx::Gemm', 983040], ['Net/ReLU[relu]/onnx::Relu', 960], ['Net/Linear[fc2]/onnx::Gemm', 5280]]), (985158, [['Net/Linear[fc1]/onnx::Gemm', 978944], ['Net/ReLU[relu]/onnx::Relu', 956], ['Net/Linear[fc2]/onnx::Gemm', 5258]]), (981036, [['Net/Linear[fc1]/onnx::Gemm', 974848], ['Net/ReLU[relu]/onnx::Relu', 952], ['Net/Linear[fc2]/onnx::Gemm', 5236]]), (976914, [['Net/Linear[fc1]/onnx::Gemm', 970752], ['Net/ReLU[relu]/onnx::Relu', 948], ['Net/Linear[fc2]/onnx::Gemm', 5214]]), (972792, [['Net/Linear[fc1]/onnx::Gemm', 966656], ['Net/ReLU[relu]/onnx::Relu', 944], ['Net/Linear[fc2]/onnx::Gemm', 5192]]), (968670, [['Net/Linear[fc1]/onnx::Gemm', 962560], ['Net/ReLU[relu]/onnx::Relu', 940], ['Net/Linear[fc2]/onnx::Gemm', 5170]]), (964548, [['Net/Linear[fc1]/onnx::Gemm', 958464], ['Net/ReLU[relu]/onnx::Relu', 936], ['Net/Linear[fc2]/onnx::Gemm', 5148]]), (960426, [['Net/Linear[fc1]/onnx::Gemm', 954368], ['Net/ReLU[relu]/onnx::Relu', 932], ['Net/Linear[fc2]/onnx::Gemm', 5126]]), (956304, [['Net/Linear[fc1]/onnx::Gemm', 950272], ['Net/ReLU[relu]/onnx::Relu', 928], ['Net/Linear[fc2]/onnx::Gemm', 5104]]), (952182, [['Net/Linear[fc1]/onnx::Gemm', 946176], ['Net/ReLU[relu]/onnx::Relu', 924], ['Net/Linear[fc2]/onnx::Gemm', 5082]]), (948060, [['Net/Linear[fc1]/onnx::Gemm', 942080], ['Net/ReLU[relu]/onnx::Relu', 920], ['Net/Linear[fc2]/onnx::Gemm', 5060]]), (943938, [['Net/Linear[fc1]/onnx::Gemm', 937984], ['Net/ReLU[relu]/onnx::Relu', 916], ['Net/Linear[fc2]/onnx::Gemm', 5038]]), (939816, [['Net/Linear[fc1]/onnx::Gemm', 933888], ['Net/ReLU[relu]/onnx::Relu', 912], ['Net/Linear[fc2]/onnx::Gemm', 5016]]), (935694, [['Net/Linear[fc1]/onnx::Gemm', 929792], ['Net/ReLU[relu]/onnx::Relu', 908], ['Net/Linear[fc2]/onnx::Gemm', 4994]]), (931572, [['Net/Linear[fc1]/onnx::Gemm', 925696], ['Net/ReLU[relu]/onnx::Relu', 904], ['Net/Linear[fc2]/onnx::Gemm', 4972]]), (927450, [['Net/Linear[fc1]/onnx::Gemm', 921600], ['Net/ReLU[relu]/onnx::Relu', 900], ['Net/Linear[fc2]/onnx::Gemm', 4950]]), (923328, [['Net/Linear[fc1]/onnx::Gemm', 917504], ['Net/ReLU[relu]/onnx::Relu', 896], ['Net/Linear[fc2]/onnx::Gemm', 4928]]), (919206, [['Net/Linear[fc1]/onnx::Gemm', 913408], ['Net/ReLU[relu]/onnx::Relu', 892], ['Net/Linear[fc2]/onnx::Gemm', 4906]]), (915084, [['Net/Linear[fc1]/onnx::Gemm', 909312], ['Net/ReLU[relu]/onnx::Relu', 888], ['Net/Linear[fc2]/onnx::Gemm', 4884]]), (910962, [['Net/Linear[fc1]/onnx::Gemm', 905216], ['Net/ReLU[relu]/onnx::Relu', 884], ['Net/Linear[fc2]/onnx::Gemm', 4862]]), (906840, [['Net/Linear[fc1]/onnx::Gemm', 901120], ['Net/ReLU[relu]/onnx::Relu', 880], ['Net/Linear[fc2]/onnx::Gemm', 4840]]), (902718, [['Net/Linear[fc1]/onnx::Gemm', 897024], ['Net/ReLU[relu]/onnx::Relu', 876], ['Net/Linear[fc2]/onnx::Gemm', 4818]]), (898596, [['Net/Linear[fc1]/onnx::Gemm', 892928], ['Net/ReLU[relu]/onnx::Relu', 872], ['Net/Linear[fc2]/onnx::Gemm', 4796]]), (894474, [['Net/Linear[fc1]/onnx::Gemm', 888832], ['Net/ReLU[relu]/onnx::Relu', 868], ['Net/Linear[fc2]/onnx::Gemm', 4774]]), (890352, [['Net/Linear[fc1]/onnx::Gemm', 884736], ['Net/ReLU[relu]/onnx::Relu', 864], ['Net/Linear[fc2]/onnx::Gemm', 4752]]), (886230, [['Net/Linear[fc1]/onnx::Gemm', 880640], ['Net/ReLU[relu]/onnx::Relu', 860], ['Net/Linear[fc2]/onnx::Gemm', 4730]]), (882108, [['Net/Linear[fc1]/onnx::Gemm', 876544], ['Net/ReLU[relu]/onnx::Relu', 856], ['Net/Linear[fc2]/onnx::Gemm', 4708]]), (877986, [['Net/Linear[fc1]/onnx::Gemm', 872448], ['Net/ReLU[relu]/onnx::Relu', 852], ['Net/Linear[fc2]/onnx::Gemm', 4686]]), (873864, [['Net/Linear[fc1]/onnx::Gemm', 868352], ['Net/ReLU[relu]/onnx::Relu', 848], ['Net/Linear[fc2]/onnx::Gemm', 4664]]), (869742, [['Net/Linear[fc1]/onnx::Gemm', 864256], ['Net/ReLU[relu]/onnx::Relu', 844], ['Net/Linear[fc2]/onnx::Gemm', 4642]]), (865620, [['Net/Linear[fc1]/onnx::Gemm', 860160], ['Net/ReLU[relu]/onnx::Relu', 840], ['Net/Linear[fc2]/onnx::Gemm', 4620]]), (861498, [['Net/Linear[fc1]/onnx::Gemm', 856064], ['Net/ReLU[relu]/onnx::Relu', 836], ['Net/Linear[fc2]/onnx::Gemm', 4598]]), (857376, [['Net/Linear[fc1]/onnx::Gemm', 851968], ['Net/ReLU[relu]/onnx::Relu', 832], ['Net/Linear[fc2]/onnx::Gemm', 4576]]), (853254, [['Net/Linear[fc1]/onnx::Gemm', 847872], ['Net/ReLU[relu]/onnx::Relu', 828], ['Net/Linear[fc2]/onnx::Gemm', 4554]]), (849132, [['Net/Linear[fc1]/onnx::Gemm', 843776], ['Net/ReLU[relu]/onnx::Relu', 824], ['Net/Linear[fc2]/onnx::Gemm', 4532]]), (845010, [['Net/Linear[fc1]/onnx::Gemm', 839680], ['Net/ReLU[relu]/onnx::Relu', 820], ['Net/Linear[fc2]/onnx::Gemm', 4510]]), (840888, [['Net/Linear[fc1]/onnx::Gemm', 835584], ['Net/ReLU[relu]/onnx::Relu', 816], ['Net/Linear[fc2]/onnx::Gemm', 4488]]), (836766, [['Net/Linear[fc1]/onnx::Gemm', 831488], ['Net/ReLU[relu]/onnx::Relu', 812], ['Net/Linear[fc2]/onnx::Gemm', 4466]]), (832644, [['Net/Linear[fc1]/onnx::Gemm', 827392], ['Net/ReLU[relu]/onnx::Relu', 808], ['Net/Linear[fc2]/onnx::Gemm', 4444]]), (828522, [['Net/Linear[fc1]/onnx::Gemm', 823296], ['Net/ReLU[relu]/onnx::Relu', 804], ['Net/Linear[fc2]/onnx::Gemm', 4422]]), (824400, [['Net/Linear[fc1]/onnx::Gemm', 819200], ['Net/ReLU[relu]/onnx::Relu', 800], ['Net/Linear[fc2]/onnx::Gemm', 4400]]), (820278, [['Net/Linear[fc1]/onnx::Gemm', 815104], ['Net/ReLU[relu]/onnx::Relu', 796], ['Net/Linear[fc2]/onnx::Gemm', 4378]]), (816156, [['Net/Linear[fc1]/onnx::Gemm', 811008], ['Net/ReLU[relu]/onnx::Relu', 792], ['Net/Linear[fc2]/onnx::Gemm', 4356]]), (812034, [['Net/Linear[fc1]/onnx::Gemm', 806912], ['Net/ReLU[relu]/onnx::Relu', 788], ['Net/Linear[fc2]/onnx::Gemm', 4334]]), (807912, [['Net/Linear[fc1]/onnx::Gemm', 802816], ['Net/ReLU[relu]/onnx::Relu', 784], ['Net/Linear[fc2]/onnx::Gemm', 4312]]), (803790, [['Net/Linear[fc1]/onnx::Gemm', 798720], ['Net/ReLU[relu]/onnx::Relu', 780], ['Net/Linear[fc2]/onnx::Gemm', 4290]]), (799668, [['Net/Linear[fc1]/onnx::Gemm', 794624], ['Net/ReLU[relu]/onnx::Relu', 776], ['Net/Linear[fc2]/onnx::Gemm', 4268]]), (795546, [['Net/Linear[fc1]/onnx::Gemm', 790528], ['Net/ReLU[relu]/onnx::Relu', 772], ['Net/Linear[fc2]/onnx::Gemm', 4246]]), (791424, [['Net/Linear[fc1]/onnx::Gemm', 786432], ['Net/ReLU[relu]/onnx::Relu', 768], ['Net/Linear[fc2]/onnx::Gemm', 4224]]), (787302, [['Net/Linear[fc1]/onnx::Gemm', 782336], ['Net/ReLU[relu]/onnx::Relu', 764], ['Net/Linear[fc2]/onnx::Gemm', 4202]]), (783180, [['Net/Linear[fc1]/onnx::Gemm', 778240], ['Net/ReLU[relu]/onnx::Relu', 760], ['Net/Linear[fc2]/onnx::Gemm', 4180]]), (779058, [['Net/Linear[fc1]/onnx::Gemm', 774144], ['Net/ReLU[relu]/onnx::Relu', 756], ['Net/Linear[fc2]/onnx::Gemm', 4158]]), (774936, [['Net/Linear[fc1]/onnx::Gemm', 770048], ['Net/ReLU[relu]/onnx::Relu', 752], ['Net/Linear[fc2]/onnx::Gemm', 4136]]), (770814, [['Net/Linear[fc1]/onnx::Gemm', 765952], ['Net/ReLU[relu]/onnx::Relu', 748], ['Net/Linear[fc2]/onnx::Gemm', 4114]]), (766692, [['Net/Linear[fc1]/onnx::Gemm', 761856], ['Net/ReLU[relu]/onnx::Relu', 744], ['Net/Linear[fc2]/onnx::Gemm', 4092]]), (762570, [['Net/Linear[fc1]/onnx::Gemm', 757760], ['Net/ReLU[relu]/onnx::Relu', 740], ['Net/Linear[fc2]/onnx::Gemm', 4070]]), (758448, [['Net/Linear[fc1]/onnx::Gemm', 753664], ['Net/ReLU[relu]/onnx::Relu', 736], ['Net/Linear[fc2]/onnx::Gemm', 4048]]), (754326, [['Net/Linear[fc1]/onnx::Gemm', 749568], ['Net/ReLU[relu]/onnx::Relu', 732], ['Net/Linear[fc2]/onnx::Gemm', 4026]]), (750204, [['Net/Linear[fc1]/onnx::Gemm', 745472], ['Net/ReLU[relu]/onnx::Relu', 728], ['Net/Linear[fc2]/onnx::Gemm', 4004]]), (746082, [['Net/Linear[fc1]/onnx::Gemm', 741376], ['Net/ReLU[relu]/onnx::Relu', 724], ['Net/Linear[fc2]/onnx::Gemm', 3982]]), (741960, [['Net/Linear[fc1]/onnx::Gemm', 737280], ['Net/ReLU[relu]/onnx::Relu', 720], ['Net/Linear[fc2]/onnx::Gemm', 3960]]), (737838, [['Net/Linear[fc1]/onnx::Gemm', 733184], ['Net/ReLU[relu]/onnx::Relu', 716], ['Net/Linear[fc2]/onnx::Gemm', 3938]]), (733716, [['Net/Linear[fc1]/onnx::Gemm', 729088], ['Net/ReLU[relu]/onnx::Relu', 712], ['Net/Linear[fc2]/onnx::Gemm', 3916]]), (729594, [['Net/Linear[fc1]/onnx::Gemm', 724992], ['Net/ReLU[relu]/onnx::Relu', 708], ['Net/Linear[fc2]/onnx::Gemm', 3894]]), (725472, [['Net/Linear[fc1]/onnx::Gemm', 720896], ['Net/ReLU[relu]/onnx::Relu', 704], ['Net/Linear[fc2]/onnx::Gemm', 3872]]), (721350, [['Net/Linear[fc1]/onnx::Gemm', 716800], ['Net/ReLU[relu]/onnx::Relu', 700], ['Net/Linear[fc2]/onnx::Gemm', 3850]]), (717228, [['Net/Linear[fc1]/onnx::Gemm', 712704], ['Net/ReLU[relu]/onnx::Relu', 696], ['Net/Linear[fc2]/onnx::Gemm', 3828]]), (713106, [['Net/Linear[fc1]/onnx::Gemm', 708608], ['Net/ReLU[relu]/onnx::Relu', 692], ['Net/Linear[fc2]/onnx::Gemm', 3806]]), (708984, [['Net/Linear[fc1]/onnx::Gemm', 704512], ['Net/ReLU[relu]/onnx::Relu', 688], ['Net/Linear[fc2]/onnx::Gemm', 3784]]), (704862, [['Net/Linear[fc1]/onnx::Gemm', 700416], ['Net/ReLU[relu]/onnx::Relu', 684], ['Net/Linear[fc2]/onnx::Gemm', 3762]]), (700740, [['Net/Linear[fc1]/onnx::Gemm', 696320], ['Net/ReLU[relu]/onnx::Relu', 680], ['Net/Linear[fc2]/onnx::Gemm', 3740]]), (696618, [['Net/Linear[fc1]/onnx::Gemm', 692224], ['Net/ReLU[relu]/onnx::Relu', 676], ['Net/Linear[fc2]/onnx::Gemm', 3718]]), (692496, [['Net/Linear[fc1]/onnx::Gemm', 688128], ['Net/ReLU[relu]/onnx::Relu', 672], ['Net/Linear[fc2]/onnx::Gemm', 3696]]), (688374, [['Net/Linear[fc1]/onnx::Gemm', 684032], ['Net/ReLU[relu]/onnx::Relu', 668], ['Net/Linear[fc2]/onnx::Gemm', 3674]]), (684252, [['Net/Linear[fc1]/onnx::Gemm', 679936], ['Net/ReLU[relu]/onnx::Relu', 664], ['Net/Linear[fc2]/onnx::Gemm', 3652]]), (680130, [['Net/Linear[fc1]/onnx::Gemm', 675840], ['Net/ReLU[relu]/onnx::Relu', 660], ['Net/Linear[fc2]/onnx::Gemm', 3630]]), (676008, [['Net/Linear[fc1]/onnx::Gemm', 671744], ['Net/ReLU[relu]/onnx::Relu', 656], ['Net/Linear[fc2]/onnx::Gemm', 3608]]), (671886, [['Net/Linear[fc1]/onnx::Gemm', 667648], ['Net/ReLU[relu]/onnx::Relu', 652], ['Net/Linear[fc2]/onnx::Gemm', 3586]]), (667764, [['Net/Linear[fc1]/onnx::Gemm', 663552], ['Net/ReLU[relu]/onnx::Relu', 648], ['Net/Linear[fc2]/onnx::Gemm', 3564]]), (663642, [['Net/Linear[fc1]/onnx::Gemm', 659456], ['Net/ReLU[relu]/onnx::Relu', 644], ['Net/Linear[fc2]/onnx::Gemm', 3542]]), (659520, [['Net/Linear[fc1]/onnx::Gemm', 655360], ['Net/ReLU[relu]/onnx::Relu', 640], ['Net/Linear[fc2]/onnx::Gemm', 3520]]), (655398, [['Net/Linear[fc1]/onnx::Gemm', 651264], ['Net/ReLU[relu]/onnx::Relu', 636], ['Net/Linear[fc2]/onnx::Gemm', 3498]]), (651276, [['Net/Linear[fc1]/onnx::Gemm', 647168], ['Net/ReLU[relu]/onnx::Relu', 632], ['Net/Linear[fc2]/onnx::Gemm', 3476]]), (647154, [['Net/Linear[fc1]/onnx::Gemm', 643072], ['Net/ReLU[relu]/onnx::Relu', 628], ['Net/Linear[fc2]/onnx::Gemm', 3454]]), (643032, [['Net/Linear[fc1]/onnx::Gemm', 638976], ['Net/ReLU[relu]/onnx::Relu', 624], ['Net/Linear[fc2]/onnx::Gemm', 3432]]), (638910, [['Net/Linear[fc1]/onnx::Gemm', 634880], ['Net/ReLU[relu]/onnx::Relu', 620], ['Net/Linear[fc2]/onnx::Gemm', 3410]]), (634788, [['Net/Linear[fc1]/onnx::Gemm', 630784], ['Net/ReLU[relu]/onnx::Relu', 616], ['Net/Linear[fc2]/onnx::Gemm', 3388]]), (630666, [['Net/Linear[fc1]/onnx::Gemm', 626688], ['Net/ReLU[relu]/onnx::Relu', 612], ['Net/Linear[fc2]/onnx::Gemm', 3366]]), (626544, [['Net/Linear[fc1]/onnx::Gemm', 622592], ['Net/ReLU[relu]/onnx::Relu', 608], ['Net/Linear[fc2]/onnx::Gemm', 3344]]), (622422, [['Net/Linear[fc1]/onnx::Gemm', 618496], ['Net/ReLU[relu]/onnx::Relu', 604], ['Net/Linear[fc2]/onnx::Gemm', 3322]]), (618300, [['Net/Linear[fc1]/onnx::Gemm', 614400], ['Net/ReLU[relu]/onnx::Relu', 600], ['Net/Linear[fc2]/onnx::Gemm', 3300]]), (614178, [['Net/Linear[fc1]/onnx::Gemm', 610304], ['Net/ReLU[relu]/onnx::Relu', 596], ['Net/Linear[fc2]/onnx::Gemm', 3278]]), (610056, [['Net/Linear[fc1]/onnx::Gemm', 606208], ['Net/ReLU[relu]/onnx::Relu', 592], ['Net/Linear[fc2]/onnx::Gemm', 3256]]), (605934, [['Net/Linear[fc1]/onnx::Gemm', 602112], ['Net/ReLU[relu]/onnx::Relu', 588], ['Net/Linear[fc2]/onnx::Gemm', 3234]]), (601812, [['Net/Linear[fc1]/onnx::Gemm', 598016], ['Net/ReLU[relu]/onnx::Relu', 584], ['Net/Linear[fc2]/onnx::Gemm', 3212]]), (597690, [['Net/Linear[fc1]/onnx::Gemm', 593920], ['Net/ReLU[relu]/onnx::Relu', 580], ['Net/Linear[fc2]/onnx::Gemm', 3190]]), (593568, [['Net/Linear[fc1]/onnx::Gemm', 589824], ['Net/ReLU[relu]/onnx::Relu', 576], ['Net/Linear[fc2]/onnx::Gemm', 3168]]), (589446, [['Net/Linear[fc1]/onnx::Gemm', 585728], ['Net/ReLU[relu]/onnx::Relu', 572], ['Net/Linear[fc2]/onnx::Gemm', 3146]]), (585324, [['Net/Linear[fc1]/onnx::Gemm', 581632], ['Net/ReLU[relu]/onnx::Relu', 568], ['Net/Linear[fc2]/onnx::Gemm', 3124]]), (581202, [['Net/Linear[fc1]/onnx::Gemm', 577536], ['Net/ReLU[relu]/onnx::Relu', 564], ['Net/Linear[fc2]/onnx::Gemm', 3102]]), (577080, [['Net/Linear[fc1]/onnx::Gemm', 573440], ['Net/ReLU[relu]/onnx::Relu', 560], ['Net/Linear[fc2]/onnx::Gemm', 3080]]), (572958, [['Net/Linear[fc1]/onnx::Gemm', 569344], ['Net/ReLU[relu]/onnx::Relu', 556], ['Net/Linear[fc2]/onnx::Gemm', 3058]]), (568836, [['Net/Linear[fc1]/onnx::Gemm', 565248], ['Net/ReLU[relu]/onnx::Relu', 552], ['Net/Linear[fc2]/onnx::Gemm', 3036]]), (564714, [['Net/Linear[fc1]/onnx::Gemm', 561152], ['Net/ReLU[relu]/onnx::Relu', 548], ['Net/Linear[fc2]/onnx::Gemm', 3014]]), (560592, [['Net/Linear[fc1]/onnx::Gemm', 557056], ['Net/ReLU[relu]/onnx::Relu', 544], ['Net/Linear[fc2]/onnx::Gemm', 2992]]), (556470, [['Net/Linear[fc1]/onnx::Gemm', 552960], ['Net/ReLU[relu]/onnx::Relu', 540], ['Net/Linear[fc2]/onnx::Gemm', 2970]]), (552348, [['Net/Linear[fc1]/onnx::Gemm', 548864], ['Net/ReLU[relu]/onnx::Relu', 536], ['Net/Linear[fc2]/onnx::Gemm', 2948]]), (548226, [['Net/Linear[fc1]/onnx::Gemm', 544768], ['Net/ReLU[relu]/onnx::Relu', 532], ['Net/Linear[fc2]/onnx::Gemm', 2926]]), (544104, [['Net/Linear[fc1]/onnx::Gemm', 540672], ['Net/ReLU[relu]/onnx::Relu', 528], ['Net/Linear[fc2]/onnx::Gemm', 2904]]), (539982, [['Net/Linear[fc1]/onnx::Gemm', 536576], ['Net/ReLU[relu]/onnx::Relu', 524], ['Net/Linear[fc2]/onnx::Gemm', 2882]]), (535860, [['Net/Linear[fc1]/onnx::Gemm', 532480], ['Net/ReLU[relu]/onnx::Relu', 520], ['Net/Linear[fc2]/onnx::Gemm', 2860]]), (531738, [['Net/Linear[fc1]/onnx::Gemm', 528384], ['Net/ReLU[relu]/onnx::Relu', 516], ['Net/Linear[fc2]/onnx::Gemm', 2838]]), (527616, [['Net/Linear[fc1]/onnx::Gemm', 524288], ['Net/ReLU[relu]/onnx::Relu', 512], ['Net/Linear[fc2]/onnx::Gemm', 2816]]), (523494, [['Net/Linear[fc1]/onnx::Gemm', 520192], ['Net/ReLU[relu]/onnx::Relu', 508], ['Net/Linear[fc2]/onnx::Gemm', 2794]]), (519372, [['Net/Linear[fc1]/onnx::Gemm', 516096], ['Net/ReLU[relu]/onnx::Relu', 504], ['Net/Linear[fc2]/onnx::Gemm', 2772]]), (515250, [['Net/Linear[fc1]/onnx::Gemm', 512000], ['Net/ReLU[relu]/onnx::Relu', 500], ['Net/Linear[fc2]/onnx::Gemm', 2750]]), (511128, [['Net/Linear[fc1]/onnx::Gemm', 507904], ['Net/ReLU[relu]/onnx::Relu', 496], ['Net/Linear[fc2]/onnx::Gemm', 2728]]), (507006, [['Net/Linear[fc1]/onnx::Gemm', 503808], ['Net/ReLU[relu]/onnx::Relu', 492], ['Net/Linear[fc2]/onnx::Gemm', 2706]]), (502884, [['Net/Linear[fc1]/onnx::Gemm', 499712], ['Net/ReLU[relu]/onnx::Relu', 488], ['Net/Linear[fc2]/onnx::Gemm', 2684]]), (498762, [['Net/Linear[fc1]/onnx::Gemm', 495616], ['Net/ReLU[relu]/onnx::Relu', 484], ['Net/Linear[fc2]/onnx::Gemm', 2662]]), (494640, [['Net/Linear[fc1]/onnx::Gemm', 491520], ['Net/ReLU[relu]/onnx::Relu', 480], ['Net/Linear[fc2]/onnx::Gemm', 2640]]), (490518, [['Net/Linear[fc1]/onnx::Gemm', 487424], ['Net/ReLU[relu]/onnx::Relu', 476], ['Net/Linear[fc2]/onnx::Gemm', 2618]]), (486396, [['Net/Linear[fc1]/onnx::Gemm', 483328], ['Net/ReLU[relu]/onnx::Relu', 472], ['Net/Linear[fc2]/onnx::Gemm', 2596]]), (482274, [['Net/Linear[fc1]/onnx::Gemm', 479232], ['Net/ReLU[relu]/onnx::Relu', 468], ['Net/Linear[fc2]/onnx::Gemm', 2574]]), (478152, [['Net/Linear[fc1]/onnx::Gemm', 475136], ['Net/ReLU[relu]/onnx::Relu', 464], ['Net/Linear[fc2]/onnx::Gemm', 2552]]), (474030, [['Net/Linear[fc1]/onnx::Gemm', 471040], ['Net/ReLU[relu]/onnx::Relu', 460], ['Net/Linear[fc2]/onnx::Gemm', 2530]]), (469908, [['Net/Linear[fc1]/onnx::Gemm', 466944], ['Net/ReLU[relu]/onnx::Relu', 456], ['Net/Linear[fc2]/onnx::Gemm', 2508]]), (465786, [['Net/Linear[fc1]/onnx::Gemm', 462848], ['Net/ReLU[relu]/onnx::Relu', 452], ['Net/Linear[fc2]/onnx::Gemm', 2486]]), (461664, [['Net/Linear[fc1]/onnx::Gemm', 458752], ['Net/ReLU[relu]/onnx::Relu', 448], ['Net/Linear[fc2]/onnx::Gemm', 2464]]), (457542, [['Net/Linear[fc1]/onnx::Gemm', 454656], ['Net/ReLU[relu]/onnx::Relu', 444], ['Net/Linear[fc2]/onnx::Gemm', 2442]]), (453420, [['Net/Linear[fc1]/onnx::Gemm', 450560], ['Net/ReLU[relu]/onnx::Relu', 440], ['Net/Linear[fc2]/onnx::Gemm', 2420]]), (449298, [['Net/Linear[fc1]/onnx::Gemm', 446464], ['Net/ReLU[relu]/onnx::Relu', 436], ['Net/Linear[fc2]/onnx::Gemm', 2398]]), (445176, [['Net/Linear[fc1]/onnx::Gemm', 442368], ['Net/ReLU[relu]/onnx::Relu', 432], ['Net/Linear[fc2]/onnx::Gemm', 2376]]), (441054, [['Net/Linear[fc1]/onnx::Gemm', 438272], ['Net/ReLU[relu]/onnx::Relu', 428], ['Net/Linear[fc2]/onnx::Gemm', 2354]]), (436932, [['Net/Linear[fc1]/onnx::Gemm', 434176], ['Net/ReLU[relu]/onnx::Relu', 424], ['Net/Linear[fc2]/onnx::Gemm', 2332]]), (432810, [['Net/Linear[fc1]/onnx::Gemm', 430080], ['Net/ReLU[relu]/onnx::Relu', 420], ['Net/Linear[fc2]/onnx::Gemm', 2310]]), (428688, [['Net/Linear[fc1]/onnx::Gemm', 425984], ['Net/ReLU[relu]/onnx::Relu', 416], ['Net/Linear[fc2]/onnx::Gemm', 2288]]), (424566, [['Net/Linear[fc1]/onnx::Gemm', 421888], ['Net/ReLU[relu]/onnx::Relu', 412], ['Net/Linear[fc2]/onnx::Gemm', 2266]]), (420444, [['Net/Linear[fc1]/onnx::Gemm', 417792], ['Net/ReLU[relu]/onnx::Relu', 408], ['Net/Linear[fc2]/onnx::Gemm', 2244]]), (416322, [['Net/Linear[fc1]/onnx::Gemm', 413696], ['Net/ReLU[relu]/onnx::Relu', 404], ['Net/Linear[fc2]/onnx::Gemm', 2222]]), (412200, [['Net/Linear[fc1]/onnx::Gemm', 409600], ['Net/ReLU[relu]/onnx::Relu', 400], ['Net/Linear[fc2]/onnx::Gemm', 2200]]), (408078, [['Net/Linear[fc1]/onnx::Gemm', 405504], ['Net/ReLU[relu]/onnx::Relu', 396], ['Net/Linear[fc2]/onnx::Gemm', 2178]]), (403956, [['Net/Linear[fc1]/onnx::Gemm', 401408], ['Net/ReLU[relu]/onnx::Relu', 392], ['Net/Linear[fc2]/onnx::Gemm', 2156]]), (399834, [['Net/Linear[fc1]/onnx::Gemm', 397312], ['Net/ReLU[relu]/onnx::Relu', 388], ['Net/Linear[fc2]/onnx::Gemm', 2134]]), (395712, [['Net/Linear[fc1]/onnx::Gemm', 393216], ['Net/ReLU[relu]/onnx::Relu', 384], ['Net/Linear[fc2]/onnx::Gemm', 2112]]), (391590, [['Net/Linear[fc1]/onnx::Gemm', 389120], ['Net/ReLU[relu]/onnx::Relu', 380], ['Net/Linear[fc2]/onnx::Gemm', 2090]]), (387468, [['Net/Linear[fc1]/onnx::Gemm', 385024], ['Net/ReLU[relu]/onnx::Relu', 376], ['Net/Linear[fc2]/onnx::Gemm', 2068]]), (383346, [['Net/Linear[fc1]/onnx::Gemm', 380928], ['Net/ReLU[relu]/onnx::Relu', 372], ['Net/Linear[fc2]/onnx::Gemm', 2046]]), (379224, [['Net/Linear[fc1]/onnx::Gemm', 376832], ['Net/ReLU[relu]/onnx::Relu', 368], ['Net/Linear[fc2]/onnx::Gemm', 2024]]), (375102, [['Net/Linear[fc1]/onnx::Gemm', 372736], ['Net/ReLU[relu]/onnx::Relu', 364], ['Net/Linear[fc2]/onnx::Gemm', 2002]]), (370980, [['Net/Linear[fc1]/onnx::Gemm', 368640], ['Net/ReLU[relu]/onnx::Relu', 360], ['Net/Linear[fc2]/onnx::Gemm', 1980]]), (366858, [['Net/Linear[fc1]/onnx::Gemm', 364544], ['Net/ReLU[relu]/onnx::Relu', 356], ['Net/Linear[fc2]/onnx::Gemm', 1958]]), (362736, [['Net/Linear[fc1]/onnx::Gemm', 360448], ['Net/ReLU[relu]/onnx::Relu', 352], ['Net/Linear[fc2]/onnx::Gemm', 1936]]), (358614, [['Net/Linear[fc1]/onnx::Gemm', 356352], ['Net/ReLU[relu]/onnx::Relu', 348], ['Net/Linear[fc2]/onnx::Gemm', 1914]]), (354492, [['Net/Linear[fc1]/onnx::Gemm', 352256], ['Net/ReLU[relu]/onnx::Relu', 344], ['Net/Linear[fc2]/onnx::Gemm', 1892]]), (350370, [['Net/Linear[fc1]/onnx::Gemm', 348160], ['Net/ReLU[relu]/onnx::Relu', 340], ['Net/Linear[fc2]/onnx::Gemm', 1870]]), (346248, [['Net/Linear[fc1]/onnx::Gemm', 344064], ['Net/ReLU[relu]/onnx::Relu', 336], ['Net/Linear[fc2]/onnx::Gemm', 1848]]), (342126, [['Net/Linear[fc1]/onnx::Gemm', 339968], ['Net/ReLU[relu]/onnx::Relu', 332], ['Net/Linear[fc2]/onnx::Gemm', 1826]]), (338004, [['Net/Linear[fc1]/onnx::Gemm', 335872], ['Net/ReLU[relu]/onnx::Relu', 328], ['Net/Linear[fc2]/onnx::Gemm', 1804]]), (333882, [['Net/Linear[fc1]/onnx::Gemm', 331776], ['Net/ReLU[relu]/onnx::Relu', 324], ['Net/Linear[fc2]/onnx::Gemm', 1782]]), (329760, [['Net/Linear[fc1]/onnx::Gemm', 327680], ['Net/ReLU[relu]/onnx::Relu', 320], ['Net/Linear[fc2]/onnx::Gemm', 1760]]), (325638, [['Net/Linear[fc1]/onnx::Gemm', 323584], ['Net/ReLU[relu]/onnx::Relu', 316], ['Net/Linear[fc2]/onnx::Gemm', 1738]]), (321516, [['Net/Linear[fc1]/onnx::Gemm', 319488], ['Net/ReLU[relu]/onnx::Relu', 312], ['Net/Linear[fc2]/onnx::Gemm', 1716]]), (317394, [['Net/Linear[fc1]/onnx::Gemm', 315392], ['Net/ReLU[relu]/onnx::Relu', 308], ['Net/Linear[fc2]/onnx::Gemm', 1694]]), (313272, [['Net/Linear[fc1]/onnx::Gemm', 311296], ['Net/ReLU[relu]/onnx::Relu', 304], ['Net/Linear[fc2]/onnx::Gemm', 1672]]), (309150, [['Net/Linear[fc1]/onnx::Gemm', 307200], ['Net/ReLU[relu]/onnx::Relu', 300], ['Net/Linear[fc2]/onnx::Gemm', 1650]]), (305028, [['Net/Linear[fc1]/onnx::Gemm', 303104], ['Net/ReLU[relu]/onnx::Relu', 296], ['Net/Linear[fc2]/onnx::Gemm', 1628]]), (300906, [['Net/Linear[fc1]/onnx::Gemm', 299008], ['Net/ReLU[relu]/onnx::Relu', 292], ['Net/Linear[fc2]/onnx::Gemm', 1606]]), (296784, [['Net/Linear[fc1]/onnx::Gemm', 294912], ['Net/ReLU[relu]/onnx::Relu', 288], ['Net/Linear[fc2]/onnx::Gemm', 1584]]), (292662, [['Net/Linear[fc1]/onnx::Gemm', 290816], ['Net/ReLU[relu]/onnx::Relu', 284], ['Net/Linear[fc2]/onnx::Gemm', 1562]]), (288540, [['Net/Linear[fc1]/onnx::Gemm', 286720], ['Net/ReLU[relu]/onnx::Relu', 280], ['Net/Linear[fc2]/onnx::Gemm', 1540]]), (284418, [['Net/Linear[fc1]/onnx::Gemm', 282624], ['Net/ReLU[relu]/onnx::Relu', 276], ['Net/Linear[fc2]/onnx::Gemm', 1518]]), (280296, [['Net/Linear[fc1]/onnx::Gemm', 278528], ['Net/ReLU[relu]/onnx::Relu', 272], ['Net/Linear[fc2]/onnx::Gemm', 1496]]), (276174, [['Net/Linear[fc1]/onnx::Gemm', 274432], ['Net/ReLU[relu]/onnx::Relu', 268], ['Net/Linear[fc2]/onnx::Gemm', 1474]]), (272052, [['Net/Linear[fc1]/onnx::Gemm', 270336], ['Net/ReLU[relu]/onnx::Relu', 264], ['Net/Linear[fc2]/onnx::Gemm', 1452]]), (267930, [['Net/Linear[fc1]/onnx::Gemm', 266240], ['Net/ReLU[relu]/onnx::Relu', 260], ['Net/Linear[fc2]/onnx::Gemm', 1430]]), (263808, [['Net/Linear[fc1]/onnx::Gemm', 262144], ['Net/ReLU[relu]/onnx::Relu', 256], ['Net/Linear[fc2]/onnx::Gemm', 1408]]), (259686, [['Net/Linear[fc1]/onnx::Gemm', 258048], ['Net/ReLU[relu]/onnx::Relu', 252], ['Net/Linear[fc2]/onnx::Gemm', 1386]]), (255564, [['Net/Linear[fc1]/onnx::Gemm', 253952], ['Net/ReLU[relu]/onnx::Relu', 248], ['Net/Linear[fc2]/onnx::Gemm', 1364]]), (251442, [['Net/Linear[fc1]/onnx::Gemm', 249856], ['Net/ReLU[relu]/onnx::Relu', 244], ['Net/Linear[fc2]/onnx::Gemm', 1342]]), (247320, [['Net/Linear[fc1]/onnx::Gemm', 245760], ['Net/ReLU[relu]/onnx::Relu', 240], ['Net/Linear[fc2]/onnx::Gemm', 1320]]), (243198, [['Net/Linear[fc1]/onnx::Gemm', 241664], ['Net/ReLU[relu]/onnx::Relu', 236], ['Net/Linear[fc2]/onnx::Gemm', 1298]]), (239076, [['Net/Linear[fc1]/onnx::Gemm', 237568], ['Net/ReLU[relu]/onnx::Relu', 232], ['Net/Linear[fc2]/onnx::Gemm', 1276]]), (234954, [['Net/Linear[fc1]/onnx::Gemm', 233472], ['Net/ReLU[relu]/onnx::Relu', 228], ['Net/Linear[fc2]/onnx::Gemm', 1254]]), (230832, [['Net/Linear[fc1]/onnx::Gemm', 229376], ['Net/ReLU[relu]/onnx::Relu', 224], ['Net/Linear[fc2]/onnx::Gemm', 1232]]), (226710, [['Net/Linear[fc1]/onnx::Gemm', 225280], ['Net/ReLU[relu]/onnx::Relu', 220], ['Net/Linear[fc2]/onnx::Gemm', 1210]]), (222588, [['Net/Linear[fc1]/onnx::Gemm', 221184], ['Net/ReLU[relu]/onnx::Relu', 216], ['Net/Linear[fc2]/onnx::Gemm', 1188]]), (218466, [['Net/Linear[fc1]/onnx::Gemm', 217088], ['Net/ReLU[relu]/onnx::Relu', 212], ['Net/Linear[fc2]/onnx::Gemm', 1166]]), (214344, [['Net/Linear[fc1]/onnx::Gemm', 212992], ['Net/ReLU[relu]/onnx::Relu', 208], ['Net/Linear[fc2]/onnx::Gemm', 1144]]), (210222, [['Net/Linear[fc1]/onnx::Gemm', 208896], ['Net/ReLU[relu]/onnx::Relu', 204], ['Net/Linear[fc2]/onnx::Gemm', 1122]]), (206100, [['Net/Linear[fc1]/onnx::Gemm', 204800], ['Net/ReLU[relu]/onnx::Relu', 200], ['Net/Linear[fc2]/onnx::Gemm', 1100]]), (201978, [['Net/Linear[fc1]/onnx::Gemm', 200704], ['Net/ReLU[relu]/onnx::Relu', 196], ['Net/Linear[fc2]/onnx::Gemm', 1078]]), (197856, [['Net/Linear[fc1]/onnx::Gemm', 196608], ['Net/ReLU[relu]/onnx::Relu', 192], ['Net/Linear[fc2]/onnx::Gemm', 1056]]), (193734, [['Net/Linear[fc1]/onnx::Gemm', 192512], ['Net/ReLU[relu]/onnx::Relu', 188], ['Net/Linear[fc2]/onnx::Gemm', 1034]]), (189612, [['Net/Linear[fc1]/onnx::Gemm', 188416], ['Net/ReLU[relu]/onnx::Relu', 184], ['Net/Linear[fc2]/onnx::Gemm', 1012]]), (185490, [['Net/Linear[fc1]/onnx::Gemm', 184320], ['Net/ReLU[relu]/onnx::Relu', 180], ['Net/Linear[fc2]/onnx::Gemm', 990]]), (181368, [['Net/Linear[fc1]/onnx::Gemm', 180224], ['Net/ReLU[relu]/onnx::Relu', 176], ['Net/Linear[fc2]/onnx::Gemm', 968]]), (177246, [['Net/Linear[fc1]/onnx::Gemm', 176128], ['Net/ReLU[relu]/onnx::Relu', 172], ['Net/Linear[fc2]/onnx::Gemm', 946]]), (173124, [['Net/Linear[fc1]/onnx::Gemm', 172032], ['Net/ReLU[relu]/onnx::Relu', 168], ['Net/Linear[fc2]/onnx::Gemm', 924]]), (169002, [['Net/Linear[fc1]/onnx::Gemm', 167936], ['Net/ReLU[relu]/onnx::Relu', 164], ['Net/Linear[fc2]/onnx::Gemm', 902]]), (164880, [['Net/Linear[fc1]/onnx::Gemm', 163840], ['Net/ReLU[relu]/onnx::Relu', 160], ['Net/Linear[fc2]/onnx::Gemm', 880]]), (160758, [['Net/Linear[fc1]/onnx::Gemm', 159744], ['Net/ReLU[relu]/onnx::Relu', 156], ['Net/Linear[fc2]/onnx::Gemm', 858]]), (156636, [['Net/Linear[fc1]/onnx::Gemm', 155648], ['Net/ReLU[relu]/onnx::Relu', 152], ['Net/Linear[fc2]/onnx::Gemm', 836]]), (152514, [['Net/Linear[fc1]/onnx::Gemm', 151552], ['Net/ReLU[relu]/onnx::Relu', 148], ['Net/Linear[fc2]/onnx::Gemm', 814]]), (148392, [['Net/Linear[fc1]/onnx::Gemm', 147456], ['Net/ReLU[relu]/onnx::Relu', 144], ['Net/Linear[fc2]/onnx::Gemm', 792]]), (144270, [['Net/Linear[fc1]/onnx::Gemm', 143360], ['Net/ReLU[relu]/onnx::Relu', 140], ['Net/Linear[fc2]/onnx::Gemm', 770]]), (140148, [['Net/Linear[fc1]/onnx::Gemm', 139264], ['Net/ReLU[relu]/onnx::Relu', 136], ['Net/Linear[fc2]/onnx::Gemm', 748]]), (136026, [['Net/Linear[fc1]/onnx::Gemm', 135168], ['Net/ReLU[relu]/onnx::Relu', 132], ['Net/Linear[fc2]/onnx::Gemm', 726]]), (131904, [['Net/Linear[fc1]/onnx::Gemm', 131072], ['Net/ReLU[relu]/onnx::Relu', 128], ['Net/Linear[fc2]/onnx::Gemm', 704]]), (127782, [['Net/Linear[fc1]/onnx::Gemm', 126976], ['Net/ReLU[relu]/onnx::Relu', 124], ['Net/Linear[fc2]/onnx::Gemm', 682]]), (123660, [['Net/Linear[fc1]/onnx::Gemm', 122880], ['Net/ReLU[relu]/onnx::Relu', 120], ['Net/Linear[fc2]/onnx::Gemm', 660]]), (119538, [['Net/Linear[fc1]/onnx::Gemm', 118784], ['Net/ReLU[relu]/onnx::Relu', 116], ['Net/Linear[fc2]/onnx::Gemm', 638]]), (115416, [['Net/Linear[fc1]/onnx::Gemm', 114688], ['Net/ReLU[relu]/onnx::Relu', 112], ['Net/Linear[fc2]/onnx::Gemm', 616]]), (111294, [['Net/Linear[fc1]/onnx::Gemm', 110592], ['Net/ReLU[relu]/onnx::Relu', 108], ['Net/Linear[fc2]/onnx::Gemm', 594]]), (107172, [['Net/Linear[fc1]/onnx::Gemm', 106496], ['Net/ReLU[relu]/onnx::Relu', 104], ['Net/Linear[fc2]/onnx::Gemm', 572]]), (103050, [['Net/Linear[fc1]/onnx::Gemm', 102400], ['Net/ReLU[relu]/onnx::Relu', 100], ['Net/Linear[fc2]/onnx::Gemm', 550]]), (98928, [['Net/Linear[fc1]/onnx::Gemm', 98304], ['Net/ReLU[relu]/onnx::Relu', 96], ['Net/Linear[fc2]/onnx::Gemm', 528]]), (94806, [['Net/Linear[fc1]/onnx::Gemm', 94208], ['Net/ReLU[relu]/onnx::Relu', 92], ['Net/Linear[fc2]/onnx::Gemm', 506]]), (90684, [['Net/Linear[fc1]/onnx::Gemm', 90112], ['Net/ReLU[relu]/onnx::Relu', 88], ['Net/Linear[fc2]/onnx::Gemm', 484]]), (86562, [['Net/Linear[fc1]/onnx::Gemm', 86016], ['Net/ReLU[relu]/onnx::Relu', 84], ['Net/Linear[fc2]/onnx::Gemm', 462]]), (82440, [['Net/Linear[fc1]/onnx::Gemm', 81920], ['Net/ReLU[relu]/onnx::Relu', 80], ['Net/Linear[fc2]/onnx::Gemm', 440]]), (78318, [['Net/Linear[fc1]/onnx::Gemm', 77824], ['Net/ReLU[relu]/onnx::Relu', 76], ['Net/Linear[fc2]/onnx::Gemm', 418]]), (74196, [['Net/Linear[fc1]/onnx::Gemm', 73728], ['Net/ReLU[relu]/onnx::Relu', 72], ['Net/Linear[fc2]/onnx::Gemm', 396]]), (70074, [['Net/Linear[fc1]/onnx::Gemm', 69632], ['Net/ReLU[relu]/onnx::Relu', 68], ['Net/Linear[fc2]/onnx::Gemm', 374]]), (65952, [['Net/Linear[fc1]/onnx::Gemm', 65536], ['Net/ReLU[relu]/onnx::Relu', 64], ['Net/Linear[fc2]/onnx::Gemm', 352]]), (61830, [['Net/Linear[fc1]/onnx::Gemm', 61440], ['Net/ReLU[relu]/onnx::Relu', 60], ['Net/Linear[fc2]/onnx::Gemm', 330]]), (57708, [['Net/Linear[fc1]/onnx::Gemm', 57344], ['Net/ReLU[relu]/onnx::Relu', 56], ['Net/Linear[fc2]/onnx::Gemm', 308]]), (53586, [['Net/Linear[fc1]/onnx::Gemm', 53248], ['Net/ReLU[relu]/onnx::Relu', 52], ['Net/Linear[fc2]/onnx::Gemm', 286]]), (49464, [['Net/Linear[fc1]/onnx::Gemm', 49152], ['Net/ReLU[relu]/onnx::Relu', 48], ['Net/Linear[fc2]/onnx::Gemm', 264]]), (45342, [['Net/Linear[fc1]/onnx::Gemm', 45056], ['Net/ReLU[relu]/onnx::Relu', 44], ['Net/Linear[fc2]/onnx::Gemm', 242]]), (41220, [['Net/Linear[fc1]/onnx::Gemm', 40960], ['Net/ReLU[relu]/onnx::Relu', 40], ['Net/Linear[fc2]/onnx::Gemm', 220]]), (37098, [['Net/Linear[fc1]/onnx::Gemm', 36864], ['Net/ReLU[relu]/onnx::Relu', 36], ['Net/Linear[fc2]/onnx::Gemm', 198]]), (32976, [['Net/Linear[fc1]/onnx::Gemm', 32768], ['Net/ReLU[relu]/onnx::Relu', 32], ['Net/Linear[fc2]/onnx::Gemm', 176]]), (28854, [['Net/Linear[fc1]/onnx::Gemm', 28672], ['Net/ReLU[relu]/onnx::Relu', 28], ['Net/Linear[fc2]/onnx::Gemm', 154]]), (24732, [['Net/Linear[fc1]/onnx::Gemm', 24576], ['Net/ReLU[relu]/onnx::Relu', 24], ['Net/Linear[fc2]/onnx::Gemm', 132]]), (20610, [['Net/Linear[fc1]/onnx::Gemm', 20480], ['Net/ReLU[relu]/onnx::Relu', 20], ['Net/Linear[fc2]/onnx::Gemm', 110]]), (16488, [['Net/Linear[fc1]/onnx::Gemm', 16384], ['Net/ReLU[relu]/onnx::Relu', 16], ['Net/Linear[fc2]/onnx::Gemm', 88]]), (12366, [['Net/Linear[fc1]/onnx::Gemm', 12288], ['Net/ReLU[relu]/onnx::Relu', 12], ['Net/Linear[fc2]/onnx::Gemm', 66]]), (8244, [['Net/Linear[fc1]/onnx::Gemm', 8192], ['Net/ReLU[relu]/onnx::Relu', 8], ['Net/Linear[fc2]/onnx::Gemm', 44]]), (4122, [['Net/Linear[fc1]/onnx::Gemm', 4096], ['Net/ReLU[relu]/onnx::Relu', 4], ['Net/Linear[fc2]/onnx::Gemm', 22]])]\n",
    "flops_1 = [(1030500, [['Net/Linear[fc1]/onnx::Gemm', 1024000], ['Net/ReLU[relu]/onnx::Relu', 1000], ['Net/Linear[fc2]/onnx::Gemm', 5500]]), (1028439, [['Net/Linear[fc1]/onnx::Gemm', 1021952], ['Net/ReLU[relu]/onnx::Relu', 998], ['Net/Linear[fc2]/onnx::Gemm', 5489]]), (1026378, [['Net/Linear[fc1]/onnx::Gemm', 1019904], ['Net/ReLU[relu]/onnx::Relu', 996], ['Net/Linear[fc2]/onnx::Gemm', 5478]]), (1024317, [['Net/Linear[fc1]/onnx::Gemm', 1017856], ['Net/ReLU[relu]/onnx::Relu', 994], ['Net/Linear[fc2]/onnx::Gemm', 5467]]), (1022256, [['Net/Linear[fc1]/onnx::Gemm', 1015808], ['Net/ReLU[relu]/onnx::Relu', 992], ['Net/Linear[fc2]/onnx::Gemm', 5456]]), (1020195, [['Net/Linear[fc1]/onnx::Gemm', 1013760], ['Net/ReLU[relu]/onnx::Relu', 990], ['Net/Linear[fc2]/onnx::Gemm', 5445]]), (1018134, [['Net/Linear[fc1]/onnx::Gemm', 1011712], ['Net/ReLU[relu]/onnx::Relu', 988], ['Net/Linear[fc2]/onnx::Gemm', 5434]]), (1016073, [['Net/Linear[fc1]/onnx::Gemm', 1009664], ['Net/ReLU[relu]/onnx::Relu', 986], ['Net/Linear[fc2]/onnx::Gemm', 5423]]), (1014012, [['Net/Linear[fc1]/onnx::Gemm', 1007616], ['Net/ReLU[relu]/onnx::Relu', 984], ['Net/Linear[fc2]/onnx::Gemm', 5412]]), (1011951, [['Net/Linear[fc1]/onnx::Gemm', 1005568], ['Net/ReLU[relu]/onnx::Relu', 982], ['Net/Linear[fc2]/onnx::Gemm', 5401]]), (1009890, [['Net/Linear[fc1]/onnx::Gemm', 1003520], ['Net/ReLU[relu]/onnx::Relu', 980], ['Net/Linear[fc2]/onnx::Gemm', 5390]]), (1007829, [['Net/Linear[fc1]/onnx::Gemm', 1001472], ['Net/ReLU[relu]/onnx::Relu', 978], ['Net/Linear[fc2]/onnx::Gemm', 5379]]), (1005768, [['Net/Linear[fc1]/onnx::Gemm', 999424], ['Net/ReLU[relu]/onnx::Relu', 976], ['Net/Linear[fc2]/onnx::Gemm', 5368]]), (1003707, [['Net/Linear[fc1]/onnx::Gemm', 997376], ['Net/ReLU[relu]/onnx::Relu', 974], ['Net/Linear[fc2]/onnx::Gemm', 5357]]), (1001646, [['Net/Linear[fc1]/onnx::Gemm', 995328], ['Net/ReLU[relu]/onnx::Relu', 972], ['Net/Linear[fc2]/onnx::Gemm', 5346]]), (999585, [['Net/Linear[fc1]/onnx::Gemm', 993280], ['Net/ReLU[relu]/onnx::Relu', 970], ['Net/Linear[fc2]/onnx::Gemm', 5335]]), (997524, [['Net/Linear[fc1]/onnx::Gemm', 991232], ['Net/ReLU[relu]/onnx::Relu', 968], ['Net/Linear[fc2]/onnx::Gemm', 5324]]), (995463, [['Net/Linear[fc1]/onnx::Gemm', 989184], ['Net/ReLU[relu]/onnx::Relu', 966], ['Net/Linear[fc2]/onnx::Gemm', 5313]]), (993402, [['Net/Linear[fc1]/onnx::Gemm', 987136], ['Net/ReLU[relu]/onnx::Relu', 964], ['Net/Linear[fc2]/onnx::Gemm', 5302]]), (991341, [['Net/Linear[fc1]/onnx::Gemm', 985088], ['Net/ReLU[relu]/onnx::Relu', 962], ['Net/Linear[fc2]/onnx::Gemm', 5291]]), (989280, [['Net/Linear[fc1]/onnx::Gemm', 983040], ['Net/ReLU[relu]/onnx::Relu', 960], ['Net/Linear[fc2]/onnx::Gemm', 5280]]), (987219, [['Net/Linear[fc1]/onnx::Gemm', 980992], ['Net/ReLU[relu]/onnx::Relu', 958], ['Net/Linear[fc2]/onnx::Gemm', 5269]]), (985158, [['Net/Linear[fc1]/onnx::Gemm', 978944], ['Net/ReLU[relu]/onnx::Relu', 956], ['Net/Linear[fc2]/onnx::Gemm', 5258]]), (983097, [['Net/Linear[fc1]/onnx::Gemm', 976896], ['Net/ReLU[relu]/onnx::Relu', 954], ['Net/Linear[fc2]/onnx::Gemm', 5247]]), (981036, [['Net/Linear[fc1]/onnx::Gemm', 974848], ['Net/ReLU[relu]/onnx::Relu', 952], ['Net/Linear[fc2]/onnx::Gemm', 5236]]), (978975, [['Net/Linear[fc1]/onnx::Gemm', 972800], ['Net/ReLU[relu]/onnx::Relu', 950], ['Net/Linear[fc2]/onnx::Gemm', 5225]]), (976914, [['Net/Linear[fc1]/onnx::Gemm', 970752], ['Net/ReLU[relu]/onnx::Relu', 948], ['Net/Linear[fc2]/onnx::Gemm', 5214]]), (974853, [['Net/Linear[fc1]/onnx::Gemm', 968704], ['Net/ReLU[relu]/onnx::Relu', 946], ['Net/Linear[fc2]/onnx::Gemm', 5203]]), (972792, [['Net/Linear[fc1]/onnx::Gemm', 966656], ['Net/ReLU[relu]/onnx::Relu', 944], ['Net/Linear[fc2]/onnx::Gemm', 5192]]), (970731, [['Net/Linear[fc1]/onnx::Gemm', 964608], ['Net/ReLU[relu]/onnx::Relu', 942], ['Net/Linear[fc2]/onnx::Gemm', 5181]]), (968670, [['Net/Linear[fc1]/onnx::Gemm', 962560], ['Net/ReLU[relu]/onnx::Relu', 940], ['Net/Linear[fc2]/onnx::Gemm', 5170]]), (966609, [['Net/Linear[fc1]/onnx::Gemm', 960512], ['Net/ReLU[relu]/onnx::Relu', 938], ['Net/Linear[fc2]/onnx::Gemm', 5159]]), (964548, [['Net/Linear[fc1]/onnx::Gemm', 958464], ['Net/ReLU[relu]/onnx::Relu', 936], ['Net/Linear[fc2]/onnx::Gemm', 5148]]), (962487, [['Net/Linear[fc1]/onnx::Gemm', 956416], ['Net/ReLU[relu]/onnx::Relu', 934], ['Net/Linear[fc2]/onnx::Gemm', 5137]]), (960426, [['Net/Linear[fc1]/onnx::Gemm', 954368], ['Net/ReLU[relu]/onnx::Relu', 932], ['Net/Linear[fc2]/onnx::Gemm', 5126]]), (958365, [['Net/Linear[fc1]/onnx::Gemm', 952320], ['Net/ReLU[relu]/onnx::Relu', 930], ['Net/Linear[fc2]/onnx::Gemm', 5115]]), (956304, [['Net/Linear[fc1]/onnx::Gemm', 950272], ['Net/ReLU[relu]/onnx::Relu', 928], ['Net/Linear[fc2]/onnx::Gemm', 5104]]), (954243, [['Net/Linear[fc1]/onnx::Gemm', 948224], ['Net/ReLU[relu]/onnx::Relu', 926], ['Net/Linear[fc2]/onnx::Gemm', 5093]]), (952182, [['Net/Linear[fc1]/onnx::Gemm', 946176], ['Net/ReLU[relu]/onnx::Relu', 924], ['Net/Linear[fc2]/onnx::Gemm', 5082]]), (950121, [['Net/Linear[fc1]/onnx::Gemm', 944128], ['Net/ReLU[relu]/onnx::Relu', 922], ['Net/Linear[fc2]/onnx::Gemm', 5071]]), (948060, [['Net/Linear[fc1]/onnx::Gemm', 942080], ['Net/ReLU[relu]/onnx::Relu', 920], ['Net/Linear[fc2]/onnx::Gemm', 5060]]), (945999, [['Net/Linear[fc1]/onnx::Gemm', 940032], ['Net/ReLU[relu]/onnx::Relu', 918], ['Net/Linear[fc2]/onnx::Gemm', 5049]]), (943938, [['Net/Linear[fc1]/onnx::Gemm', 937984], ['Net/ReLU[relu]/onnx::Relu', 916], ['Net/Linear[fc2]/onnx::Gemm', 5038]]), (941877, [['Net/Linear[fc1]/onnx::Gemm', 935936], ['Net/ReLU[relu]/onnx::Relu', 914], ['Net/Linear[fc2]/onnx::Gemm', 5027]]), (939816, [['Net/Linear[fc1]/onnx::Gemm', 933888], ['Net/ReLU[relu]/onnx::Relu', 912], ['Net/Linear[fc2]/onnx::Gemm', 5016]]), (937755, [['Net/Linear[fc1]/onnx::Gemm', 931840], ['Net/ReLU[relu]/onnx::Relu', 910], ['Net/Linear[fc2]/onnx::Gemm', 5005]]), (935694, [['Net/Linear[fc1]/onnx::Gemm', 929792], ['Net/ReLU[relu]/onnx::Relu', 908], ['Net/Linear[fc2]/onnx::Gemm', 4994]]), (933633, [['Net/Linear[fc1]/onnx::Gemm', 927744], ['Net/ReLU[relu]/onnx::Relu', 906], ['Net/Linear[fc2]/onnx::Gemm', 4983]]), (931572, [['Net/Linear[fc1]/onnx::Gemm', 925696], ['Net/ReLU[relu]/onnx::Relu', 904], ['Net/Linear[fc2]/onnx::Gemm', 4972]]), (929511, [['Net/Linear[fc1]/onnx::Gemm', 923648], ['Net/ReLU[relu]/onnx::Relu', 902], ['Net/Linear[fc2]/onnx::Gemm', 4961]]), (927450, [['Net/Linear[fc1]/onnx::Gemm', 921600], ['Net/ReLU[relu]/onnx::Relu', 900], ['Net/Linear[fc2]/onnx::Gemm', 4950]]), (925389, [['Net/Linear[fc1]/onnx::Gemm', 919552], ['Net/ReLU[relu]/onnx::Relu', 898], ['Net/Linear[fc2]/onnx::Gemm', 4939]]), (923328, [['Net/Linear[fc1]/onnx::Gemm', 917504], ['Net/ReLU[relu]/onnx::Relu', 896], ['Net/Linear[fc2]/onnx::Gemm', 4928]]), (921267, [['Net/Linear[fc1]/onnx::Gemm', 915456], ['Net/ReLU[relu]/onnx::Relu', 894], ['Net/Linear[fc2]/onnx::Gemm', 4917]]), (919206, [['Net/Linear[fc1]/onnx::Gemm', 913408], ['Net/ReLU[relu]/onnx::Relu', 892], ['Net/Linear[fc2]/onnx::Gemm', 4906]]), (917145, [['Net/Linear[fc1]/onnx::Gemm', 911360], ['Net/ReLU[relu]/onnx::Relu', 890], ['Net/Linear[fc2]/onnx::Gemm', 4895]]), (915084, [['Net/Linear[fc1]/onnx::Gemm', 909312], ['Net/ReLU[relu]/onnx::Relu', 888], ['Net/Linear[fc2]/onnx::Gemm', 4884]]), (913023, [['Net/Linear[fc1]/onnx::Gemm', 907264], ['Net/ReLU[relu]/onnx::Relu', 886], ['Net/Linear[fc2]/onnx::Gemm', 4873]]), (910962, [['Net/Linear[fc1]/onnx::Gemm', 905216], ['Net/ReLU[relu]/onnx::Relu', 884], ['Net/Linear[fc2]/onnx::Gemm', 4862]]), (908901, [['Net/Linear[fc1]/onnx::Gemm', 903168], ['Net/ReLU[relu]/onnx::Relu', 882], ['Net/Linear[fc2]/onnx::Gemm', 4851]]), (906840, [['Net/Linear[fc1]/onnx::Gemm', 901120], ['Net/ReLU[relu]/onnx::Relu', 880], ['Net/Linear[fc2]/onnx::Gemm', 4840]]), (904779, [['Net/Linear[fc1]/onnx::Gemm', 899072], ['Net/ReLU[relu]/onnx::Relu', 878], ['Net/Linear[fc2]/onnx::Gemm', 4829]]), (902718, [['Net/Linear[fc1]/onnx::Gemm', 897024], ['Net/ReLU[relu]/onnx::Relu', 876], ['Net/Linear[fc2]/onnx::Gemm', 4818]]), (900657, [['Net/Linear[fc1]/onnx::Gemm', 894976], ['Net/ReLU[relu]/onnx::Relu', 874], ['Net/Linear[fc2]/onnx::Gemm', 4807]]), (898596, [['Net/Linear[fc1]/onnx::Gemm', 892928], ['Net/ReLU[relu]/onnx::Relu', 872], ['Net/Linear[fc2]/onnx::Gemm', 4796]]), (896535, [['Net/Linear[fc1]/onnx::Gemm', 890880], ['Net/ReLU[relu]/onnx::Relu', 870], ['Net/Linear[fc2]/onnx::Gemm', 4785]]), (894474, [['Net/Linear[fc1]/onnx::Gemm', 888832], ['Net/ReLU[relu]/onnx::Relu', 868], ['Net/Linear[fc2]/onnx::Gemm', 4774]]), (892413, [['Net/Linear[fc1]/onnx::Gemm', 886784], ['Net/ReLU[relu]/onnx::Relu', 866], ['Net/Linear[fc2]/onnx::Gemm', 4763]]), (890352, [['Net/Linear[fc1]/onnx::Gemm', 884736], ['Net/ReLU[relu]/onnx::Relu', 864], ['Net/Linear[fc2]/onnx::Gemm', 4752]]), (888291, [['Net/Linear[fc1]/onnx::Gemm', 882688], ['Net/ReLU[relu]/onnx::Relu', 862], ['Net/Linear[fc2]/onnx::Gemm', 4741]]), (886230, [['Net/Linear[fc1]/onnx::Gemm', 880640], ['Net/ReLU[relu]/onnx::Relu', 860], ['Net/Linear[fc2]/onnx::Gemm', 4730]]), (884169, [['Net/Linear[fc1]/onnx::Gemm', 878592], ['Net/ReLU[relu]/onnx::Relu', 858], ['Net/Linear[fc2]/onnx::Gemm', 4719]]), (882108, [['Net/Linear[fc1]/onnx::Gemm', 876544], ['Net/ReLU[relu]/onnx::Relu', 856], ['Net/Linear[fc2]/onnx::Gemm', 4708]]), (880047, [['Net/Linear[fc1]/onnx::Gemm', 874496], ['Net/ReLU[relu]/onnx::Relu', 854], ['Net/Linear[fc2]/onnx::Gemm', 4697]]), (877986, [['Net/Linear[fc1]/onnx::Gemm', 872448], ['Net/ReLU[relu]/onnx::Relu', 852], ['Net/Linear[fc2]/onnx::Gemm', 4686]]), (875925, [['Net/Linear[fc1]/onnx::Gemm', 870400], ['Net/ReLU[relu]/onnx::Relu', 850], ['Net/Linear[fc2]/onnx::Gemm', 4675]]), (873864, [['Net/Linear[fc1]/onnx::Gemm', 868352], ['Net/ReLU[relu]/onnx::Relu', 848], ['Net/Linear[fc2]/onnx::Gemm', 4664]]), (871803, [['Net/Linear[fc1]/onnx::Gemm', 866304], ['Net/ReLU[relu]/onnx::Relu', 846], ['Net/Linear[fc2]/onnx::Gemm', 4653]]), (869742, [['Net/Linear[fc1]/onnx::Gemm', 864256], ['Net/ReLU[relu]/onnx::Relu', 844], ['Net/Linear[fc2]/onnx::Gemm', 4642]]), (867681, [['Net/Linear[fc1]/onnx::Gemm', 862208], ['Net/ReLU[relu]/onnx::Relu', 842], ['Net/Linear[fc2]/onnx::Gemm', 4631]]), (865620, [['Net/Linear[fc1]/onnx::Gemm', 860160], ['Net/ReLU[relu]/onnx::Relu', 840], ['Net/Linear[fc2]/onnx::Gemm', 4620]]), (863559, [['Net/Linear[fc1]/onnx::Gemm', 858112], ['Net/ReLU[relu]/onnx::Relu', 838], ['Net/Linear[fc2]/onnx::Gemm', 4609]]), (861498, [['Net/Linear[fc1]/onnx::Gemm', 856064], ['Net/ReLU[relu]/onnx::Relu', 836], ['Net/Linear[fc2]/onnx::Gemm', 4598]]), (859437, [['Net/Linear[fc1]/onnx::Gemm', 854016], ['Net/ReLU[relu]/onnx::Relu', 834], ['Net/Linear[fc2]/onnx::Gemm', 4587]]), (857376, [['Net/Linear[fc1]/onnx::Gemm', 851968], ['Net/ReLU[relu]/onnx::Relu', 832], ['Net/Linear[fc2]/onnx::Gemm', 4576]]), (855315, [['Net/Linear[fc1]/onnx::Gemm', 849920], ['Net/ReLU[relu]/onnx::Relu', 830], ['Net/Linear[fc2]/onnx::Gemm', 4565]]), (853254, [['Net/Linear[fc1]/onnx::Gemm', 847872], ['Net/ReLU[relu]/onnx::Relu', 828], ['Net/Linear[fc2]/onnx::Gemm', 4554]]), (851193, [['Net/Linear[fc1]/onnx::Gemm', 845824], ['Net/ReLU[relu]/onnx::Relu', 826], ['Net/Linear[fc2]/onnx::Gemm', 4543]]), (849132, [['Net/Linear[fc1]/onnx::Gemm', 843776], ['Net/ReLU[relu]/onnx::Relu', 824], ['Net/Linear[fc2]/onnx::Gemm', 4532]]), (847071, [['Net/Linear[fc1]/onnx::Gemm', 841728], ['Net/ReLU[relu]/onnx::Relu', 822], ['Net/Linear[fc2]/onnx::Gemm', 4521]]), (845010, [['Net/Linear[fc1]/onnx::Gemm', 839680], ['Net/ReLU[relu]/onnx::Relu', 820], ['Net/Linear[fc2]/onnx::Gemm', 4510]]), (842949, [['Net/Linear[fc1]/onnx::Gemm', 837632], ['Net/ReLU[relu]/onnx::Relu', 818], ['Net/Linear[fc2]/onnx::Gemm', 4499]]), (840888, [['Net/Linear[fc1]/onnx::Gemm', 835584], ['Net/ReLU[relu]/onnx::Relu', 816], ['Net/Linear[fc2]/onnx::Gemm', 4488]]), (838827, [['Net/Linear[fc1]/onnx::Gemm', 833536], ['Net/ReLU[relu]/onnx::Relu', 814], ['Net/Linear[fc2]/onnx::Gemm', 4477]]), (836766, [['Net/Linear[fc1]/onnx::Gemm', 831488], ['Net/ReLU[relu]/onnx::Relu', 812], ['Net/Linear[fc2]/onnx::Gemm', 4466]]), (834705, [['Net/Linear[fc1]/onnx::Gemm', 829440], ['Net/ReLU[relu]/onnx::Relu', 810], ['Net/Linear[fc2]/onnx::Gemm', 4455]]), (832644, [['Net/Linear[fc1]/onnx::Gemm', 827392], ['Net/ReLU[relu]/onnx::Relu', 808], ['Net/Linear[fc2]/onnx::Gemm', 4444]]), (830583, [['Net/Linear[fc1]/onnx::Gemm', 825344], ['Net/ReLU[relu]/onnx::Relu', 806], ['Net/Linear[fc2]/onnx::Gemm', 4433]]), (828522, [['Net/Linear[fc1]/onnx::Gemm', 823296], ['Net/ReLU[relu]/onnx::Relu', 804], ['Net/Linear[fc2]/onnx::Gemm', 4422]]), (826461, [['Net/Linear[fc1]/onnx::Gemm', 821248], ['Net/ReLU[relu]/onnx::Relu', 802], ['Net/Linear[fc2]/onnx::Gemm', 4411]]), (824400, [['Net/Linear[fc1]/onnx::Gemm', 819200], ['Net/ReLU[relu]/onnx::Relu', 800], ['Net/Linear[fc2]/onnx::Gemm', 4400]]), (822339, [['Net/Linear[fc1]/onnx::Gemm', 817152], ['Net/ReLU[relu]/onnx::Relu', 798], ['Net/Linear[fc2]/onnx::Gemm', 4389]]), (820278, [['Net/Linear[fc1]/onnx::Gemm', 815104], ['Net/ReLU[relu]/onnx::Relu', 796], ['Net/Linear[fc2]/onnx::Gemm', 4378]]), (818217, [['Net/Linear[fc1]/onnx::Gemm', 813056], ['Net/ReLU[relu]/onnx::Relu', 794], ['Net/Linear[fc2]/onnx::Gemm', 4367]]), (816156, [['Net/Linear[fc1]/onnx::Gemm', 811008], ['Net/ReLU[relu]/onnx::Relu', 792], ['Net/Linear[fc2]/onnx::Gemm', 4356]]), (814095, [['Net/Linear[fc1]/onnx::Gemm', 808960], ['Net/ReLU[relu]/onnx::Relu', 790], ['Net/Linear[fc2]/onnx::Gemm', 4345]]), (812034, [['Net/Linear[fc1]/onnx::Gemm', 806912], ['Net/ReLU[relu]/onnx::Relu', 788], ['Net/Linear[fc2]/onnx::Gemm', 4334]]), (809973, [['Net/Linear[fc1]/onnx::Gemm', 804864], ['Net/ReLU[relu]/onnx::Relu', 786], ['Net/Linear[fc2]/onnx::Gemm', 4323]]), (807912, [['Net/Linear[fc1]/onnx::Gemm', 802816], ['Net/ReLU[relu]/onnx::Relu', 784], ['Net/Linear[fc2]/onnx::Gemm', 4312]]), (805851, [['Net/Linear[fc1]/onnx::Gemm', 800768], ['Net/ReLU[relu]/onnx::Relu', 782], ['Net/Linear[fc2]/onnx::Gemm', 4301]]), (803790, [['Net/Linear[fc1]/onnx::Gemm', 798720], ['Net/ReLU[relu]/onnx::Relu', 780], ['Net/Linear[fc2]/onnx::Gemm', 4290]]), (801729, [['Net/Linear[fc1]/onnx::Gemm', 796672], ['Net/ReLU[relu]/onnx::Relu', 778], ['Net/Linear[fc2]/onnx::Gemm', 4279]]), (799668, [['Net/Linear[fc1]/onnx::Gemm', 794624], ['Net/ReLU[relu]/onnx::Relu', 776], ['Net/Linear[fc2]/onnx::Gemm', 4268]]), (797607, [['Net/Linear[fc1]/onnx::Gemm', 792576], ['Net/ReLU[relu]/onnx::Relu', 774], ['Net/Linear[fc2]/onnx::Gemm', 4257]]), (795546, [['Net/Linear[fc1]/onnx::Gemm', 790528], ['Net/ReLU[relu]/onnx::Relu', 772], ['Net/Linear[fc2]/onnx::Gemm', 4246]]), (793485, [['Net/Linear[fc1]/onnx::Gemm', 788480], ['Net/ReLU[relu]/onnx::Relu', 770], ['Net/Linear[fc2]/onnx::Gemm', 4235]]), (791424, [['Net/Linear[fc1]/onnx::Gemm', 786432], ['Net/ReLU[relu]/onnx::Relu', 768], ['Net/Linear[fc2]/onnx::Gemm', 4224]]), (789363, [['Net/Linear[fc1]/onnx::Gemm', 784384], ['Net/ReLU[relu]/onnx::Relu', 766], ['Net/Linear[fc2]/onnx::Gemm', 4213]]), (787302, [['Net/Linear[fc1]/onnx::Gemm', 782336], ['Net/ReLU[relu]/onnx::Relu', 764], ['Net/Linear[fc2]/onnx::Gemm', 4202]]), (785241, [['Net/Linear[fc1]/onnx::Gemm', 780288], ['Net/ReLU[relu]/onnx::Relu', 762], ['Net/Linear[fc2]/onnx::Gemm', 4191]]), (783180, [['Net/Linear[fc1]/onnx::Gemm', 778240], ['Net/ReLU[relu]/onnx::Relu', 760], ['Net/Linear[fc2]/onnx::Gemm', 4180]]), (781119, [['Net/Linear[fc1]/onnx::Gemm', 776192], ['Net/ReLU[relu]/onnx::Relu', 758], ['Net/Linear[fc2]/onnx::Gemm', 4169]]), (779058, [['Net/Linear[fc1]/onnx::Gemm', 774144], ['Net/ReLU[relu]/onnx::Relu', 756], ['Net/Linear[fc2]/onnx::Gemm', 4158]]), (776997, [['Net/Linear[fc1]/onnx::Gemm', 772096], ['Net/ReLU[relu]/onnx::Relu', 754], ['Net/Linear[fc2]/onnx::Gemm', 4147]]), (774936, [['Net/Linear[fc1]/onnx::Gemm', 770048], ['Net/ReLU[relu]/onnx::Relu', 752], ['Net/Linear[fc2]/onnx::Gemm', 4136]]), (772875, [['Net/Linear[fc1]/onnx::Gemm', 768000], ['Net/ReLU[relu]/onnx::Relu', 750], ['Net/Linear[fc2]/onnx::Gemm', 4125]]), (770814, [['Net/Linear[fc1]/onnx::Gemm', 765952], ['Net/ReLU[relu]/onnx::Relu', 748], ['Net/Linear[fc2]/onnx::Gemm', 4114]]), (768753, [['Net/Linear[fc1]/onnx::Gemm', 763904], ['Net/ReLU[relu]/onnx::Relu', 746], ['Net/Linear[fc2]/onnx::Gemm', 4103]]), (766692, [['Net/Linear[fc1]/onnx::Gemm', 761856], ['Net/ReLU[relu]/onnx::Relu', 744], ['Net/Linear[fc2]/onnx::Gemm', 4092]]), (764631, [['Net/Linear[fc1]/onnx::Gemm', 759808], ['Net/ReLU[relu]/onnx::Relu', 742], ['Net/Linear[fc2]/onnx::Gemm', 4081]]), (762570, [['Net/Linear[fc1]/onnx::Gemm', 757760], ['Net/ReLU[relu]/onnx::Relu', 740], ['Net/Linear[fc2]/onnx::Gemm', 4070]]), (760509, [['Net/Linear[fc1]/onnx::Gemm', 755712], ['Net/ReLU[relu]/onnx::Relu', 738], ['Net/Linear[fc2]/onnx::Gemm', 4059]]), (758448, [['Net/Linear[fc1]/onnx::Gemm', 753664], ['Net/ReLU[relu]/onnx::Relu', 736], ['Net/Linear[fc2]/onnx::Gemm', 4048]]), (756387, [['Net/Linear[fc1]/onnx::Gemm', 751616], ['Net/ReLU[relu]/onnx::Relu', 734], ['Net/Linear[fc2]/onnx::Gemm', 4037]]), (754326, [['Net/Linear[fc1]/onnx::Gemm', 749568], ['Net/ReLU[relu]/onnx::Relu', 732], ['Net/Linear[fc2]/onnx::Gemm', 4026]]), (752265, [['Net/Linear[fc1]/onnx::Gemm', 747520], ['Net/ReLU[relu]/onnx::Relu', 730], ['Net/Linear[fc2]/onnx::Gemm', 4015]]), (750204, [['Net/Linear[fc1]/onnx::Gemm', 745472], ['Net/ReLU[relu]/onnx::Relu', 728], ['Net/Linear[fc2]/onnx::Gemm', 4004]]), (748143, [['Net/Linear[fc1]/onnx::Gemm', 743424], ['Net/ReLU[relu]/onnx::Relu', 726], ['Net/Linear[fc2]/onnx::Gemm', 3993]]), (746082, [['Net/Linear[fc1]/onnx::Gemm', 741376], ['Net/ReLU[relu]/onnx::Relu', 724], ['Net/Linear[fc2]/onnx::Gemm', 3982]]), (744021, [['Net/Linear[fc1]/onnx::Gemm', 739328], ['Net/ReLU[relu]/onnx::Relu', 722], ['Net/Linear[fc2]/onnx::Gemm', 3971]]), (741960, [['Net/Linear[fc1]/onnx::Gemm', 737280], ['Net/ReLU[relu]/onnx::Relu', 720], ['Net/Linear[fc2]/onnx::Gemm', 3960]]), (739899, [['Net/Linear[fc1]/onnx::Gemm', 735232], ['Net/ReLU[relu]/onnx::Relu', 718], ['Net/Linear[fc2]/onnx::Gemm', 3949]]), (737838, [['Net/Linear[fc1]/onnx::Gemm', 733184], ['Net/ReLU[relu]/onnx::Relu', 716], ['Net/Linear[fc2]/onnx::Gemm', 3938]]), (735777, [['Net/Linear[fc1]/onnx::Gemm', 731136], ['Net/ReLU[relu]/onnx::Relu', 714], ['Net/Linear[fc2]/onnx::Gemm', 3927]]), (733716, [['Net/Linear[fc1]/onnx::Gemm', 729088], ['Net/ReLU[relu]/onnx::Relu', 712], ['Net/Linear[fc2]/onnx::Gemm', 3916]]), (731655, [['Net/Linear[fc1]/onnx::Gemm', 727040], ['Net/ReLU[relu]/onnx::Relu', 710], ['Net/Linear[fc2]/onnx::Gemm', 3905]]), (729594, [['Net/Linear[fc1]/onnx::Gemm', 724992], ['Net/ReLU[relu]/onnx::Relu', 708], ['Net/Linear[fc2]/onnx::Gemm', 3894]]), (727533, [['Net/Linear[fc1]/onnx::Gemm', 722944], ['Net/ReLU[relu]/onnx::Relu', 706], ['Net/Linear[fc2]/onnx::Gemm', 3883]]), (725472, [['Net/Linear[fc1]/onnx::Gemm', 720896], ['Net/ReLU[relu]/onnx::Relu', 704], ['Net/Linear[fc2]/onnx::Gemm', 3872]]), (723411, [['Net/Linear[fc1]/onnx::Gemm', 718848], ['Net/ReLU[relu]/onnx::Relu', 702], ['Net/Linear[fc2]/onnx::Gemm', 3861]]), (721350, [['Net/Linear[fc1]/onnx::Gemm', 716800], ['Net/ReLU[relu]/onnx::Relu', 700], ['Net/Linear[fc2]/onnx::Gemm', 3850]]), (719289, [['Net/Linear[fc1]/onnx::Gemm', 714752], ['Net/ReLU[relu]/onnx::Relu', 698], ['Net/Linear[fc2]/onnx::Gemm', 3839]]), (717228, [['Net/Linear[fc1]/onnx::Gemm', 712704], ['Net/ReLU[relu]/onnx::Relu', 696], ['Net/Linear[fc2]/onnx::Gemm', 3828]]), (715167, [['Net/Linear[fc1]/onnx::Gemm', 710656], ['Net/ReLU[relu]/onnx::Relu', 694], ['Net/Linear[fc2]/onnx::Gemm', 3817]]), (713106, [['Net/Linear[fc1]/onnx::Gemm', 708608], ['Net/ReLU[relu]/onnx::Relu', 692], ['Net/Linear[fc2]/onnx::Gemm', 3806]]), (711045, [['Net/Linear[fc1]/onnx::Gemm', 706560], ['Net/ReLU[relu]/onnx::Relu', 690], ['Net/Linear[fc2]/onnx::Gemm', 3795]]), (708984, [['Net/Linear[fc1]/onnx::Gemm', 704512], ['Net/ReLU[relu]/onnx::Relu', 688], ['Net/Linear[fc2]/onnx::Gemm', 3784]]), (706923, [['Net/Linear[fc1]/onnx::Gemm', 702464], ['Net/ReLU[relu]/onnx::Relu', 686], ['Net/Linear[fc2]/onnx::Gemm', 3773]]), (704862, [['Net/Linear[fc1]/onnx::Gemm', 700416], ['Net/ReLU[relu]/onnx::Relu', 684], ['Net/Linear[fc2]/onnx::Gemm', 3762]]), (702801, [['Net/Linear[fc1]/onnx::Gemm', 698368], ['Net/ReLU[relu]/onnx::Relu', 682], ['Net/Linear[fc2]/onnx::Gemm', 3751]]), (700740, [['Net/Linear[fc1]/onnx::Gemm', 696320], ['Net/ReLU[relu]/onnx::Relu', 680], ['Net/Linear[fc2]/onnx::Gemm', 3740]]), (698679, [['Net/Linear[fc1]/onnx::Gemm', 694272], ['Net/ReLU[relu]/onnx::Relu', 678], ['Net/Linear[fc2]/onnx::Gemm', 3729]]), (696618, [['Net/Linear[fc1]/onnx::Gemm', 692224], ['Net/ReLU[relu]/onnx::Relu', 676], ['Net/Linear[fc2]/onnx::Gemm', 3718]]), (694557, [['Net/Linear[fc1]/onnx::Gemm', 690176], ['Net/ReLU[relu]/onnx::Relu', 674], ['Net/Linear[fc2]/onnx::Gemm', 3707]]), (692496, [['Net/Linear[fc1]/onnx::Gemm', 688128], ['Net/ReLU[relu]/onnx::Relu', 672], ['Net/Linear[fc2]/onnx::Gemm', 3696]]), (690435, [['Net/Linear[fc1]/onnx::Gemm', 686080], ['Net/ReLU[relu]/onnx::Relu', 670], ['Net/Linear[fc2]/onnx::Gemm', 3685]]), (688374, [['Net/Linear[fc1]/onnx::Gemm', 684032], ['Net/ReLU[relu]/onnx::Relu', 668], ['Net/Linear[fc2]/onnx::Gemm', 3674]]), (686313, [['Net/Linear[fc1]/onnx::Gemm', 681984], ['Net/ReLU[relu]/onnx::Relu', 666], ['Net/Linear[fc2]/onnx::Gemm', 3663]]), (684252, [['Net/Linear[fc1]/onnx::Gemm', 679936], ['Net/ReLU[relu]/onnx::Relu', 664], ['Net/Linear[fc2]/onnx::Gemm', 3652]]), (682191, [['Net/Linear[fc1]/onnx::Gemm', 677888], ['Net/ReLU[relu]/onnx::Relu', 662], ['Net/Linear[fc2]/onnx::Gemm', 3641]]), (680130, [['Net/Linear[fc1]/onnx::Gemm', 675840], ['Net/ReLU[relu]/onnx::Relu', 660], ['Net/Linear[fc2]/onnx::Gemm', 3630]]), (678069, [['Net/Linear[fc1]/onnx::Gemm', 673792], ['Net/ReLU[relu]/onnx::Relu', 658], ['Net/Linear[fc2]/onnx::Gemm', 3619]]), (676008, [['Net/Linear[fc1]/onnx::Gemm', 671744], ['Net/ReLU[relu]/onnx::Relu', 656], ['Net/Linear[fc2]/onnx::Gemm', 3608]]), (673947, [['Net/Linear[fc1]/onnx::Gemm', 669696], ['Net/ReLU[relu]/onnx::Relu', 654], ['Net/Linear[fc2]/onnx::Gemm', 3597]]), (671886, [['Net/Linear[fc1]/onnx::Gemm', 667648], ['Net/ReLU[relu]/onnx::Relu', 652], ['Net/Linear[fc2]/onnx::Gemm', 3586]]), (669825, [['Net/Linear[fc1]/onnx::Gemm', 665600], ['Net/ReLU[relu]/onnx::Relu', 650], ['Net/Linear[fc2]/onnx::Gemm', 3575]]), (667764, [['Net/Linear[fc1]/onnx::Gemm', 663552], ['Net/ReLU[relu]/onnx::Relu', 648], ['Net/Linear[fc2]/onnx::Gemm', 3564]]), (665703, [['Net/Linear[fc1]/onnx::Gemm', 661504], ['Net/ReLU[relu]/onnx::Relu', 646], ['Net/Linear[fc2]/onnx::Gemm', 3553]]), (663642, [['Net/Linear[fc1]/onnx::Gemm', 659456], ['Net/ReLU[relu]/onnx::Relu', 644], ['Net/Linear[fc2]/onnx::Gemm', 3542]]), (661581, [['Net/Linear[fc1]/onnx::Gemm', 657408], ['Net/ReLU[relu]/onnx::Relu', 642], ['Net/Linear[fc2]/onnx::Gemm', 3531]]), (659520, [['Net/Linear[fc1]/onnx::Gemm', 655360], ['Net/ReLU[relu]/onnx::Relu', 640], ['Net/Linear[fc2]/onnx::Gemm', 3520]]), (657459, [['Net/Linear[fc1]/onnx::Gemm', 653312], ['Net/ReLU[relu]/onnx::Relu', 638], ['Net/Linear[fc2]/onnx::Gemm', 3509]]), (655398, [['Net/Linear[fc1]/onnx::Gemm', 651264], ['Net/ReLU[relu]/onnx::Relu', 636], ['Net/Linear[fc2]/onnx::Gemm', 3498]]), (653337, [['Net/Linear[fc1]/onnx::Gemm', 649216], ['Net/ReLU[relu]/onnx::Relu', 634], ['Net/Linear[fc2]/onnx::Gemm', 3487]]), (651276, [['Net/Linear[fc1]/onnx::Gemm', 647168], ['Net/ReLU[relu]/onnx::Relu', 632], ['Net/Linear[fc2]/onnx::Gemm', 3476]]), (649215, [['Net/Linear[fc1]/onnx::Gemm', 645120], ['Net/ReLU[relu]/onnx::Relu', 630], ['Net/Linear[fc2]/onnx::Gemm', 3465]]), (647154, [['Net/Linear[fc1]/onnx::Gemm', 643072], ['Net/ReLU[relu]/onnx::Relu', 628], ['Net/Linear[fc2]/onnx::Gemm', 3454]]), (645093, [['Net/Linear[fc1]/onnx::Gemm', 641024], ['Net/ReLU[relu]/onnx::Relu', 626], ['Net/Linear[fc2]/onnx::Gemm', 3443]]), (643032, [['Net/Linear[fc1]/onnx::Gemm', 638976], ['Net/ReLU[relu]/onnx::Relu', 624], ['Net/Linear[fc2]/onnx::Gemm', 3432]]), (640971, [['Net/Linear[fc1]/onnx::Gemm', 636928], ['Net/ReLU[relu]/onnx::Relu', 622], ['Net/Linear[fc2]/onnx::Gemm', 3421]]), (638910, [['Net/Linear[fc1]/onnx::Gemm', 634880], ['Net/ReLU[relu]/onnx::Relu', 620], ['Net/Linear[fc2]/onnx::Gemm', 3410]]), (636849, [['Net/Linear[fc1]/onnx::Gemm', 632832], ['Net/ReLU[relu]/onnx::Relu', 618], ['Net/Linear[fc2]/onnx::Gemm', 3399]]), (634788, [['Net/Linear[fc1]/onnx::Gemm', 630784], ['Net/ReLU[relu]/onnx::Relu', 616], ['Net/Linear[fc2]/onnx::Gemm', 3388]]), (632727, [['Net/Linear[fc1]/onnx::Gemm', 628736], ['Net/ReLU[relu]/onnx::Relu', 614], ['Net/Linear[fc2]/onnx::Gemm', 3377]]), (630666, [['Net/Linear[fc1]/onnx::Gemm', 626688], ['Net/ReLU[relu]/onnx::Relu', 612], ['Net/Linear[fc2]/onnx::Gemm', 3366]]), (628605, [['Net/Linear[fc1]/onnx::Gemm', 624640], ['Net/ReLU[relu]/onnx::Relu', 610], ['Net/Linear[fc2]/onnx::Gemm', 3355]]), (626544, [['Net/Linear[fc1]/onnx::Gemm', 622592], ['Net/ReLU[relu]/onnx::Relu', 608], ['Net/Linear[fc2]/onnx::Gemm', 3344]]), (624483, [['Net/Linear[fc1]/onnx::Gemm', 620544], ['Net/ReLU[relu]/onnx::Relu', 606], ['Net/Linear[fc2]/onnx::Gemm', 3333]]), (622422, [['Net/Linear[fc1]/onnx::Gemm', 618496], ['Net/ReLU[relu]/onnx::Relu', 604], ['Net/Linear[fc2]/onnx::Gemm', 3322]]), (620361, [['Net/Linear[fc1]/onnx::Gemm', 616448], ['Net/ReLU[relu]/onnx::Relu', 602], ['Net/Linear[fc2]/onnx::Gemm', 3311]]), (618300, [['Net/Linear[fc1]/onnx::Gemm', 614400], ['Net/ReLU[relu]/onnx::Relu', 600], ['Net/Linear[fc2]/onnx::Gemm', 3300]]), (616239, [['Net/Linear[fc1]/onnx::Gemm', 612352], ['Net/ReLU[relu]/onnx::Relu', 598], ['Net/Linear[fc2]/onnx::Gemm', 3289]]), (614178, [['Net/Linear[fc1]/onnx::Gemm', 610304], ['Net/ReLU[relu]/onnx::Relu', 596], ['Net/Linear[fc2]/onnx::Gemm', 3278]]), (612117, [['Net/Linear[fc1]/onnx::Gemm', 608256], ['Net/ReLU[relu]/onnx::Relu', 594], ['Net/Linear[fc2]/onnx::Gemm', 3267]]), (610056, [['Net/Linear[fc1]/onnx::Gemm', 606208], ['Net/ReLU[relu]/onnx::Relu', 592], ['Net/Linear[fc2]/onnx::Gemm', 3256]]), (607995, [['Net/Linear[fc1]/onnx::Gemm', 604160], ['Net/ReLU[relu]/onnx::Relu', 590], ['Net/Linear[fc2]/onnx::Gemm', 3245]]), (605934, [['Net/Linear[fc1]/onnx::Gemm', 602112], ['Net/ReLU[relu]/onnx::Relu', 588], ['Net/Linear[fc2]/onnx::Gemm', 3234]]), (603873, [['Net/Linear[fc1]/onnx::Gemm', 600064], ['Net/ReLU[relu]/onnx::Relu', 586], ['Net/Linear[fc2]/onnx::Gemm', 3223]]), (601812, [['Net/Linear[fc1]/onnx::Gemm', 598016], ['Net/ReLU[relu]/onnx::Relu', 584], ['Net/Linear[fc2]/onnx::Gemm', 3212]]), (599751, [['Net/Linear[fc1]/onnx::Gemm', 595968], ['Net/ReLU[relu]/onnx::Relu', 582], ['Net/Linear[fc2]/onnx::Gemm', 3201]]), (597690, [['Net/Linear[fc1]/onnx::Gemm', 593920], ['Net/ReLU[relu]/onnx::Relu', 580], ['Net/Linear[fc2]/onnx::Gemm', 3190]]), (595629, [['Net/Linear[fc1]/onnx::Gemm', 591872], ['Net/ReLU[relu]/onnx::Relu', 578], ['Net/Linear[fc2]/onnx::Gemm', 3179]]), (593568, [['Net/Linear[fc1]/onnx::Gemm', 589824], ['Net/ReLU[relu]/onnx::Relu', 576], ['Net/Linear[fc2]/onnx::Gemm', 3168]]), (591507, [['Net/Linear[fc1]/onnx::Gemm', 587776], ['Net/ReLU[relu]/onnx::Relu', 574], ['Net/Linear[fc2]/onnx::Gemm', 3157]]), (589446, [['Net/Linear[fc1]/onnx::Gemm', 585728], ['Net/ReLU[relu]/onnx::Relu', 572], ['Net/Linear[fc2]/onnx::Gemm', 3146]]), (587385, [['Net/Linear[fc1]/onnx::Gemm', 583680], ['Net/ReLU[relu]/onnx::Relu', 570], ['Net/Linear[fc2]/onnx::Gemm', 3135]]), (585324, [['Net/Linear[fc1]/onnx::Gemm', 581632], ['Net/ReLU[relu]/onnx::Relu', 568], ['Net/Linear[fc2]/onnx::Gemm', 3124]]), (583263, [['Net/Linear[fc1]/onnx::Gemm', 579584], ['Net/ReLU[relu]/onnx::Relu', 566], ['Net/Linear[fc2]/onnx::Gemm', 3113]]), (581202, [['Net/Linear[fc1]/onnx::Gemm', 577536], ['Net/ReLU[relu]/onnx::Relu', 564], ['Net/Linear[fc2]/onnx::Gemm', 3102]]), (579141, [['Net/Linear[fc1]/onnx::Gemm', 575488], ['Net/ReLU[relu]/onnx::Relu', 562], ['Net/Linear[fc2]/onnx::Gemm', 3091]]), (577080, [['Net/Linear[fc1]/onnx::Gemm', 573440], ['Net/ReLU[relu]/onnx::Relu', 560], ['Net/Linear[fc2]/onnx::Gemm', 3080]]), (575019, [['Net/Linear[fc1]/onnx::Gemm', 571392], ['Net/ReLU[relu]/onnx::Relu', 558], ['Net/Linear[fc2]/onnx::Gemm', 3069]]), (572958, [['Net/Linear[fc1]/onnx::Gemm', 569344], ['Net/ReLU[relu]/onnx::Relu', 556], ['Net/Linear[fc2]/onnx::Gemm', 3058]]), (570897, [['Net/Linear[fc1]/onnx::Gemm', 567296], ['Net/ReLU[relu]/onnx::Relu', 554], ['Net/Linear[fc2]/onnx::Gemm', 3047]]), (568836, [['Net/Linear[fc1]/onnx::Gemm', 565248], ['Net/ReLU[relu]/onnx::Relu', 552], ['Net/Linear[fc2]/onnx::Gemm', 3036]]), (566775, [['Net/Linear[fc1]/onnx::Gemm', 563200], ['Net/ReLU[relu]/onnx::Relu', 550], ['Net/Linear[fc2]/onnx::Gemm', 3025]]), (564714, [['Net/Linear[fc1]/onnx::Gemm', 561152], ['Net/ReLU[relu]/onnx::Relu', 548], ['Net/Linear[fc2]/onnx::Gemm', 3014]]), (562653, [['Net/Linear[fc1]/onnx::Gemm', 559104], ['Net/ReLU[relu]/onnx::Relu', 546], ['Net/Linear[fc2]/onnx::Gemm', 3003]]), (560592, [['Net/Linear[fc1]/onnx::Gemm', 557056], ['Net/ReLU[relu]/onnx::Relu', 544], ['Net/Linear[fc2]/onnx::Gemm', 2992]]), (558531, [['Net/Linear[fc1]/onnx::Gemm', 555008], ['Net/ReLU[relu]/onnx::Relu', 542], ['Net/Linear[fc2]/onnx::Gemm', 2981]]), (556470, [['Net/Linear[fc1]/onnx::Gemm', 552960], ['Net/ReLU[relu]/onnx::Relu', 540], ['Net/Linear[fc2]/onnx::Gemm', 2970]]), (554409, [['Net/Linear[fc1]/onnx::Gemm', 550912], ['Net/ReLU[relu]/onnx::Relu', 538], ['Net/Linear[fc2]/onnx::Gemm', 2959]]), (552348, [['Net/Linear[fc1]/onnx::Gemm', 548864], ['Net/ReLU[relu]/onnx::Relu', 536], ['Net/Linear[fc2]/onnx::Gemm', 2948]]), (550287, [['Net/Linear[fc1]/onnx::Gemm', 546816], ['Net/ReLU[relu]/onnx::Relu', 534], ['Net/Linear[fc2]/onnx::Gemm', 2937]]), (548226, [['Net/Linear[fc1]/onnx::Gemm', 544768], ['Net/ReLU[relu]/onnx::Relu', 532], ['Net/Linear[fc2]/onnx::Gemm', 2926]]), (546165, [['Net/Linear[fc1]/onnx::Gemm', 542720], ['Net/ReLU[relu]/onnx::Relu', 530], ['Net/Linear[fc2]/onnx::Gemm', 2915]]), (544104, [['Net/Linear[fc1]/onnx::Gemm', 540672], ['Net/ReLU[relu]/onnx::Relu', 528], ['Net/Linear[fc2]/onnx::Gemm', 2904]]), (542043, [['Net/Linear[fc1]/onnx::Gemm', 538624], ['Net/ReLU[relu]/onnx::Relu', 526], ['Net/Linear[fc2]/onnx::Gemm', 2893]]), (539982, [['Net/Linear[fc1]/onnx::Gemm', 536576], ['Net/ReLU[relu]/onnx::Relu', 524], ['Net/Linear[fc2]/onnx::Gemm', 2882]]), (537921, [['Net/Linear[fc1]/onnx::Gemm', 534528], ['Net/ReLU[relu]/onnx::Relu', 522], ['Net/Linear[fc2]/onnx::Gemm', 2871]]), (535860, [['Net/Linear[fc1]/onnx::Gemm', 532480], ['Net/ReLU[relu]/onnx::Relu', 520], ['Net/Linear[fc2]/onnx::Gemm', 2860]]), (533799, [['Net/Linear[fc1]/onnx::Gemm', 530432], ['Net/ReLU[relu]/onnx::Relu', 518], ['Net/Linear[fc2]/onnx::Gemm', 2849]]), (531738, [['Net/Linear[fc1]/onnx::Gemm', 528384], ['Net/ReLU[relu]/onnx::Relu', 516], ['Net/Linear[fc2]/onnx::Gemm', 2838]]), (529677, [['Net/Linear[fc1]/onnx::Gemm', 526336], ['Net/ReLU[relu]/onnx::Relu', 514], ['Net/Linear[fc2]/onnx::Gemm', 2827]]), (527616, [['Net/Linear[fc1]/onnx::Gemm', 524288], ['Net/ReLU[relu]/onnx::Relu', 512], ['Net/Linear[fc2]/onnx::Gemm', 2816]]), (525555, [['Net/Linear[fc1]/onnx::Gemm', 522240], ['Net/ReLU[relu]/onnx::Relu', 510], ['Net/Linear[fc2]/onnx::Gemm', 2805]]), (523494, [['Net/Linear[fc1]/onnx::Gemm', 520192], ['Net/ReLU[relu]/onnx::Relu', 508], ['Net/Linear[fc2]/onnx::Gemm', 2794]]), (521433, [['Net/Linear[fc1]/onnx::Gemm', 518144], ['Net/ReLU[relu]/onnx::Relu', 506], ['Net/Linear[fc2]/onnx::Gemm', 2783]]), (519372, [['Net/Linear[fc1]/onnx::Gemm', 516096], ['Net/ReLU[relu]/onnx::Relu', 504], ['Net/Linear[fc2]/onnx::Gemm', 2772]]), (517311, [['Net/Linear[fc1]/onnx::Gemm', 514048], ['Net/ReLU[relu]/onnx::Relu', 502], ['Net/Linear[fc2]/onnx::Gemm', 2761]]), (515250, [['Net/Linear[fc1]/onnx::Gemm', 512000], ['Net/ReLU[relu]/onnx::Relu', 500], ['Net/Linear[fc2]/onnx::Gemm', 2750]]), (513189, [['Net/Linear[fc1]/onnx::Gemm', 509952], ['Net/ReLU[relu]/onnx::Relu', 498], ['Net/Linear[fc2]/onnx::Gemm', 2739]]), (511128, [['Net/Linear[fc1]/onnx::Gemm', 507904], ['Net/ReLU[relu]/onnx::Relu', 496], ['Net/Linear[fc2]/onnx::Gemm', 2728]]), (509067, [['Net/Linear[fc1]/onnx::Gemm', 505856], ['Net/ReLU[relu]/onnx::Relu', 494], ['Net/Linear[fc2]/onnx::Gemm', 2717]]), (507006, [['Net/Linear[fc1]/onnx::Gemm', 503808], ['Net/ReLU[relu]/onnx::Relu', 492], ['Net/Linear[fc2]/onnx::Gemm', 2706]]), (504945, [['Net/Linear[fc1]/onnx::Gemm', 501760], ['Net/ReLU[relu]/onnx::Relu', 490], ['Net/Linear[fc2]/onnx::Gemm', 2695]]), (502884, [['Net/Linear[fc1]/onnx::Gemm', 499712], ['Net/ReLU[relu]/onnx::Relu', 488], ['Net/Linear[fc2]/onnx::Gemm', 2684]]), (500823, [['Net/Linear[fc1]/onnx::Gemm', 497664], ['Net/ReLU[relu]/onnx::Relu', 486], ['Net/Linear[fc2]/onnx::Gemm', 2673]]), (498762, [['Net/Linear[fc1]/onnx::Gemm', 495616], ['Net/ReLU[relu]/onnx::Relu', 484], ['Net/Linear[fc2]/onnx::Gemm', 2662]]), (496701, [['Net/Linear[fc1]/onnx::Gemm', 493568], ['Net/ReLU[relu]/onnx::Relu', 482], ['Net/Linear[fc2]/onnx::Gemm', 2651]]), (494640, [['Net/Linear[fc1]/onnx::Gemm', 491520], ['Net/ReLU[relu]/onnx::Relu', 480], ['Net/Linear[fc2]/onnx::Gemm', 2640]]), (492579, [['Net/Linear[fc1]/onnx::Gemm', 489472], ['Net/ReLU[relu]/onnx::Relu', 478], ['Net/Linear[fc2]/onnx::Gemm', 2629]]), (490518, [['Net/Linear[fc1]/onnx::Gemm', 487424], ['Net/ReLU[relu]/onnx::Relu', 476], ['Net/Linear[fc2]/onnx::Gemm', 2618]]), (488457, [['Net/Linear[fc1]/onnx::Gemm', 485376], ['Net/ReLU[relu]/onnx::Relu', 474], ['Net/Linear[fc2]/onnx::Gemm', 2607]]), (486396, [['Net/Linear[fc1]/onnx::Gemm', 483328], ['Net/ReLU[relu]/onnx::Relu', 472], ['Net/Linear[fc2]/onnx::Gemm', 2596]]), (484335, [['Net/Linear[fc1]/onnx::Gemm', 481280], ['Net/ReLU[relu]/onnx::Relu', 470], ['Net/Linear[fc2]/onnx::Gemm', 2585]]), (482274, [['Net/Linear[fc1]/onnx::Gemm', 479232], ['Net/ReLU[relu]/onnx::Relu', 468], ['Net/Linear[fc2]/onnx::Gemm', 2574]]), (480213, [['Net/Linear[fc1]/onnx::Gemm', 477184], ['Net/ReLU[relu]/onnx::Relu', 466], ['Net/Linear[fc2]/onnx::Gemm', 2563]]), (478152, [['Net/Linear[fc1]/onnx::Gemm', 475136], ['Net/ReLU[relu]/onnx::Relu', 464], ['Net/Linear[fc2]/onnx::Gemm', 2552]]), (476091, [['Net/Linear[fc1]/onnx::Gemm', 473088], ['Net/ReLU[relu]/onnx::Relu', 462], ['Net/Linear[fc2]/onnx::Gemm', 2541]]), (474030, [['Net/Linear[fc1]/onnx::Gemm', 471040], ['Net/ReLU[relu]/onnx::Relu', 460], ['Net/Linear[fc2]/onnx::Gemm', 2530]]), (471969, [['Net/Linear[fc1]/onnx::Gemm', 468992], ['Net/ReLU[relu]/onnx::Relu', 458], ['Net/Linear[fc2]/onnx::Gemm', 2519]]), (469908, [['Net/Linear[fc1]/onnx::Gemm', 466944], ['Net/ReLU[relu]/onnx::Relu', 456], ['Net/Linear[fc2]/onnx::Gemm', 2508]]), (467847, [['Net/Linear[fc1]/onnx::Gemm', 464896], ['Net/ReLU[relu]/onnx::Relu', 454], ['Net/Linear[fc2]/onnx::Gemm', 2497]]), (465786, [['Net/Linear[fc1]/onnx::Gemm', 462848], ['Net/ReLU[relu]/onnx::Relu', 452], ['Net/Linear[fc2]/onnx::Gemm', 2486]]), (463725, [['Net/Linear[fc1]/onnx::Gemm', 460800], ['Net/ReLU[relu]/onnx::Relu', 450], ['Net/Linear[fc2]/onnx::Gemm', 2475]]), (461664, [['Net/Linear[fc1]/onnx::Gemm', 458752], ['Net/ReLU[relu]/onnx::Relu', 448], ['Net/Linear[fc2]/onnx::Gemm', 2464]]), (459603, [['Net/Linear[fc1]/onnx::Gemm', 456704], ['Net/ReLU[relu]/onnx::Relu', 446], ['Net/Linear[fc2]/onnx::Gemm', 2453]]), (457542, [['Net/Linear[fc1]/onnx::Gemm', 454656], ['Net/ReLU[relu]/onnx::Relu', 444], ['Net/Linear[fc2]/onnx::Gemm', 2442]]), (455481, [['Net/Linear[fc1]/onnx::Gemm', 452608], ['Net/ReLU[relu]/onnx::Relu', 442], ['Net/Linear[fc2]/onnx::Gemm', 2431]]), (453420, [['Net/Linear[fc1]/onnx::Gemm', 450560], ['Net/ReLU[relu]/onnx::Relu', 440], ['Net/Linear[fc2]/onnx::Gemm', 2420]]), (451359, [['Net/Linear[fc1]/onnx::Gemm', 448512], ['Net/ReLU[relu]/onnx::Relu', 438], ['Net/Linear[fc2]/onnx::Gemm', 2409]]), (449298, [['Net/Linear[fc1]/onnx::Gemm', 446464], ['Net/ReLU[relu]/onnx::Relu', 436], ['Net/Linear[fc2]/onnx::Gemm', 2398]]), (447237, [['Net/Linear[fc1]/onnx::Gemm', 444416], ['Net/ReLU[relu]/onnx::Relu', 434], ['Net/Linear[fc2]/onnx::Gemm', 2387]]), (445176, [['Net/Linear[fc1]/onnx::Gemm', 442368], ['Net/ReLU[relu]/onnx::Relu', 432], ['Net/Linear[fc2]/onnx::Gemm', 2376]]), (443115, [['Net/Linear[fc1]/onnx::Gemm', 440320], ['Net/ReLU[relu]/onnx::Relu', 430], ['Net/Linear[fc2]/onnx::Gemm', 2365]]), (441054, [['Net/Linear[fc1]/onnx::Gemm', 438272], ['Net/ReLU[relu]/onnx::Relu', 428], ['Net/Linear[fc2]/onnx::Gemm', 2354]]), (438993, [['Net/Linear[fc1]/onnx::Gemm', 436224], ['Net/ReLU[relu]/onnx::Relu', 426], ['Net/Linear[fc2]/onnx::Gemm', 2343]]), (436932, [['Net/Linear[fc1]/onnx::Gemm', 434176], ['Net/ReLU[relu]/onnx::Relu', 424], ['Net/Linear[fc2]/onnx::Gemm', 2332]]), (434871, [['Net/Linear[fc1]/onnx::Gemm', 432128], ['Net/ReLU[relu]/onnx::Relu', 422], ['Net/Linear[fc2]/onnx::Gemm', 2321]]), (432810, [['Net/Linear[fc1]/onnx::Gemm', 430080], ['Net/ReLU[relu]/onnx::Relu', 420], ['Net/Linear[fc2]/onnx::Gemm', 2310]]), (430749, [['Net/Linear[fc1]/onnx::Gemm', 428032], ['Net/ReLU[relu]/onnx::Relu', 418], ['Net/Linear[fc2]/onnx::Gemm', 2299]]), (428688, [['Net/Linear[fc1]/onnx::Gemm', 425984], ['Net/ReLU[relu]/onnx::Relu', 416], ['Net/Linear[fc2]/onnx::Gemm', 2288]]), (426627, [['Net/Linear[fc1]/onnx::Gemm', 423936], ['Net/ReLU[relu]/onnx::Relu', 414], ['Net/Linear[fc2]/onnx::Gemm', 2277]]), (424566, [['Net/Linear[fc1]/onnx::Gemm', 421888], ['Net/ReLU[relu]/onnx::Relu', 412], ['Net/Linear[fc2]/onnx::Gemm', 2266]]), (422505, [['Net/Linear[fc1]/onnx::Gemm', 419840], ['Net/ReLU[relu]/onnx::Relu', 410], ['Net/Linear[fc2]/onnx::Gemm', 2255]]), (420444, [['Net/Linear[fc1]/onnx::Gemm', 417792], ['Net/ReLU[relu]/onnx::Relu', 408], ['Net/Linear[fc2]/onnx::Gemm', 2244]]), (418383, [['Net/Linear[fc1]/onnx::Gemm', 415744], ['Net/ReLU[relu]/onnx::Relu', 406], ['Net/Linear[fc2]/onnx::Gemm', 2233]]), (416322, [['Net/Linear[fc1]/onnx::Gemm', 413696], ['Net/ReLU[relu]/onnx::Relu', 404], ['Net/Linear[fc2]/onnx::Gemm', 2222]]), (414261, [['Net/Linear[fc1]/onnx::Gemm', 411648], ['Net/ReLU[relu]/onnx::Relu', 402], ['Net/Linear[fc2]/onnx::Gemm', 2211]]), (412200, [['Net/Linear[fc1]/onnx::Gemm', 409600], ['Net/ReLU[relu]/onnx::Relu', 400], ['Net/Linear[fc2]/onnx::Gemm', 2200]]), (410139, [['Net/Linear[fc1]/onnx::Gemm', 407552], ['Net/ReLU[relu]/onnx::Relu', 398], ['Net/Linear[fc2]/onnx::Gemm', 2189]]), (408078, [['Net/Linear[fc1]/onnx::Gemm', 405504], ['Net/ReLU[relu]/onnx::Relu', 396], ['Net/Linear[fc2]/onnx::Gemm', 2178]]), (406017, [['Net/Linear[fc1]/onnx::Gemm', 403456], ['Net/ReLU[relu]/onnx::Relu', 394], ['Net/Linear[fc2]/onnx::Gemm', 2167]]), (403956, [['Net/Linear[fc1]/onnx::Gemm', 401408], ['Net/ReLU[relu]/onnx::Relu', 392], ['Net/Linear[fc2]/onnx::Gemm', 2156]]), (401895, [['Net/Linear[fc1]/onnx::Gemm', 399360], ['Net/ReLU[relu]/onnx::Relu', 390], ['Net/Linear[fc2]/onnx::Gemm', 2145]]), (399834, [['Net/Linear[fc1]/onnx::Gemm', 397312], ['Net/ReLU[relu]/onnx::Relu', 388], ['Net/Linear[fc2]/onnx::Gemm', 2134]]), (397773, [['Net/Linear[fc1]/onnx::Gemm', 395264], ['Net/ReLU[relu]/onnx::Relu', 386], ['Net/Linear[fc2]/onnx::Gemm', 2123]]), (395712, [['Net/Linear[fc1]/onnx::Gemm', 393216], ['Net/ReLU[relu]/onnx::Relu', 384], ['Net/Linear[fc2]/onnx::Gemm', 2112]]), (393651, [['Net/Linear[fc1]/onnx::Gemm', 391168], ['Net/ReLU[relu]/onnx::Relu', 382], ['Net/Linear[fc2]/onnx::Gemm', 2101]]), (391590, [['Net/Linear[fc1]/onnx::Gemm', 389120], ['Net/ReLU[relu]/onnx::Relu', 380], ['Net/Linear[fc2]/onnx::Gemm', 2090]]), (389529, [['Net/Linear[fc1]/onnx::Gemm', 387072], ['Net/ReLU[relu]/onnx::Relu', 378], ['Net/Linear[fc2]/onnx::Gemm', 2079]]), (387468, [['Net/Linear[fc1]/onnx::Gemm', 385024], ['Net/ReLU[relu]/onnx::Relu', 376], ['Net/Linear[fc2]/onnx::Gemm', 2068]]), (385407, [['Net/Linear[fc1]/onnx::Gemm', 382976], ['Net/ReLU[relu]/onnx::Relu', 374], ['Net/Linear[fc2]/onnx::Gemm', 2057]]), (383346, [['Net/Linear[fc1]/onnx::Gemm', 380928], ['Net/ReLU[relu]/onnx::Relu', 372], ['Net/Linear[fc2]/onnx::Gemm', 2046]]), (381285, [['Net/Linear[fc1]/onnx::Gemm', 378880], ['Net/ReLU[relu]/onnx::Relu', 370], ['Net/Linear[fc2]/onnx::Gemm', 2035]]), (379224, [['Net/Linear[fc1]/onnx::Gemm', 376832], ['Net/ReLU[relu]/onnx::Relu', 368], ['Net/Linear[fc2]/onnx::Gemm', 2024]]), (377163, [['Net/Linear[fc1]/onnx::Gemm', 374784], ['Net/ReLU[relu]/onnx::Relu', 366], ['Net/Linear[fc2]/onnx::Gemm', 2013]]), (375102, [['Net/Linear[fc1]/onnx::Gemm', 372736], ['Net/ReLU[relu]/onnx::Relu', 364], ['Net/Linear[fc2]/onnx::Gemm', 2002]]), (373041, [['Net/Linear[fc1]/onnx::Gemm', 370688], ['Net/ReLU[relu]/onnx::Relu', 362], ['Net/Linear[fc2]/onnx::Gemm', 1991]]), (370980, [['Net/Linear[fc1]/onnx::Gemm', 368640], ['Net/ReLU[relu]/onnx::Relu', 360], ['Net/Linear[fc2]/onnx::Gemm', 1980]]), (368919, [['Net/Linear[fc1]/onnx::Gemm', 366592], ['Net/ReLU[relu]/onnx::Relu', 358], ['Net/Linear[fc2]/onnx::Gemm', 1969]]), (366858, [['Net/Linear[fc1]/onnx::Gemm', 364544], ['Net/ReLU[relu]/onnx::Relu', 356], ['Net/Linear[fc2]/onnx::Gemm', 1958]]), (364797, [['Net/Linear[fc1]/onnx::Gemm', 362496], ['Net/ReLU[relu]/onnx::Relu', 354], ['Net/Linear[fc2]/onnx::Gemm', 1947]]), (362736, [['Net/Linear[fc1]/onnx::Gemm', 360448], ['Net/ReLU[relu]/onnx::Relu', 352], ['Net/Linear[fc2]/onnx::Gemm', 1936]]), (360675, [['Net/Linear[fc1]/onnx::Gemm', 358400], ['Net/ReLU[relu]/onnx::Relu', 350], ['Net/Linear[fc2]/onnx::Gemm', 1925]]), (358614, [['Net/Linear[fc1]/onnx::Gemm', 356352], ['Net/ReLU[relu]/onnx::Relu', 348], ['Net/Linear[fc2]/onnx::Gemm', 1914]]), (356553, [['Net/Linear[fc1]/onnx::Gemm', 354304], ['Net/ReLU[relu]/onnx::Relu', 346], ['Net/Linear[fc2]/onnx::Gemm', 1903]]), (354492, [['Net/Linear[fc1]/onnx::Gemm', 352256], ['Net/ReLU[relu]/onnx::Relu', 344], ['Net/Linear[fc2]/onnx::Gemm', 1892]]), (352431, [['Net/Linear[fc1]/onnx::Gemm', 350208], ['Net/ReLU[relu]/onnx::Relu', 342], ['Net/Linear[fc2]/onnx::Gemm', 1881]]), (350370, [['Net/Linear[fc1]/onnx::Gemm', 348160], ['Net/ReLU[relu]/onnx::Relu', 340], ['Net/Linear[fc2]/onnx::Gemm', 1870]]), (348309, [['Net/Linear[fc1]/onnx::Gemm', 346112], ['Net/ReLU[relu]/onnx::Relu', 338], ['Net/Linear[fc2]/onnx::Gemm', 1859]]), (346248, [['Net/Linear[fc1]/onnx::Gemm', 344064], ['Net/ReLU[relu]/onnx::Relu', 336], ['Net/Linear[fc2]/onnx::Gemm', 1848]]), (344187, [['Net/Linear[fc1]/onnx::Gemm', 342016], ['Net/ReLU[relu]/onnx::Relu', 334], ['Net/Linear[fc2]/onnx::Gemm', 1837]]), (342126, [['Net/Linear[fc1]/onnx::Gemm', 339968], ['Net/ReLU[relu]/onnx::Relu', 332], ['Net/Linear[fc2]/onnx::Gemm', 1826]]), (340065, [['Net/Linear[fc1]/onnx::Gemm', 337920], ['Net/ReLU[relu]/onnx::Relu', 330], ['Net/Linear[fc2]/onnx::Gemm', 1815]]), (338004, [['Net/Linear[fc1]/onnx::Gemm', 335872], ['Net/ReLU[relu]/onnx::Relu', 328], ['Net/Linear[fc2]/onnx::Gemm', 1804]]), (335943, [['Net/Linear[fc1]/onnx::Gemm', 333824], ['Net/ReLU[relu]/onnx::Relu', 326], ['Net/Linear[fc2]/onnx::Gemm', 1793]]), (333882, [['Net/Linear[fc1]/onnx::Gemm', 331776], ['Net/ReLU[relu]/onnx::Relu', 324], ['Net/Linear[fc2]/onnx::Gemm', 1782]]), (331821, [['Net/Linear[fc1]/onnx::Gemm', 329728], ['Net/ReLU[relu]/onnx::Relu', 322], ['Net/Linear[fc2]/onnx::Gemm', 1771]]), (329760, [['Net/Linear[fc1]/onnx::Gemm', 327680], ['Net/ReLU[relu]/onnx::Relu', 320], ['Net/Linear[fc2]/onnx::Gemm', 1760]]), (327699, [['Net/Linear[fc1]/onnx::Gemm', 325632], ['Net/ReLU[relu]/onnx::Relu', 318], ['Net/Linear[fc2]/onnx::Gemm', 1749]]), (325638, [['Net/Linear[fc1]/onnx::Gemm', 323584], ['Net/ReLU[relu]/onnx::Relu', 316], ['Net/Linear[fc2]/onnx::Gemm', 1738]]), (323577, [['Net/Linear[fc1]/onnx::Gemm', 321536], ['Net/ReLU[relu]/onnx::Relu', 314], ['Net/Linear[fc2]/onnx::Gemm', 1727]]), (321516, [['Net/Linear[fc1]/onnx::Gemm', 319488], ['Net/ReLU[relu]/onnx::Relu', 312], ['Net/Linear[fc2]/onnx::Gemm', 1716]]), (319455, [['Net/Linear[fc1]/onnx::Gemm', 317440], ['Net/ReLU[relu]/onnx::Relu', 310], ['Net/Linear[fc2]/onnx::Gemm', 1705]]), (317394, [['Net/Linear[fc1]/onnx::Gemm', 315392], ['Net/ReLU[relu]/onnx::Relu', 308], ['Net/Linear[fc2]/onnx::Gemm', 1694]]), (315333, [['Net/Linear[fc1]/onnx::Gemm', 313344], ['Net/ReLU[relu]/onnx::Relu', 306], ['Net/Linear[fc2]/onnx::Gemm', 1683]]), (313272, [['Net/Linear[fc1]/onnx::Gemm', 311296], ['Net/ReLU[relu]/onnx::Relu', 304], ['Net/Linear[fc2]/onnx::Gemm', 1672]]), (311211, [['Net/Linear[fc1]/onnx::Gemm', 309248], ['Net/ReLU[relu]/onnx::Relu', 302], ['Net/Linear[fc2]/onnx::Gemm', 1661]]), (309150, [['Net/Linear[fc1]/onnx::Gemm', 307200], ['Net/ReLU[relu]/onnx::Relu', 300], ['Net/Linear[fc2]/onnx::Gemm', 1650]]), (307089, [['Net/Linear[fc1]/onnx::Gemm', 305152], ['Net/ReLU[relu]/onnx::Relu', 298], ['Net/Linear[fc2]/onnx::Gemm', 1639]]), (305028, [['Net/Linear[fc1]/onnx::Gemm', 303104], ['Net/ReLU[relu]/onnx::Relu', 296], ['Net/Linear[fc2]/onnx::Gemm', 1628]]), (302967, [['Net/Linear[fc1]/onnx::Gemm', 301056], ['Net/ReLU[relu]/onnx::Relu', 294], ['Net/Linear[fc2]/onnx::Gemm', 1617]]), (300906, [['Net/Linear[fc1]/onnx::Gemm', 299008], ['Net/ReLU[relu]/onnx::Relu', 292], ['Net/Linear[fc2]/onnx::Gemm', 1606]]), (298845, [['Net/Linear[fc1]/onnx::Gemm', 296960], ['Net/ReLU[relu]/onnx::Relu', 290], ['Net/Linear[fc2]/onnx::Gemm', 1595]]), (296784, [['Net/Linear[fc1]/onnx::Gemm', 294912], ['Net/ReLU[relu]/onnx::Relu', 288], ['Net/Linear[fc2]/onnx::Gemm', 1584]]), (294723, [['Net/Linear[fc1]/onnx::Gemm', 292864], ['Net/ReLU[relu]/onnx::Relu', 286], ['Net/Linear[fc2]/onnx::Gemm', 1573]]), (292662, [['Net/Linear[fc1]/onnx::Gemm', 290816], ['Net/ReLU[relu]/onnx::Relu', 284], ['Net/Linear[fc2]/onnx::Gemm', 1562]]), (290601, [['Net/Linear[fc1]/onnx::Gemm', 288768], ['Net/ReLU[relu]/onnx::Relu', 282], ['Net/Linear[fc2]/onnx::Gemm', 1551]]), (288540, [['Net/Linear[fc1]/onnx::Gemm', 286720], ['Net/ReLU[relu]/onnx::Relu', 280], ['Net/Linear[fc2]/onnx::Gemm', 1540]]), (286479, [['Net/Linear[fc1]/onnx::Gemm', 284672], ['Net/ReLU[relu]/onnx::Relu', 278], ['Net/Linear[fc2]/onnx::Gemm', 1529]]), (284418, [['Net/Linear[fc1]/onnx::Gemm', 282624], ['Net/ReLU[relu]/onnx::Relu', 276], ['Net/Linear[fc2]/onnx::Gemm', 1518]]), (282357, [['Net/Linear[fc1]/onnx::Gemm', 280576], ['Net/ReLU[relu]/onnx::Relu', 274], ['Net/Linear[fc2]/onnx::Gemm', 1507]]), (280296, [['Net/Linear[fc1]/onnx::Gemm', 278528], ['Net/ReLU[relu]/onnx::Relu', 272], ['Net/Linear[fc2]/onnx::Gemm', 1496]]), (278235, [['Net/Linear[fc1]/onnx::Gemm', 276480], ['Net/ReLU[relu]/onnx::Relu', 270], ['Net/Linear[fc2]/onnx::Gemm', 1485]]), (276174, [['Net/Linear[fc1]/onnx::Gemm', 274432], ['Net/ReLU[relu]/onnx::Relu', 268], ['Net/Linear[fc2]/onnx::Gemm', 1474]]), (274113, [['Net/Linear[fc1]/onnx::Gemm', 272384], ['Net/ReLU[relu]/onnx::Relu', 266], ['Net/Linear[fc2]/onnx::Gemm', 1463]]), (272052, [['Net/Linear[fc1]/onnx::Gemm', 270336], ['Net/ReLU[relu]/onnx::Relu', 264], ['Net/Linear[fc2]/onnx::Gemm', 1452]]), (269991, [['Net/Linear[fc1]/onnx::Gemm', 268288], ['Net/ReLU[relu]/onnx::Relu', 262], ['Net/Linear[fc2]/onnx::Gemm', 1441]]), (267930, [['Net/Linear[fc1]/onnx::Gemm', 266240], ['Net/ReLU[relu]/onnx::Relu', 260], ['Net/Linear[fc2]/onnx::Gemm', 1430]]), (265869, [['Net/Linear[fc1]/onnx::Gemm', 264192], ['Net/ReLU[relu]/onnx::Relu', 258], ['Net/Linear[fc2]/onnx::Gemm', 1419]]), (263808, [['Net/Linear[fc1]/onnx::Gemm', 262144], ['Net/ReLU[relu]/onnx::Relu', 256], ['Net/Linear[fc2]/onnx::Gemm', 1408]]), (261747, [['Net/Linear[fc1]/onnx::Gemm', 260096], ['Net/ReLU[relu]/onnx::Relu', 254], ['Net/Linear[fc2]/onnx::Gemm', 1397]]), (259686, [['Net/Linear[fc1]/onnx::Gemm', 258048], ['Net/ReLU[relu]/onnx::Relu', 252], ['Net/Linear[fc2]/onnx::Gemm', 1386]]), (257625, [['Net/Linear[fc1]/onnx::Gemm', 256000], ['Net/ReLU[relu]/onnx::Relu', 250], ['Net/Linear[fc2]/onnx::Gemm', 1375]]), (255564, [['Net/Linear[fc1]/onnx::Gemm', 253952], ['Net/ReLU[relu]/onnx::Relu', 248], ['Net/Linear[fc2]/onnx::Gemm', 1364]]), (253503, [['Net/Linear[fc1]/onnx::Gemm', 251904], ['Net/ReLU[relu]/onnx::Relu', 246], ['Net/Linear[fc2]/onnx::Gemm', 1353]]), (251442, [['Net/Linear[fc1]/onnx::Gemm', 249856], ['Net/ReLU[relu]/onnx::Relu', 244], ['Net/Linear[fc2]/onnx::Gemm', 1342]]), (249381, [['Net/Linear[fc1]/onnx::Gemm', 247808], ['Net/ReLU[relu]/onnx::Relu', 242], ['Net/Linear[fc2]/onnx::Gemm', 1331]]), (247320, [['Net/Linear[fc1]/onnx::Gemm', 245760], ['Net/ReLU[relu]/onnx::Relu', 240], ['Net/Linear[fc2]/onnx::Gemm', 1320]]), (245259, [['Net/Linear[fc1]/onnx::Gemm', 243712], ['Net/ReLU[relu]/onnx::Relu', 238], ['Net/Linear[fc2]/onnx::Gemm', 1309]]), (243198, [['Net/Linear[fc1]/onnx::Gemm', 241664], ['Net/ReLU[relu]/onnx::Relu', 236], ['Net/Linear[fc2]/onnx::Gemm', 1298]]), (241137, [['Net/Linear[fc1]/onnx::Gemm', 239616], ['Net/ReLU[relu]/onnx::Relu', 234], ['Net/Linear[fc2]/onnx::Gemm', 1287]]), (239076, [['Net/Linear[fc1]/onnx::Gemm', 237568], ['Net/ReLU[relu]/onnx::Relu', 232], ['Net/Linear[fc2]/onnx::Gemm', 1276]]), (237015, [['Net/Linear[fc1]/onnx::Gemm', 235520], ['Net/ReLU[relu]/onnx::Relu', 230], ['Net/Linear[fc2]/onnx::Gemm', 1265]]), (234954, [['Net/Linear[fc1]/onnx::Gemm', 233472], ['Net/ReLU[relu]/onnx::Relu', 228], ['Net/Linear[fc2]/onnx::Gemm', 1254]]), (232893, [['Net/Linear[fc1]/onnx::Gemm', 231424], ['Net/ReLU[relu]/onnx::Relu', 226], ['Net/Linear[fc2]/onnx::Gemm', 1243]]), (230832, [['Net/Linear[fc1]/onnx::Gemm', 229376], ['Net/ReLU[relu]/onnx::Relu', 224], ['Net/Linear[fc2]/onnx::Gemm', 1232]]), (228771, [['Net/Linear[fc1]/onnx::Gemm', 227328], ['Net/ReLU[relu]/onnx::Relu', 222], ['Net/Linear[fc2]/onnx::Gemm', 1221]]), (226710, [['Net/Linear[fc1]/onnx::Gemm', 225280], ['Net/ReLU[relu]/onnx::Relu', 220], ['Net/Linear[fc2]/onnx::Gemm', 1210]]), (224649, [['Net/Linear[fc1]/onnx::Gemm', 223232], ['Net/ReLU[relu]/onnx::Relu', 218], ['Net/Linear[fc2]/onnx::Gemm', 1199]]), (222588, [['Net/Linear[fc1]/onnx::Gemm', 221184], ['Net/ReLU[relu]/onnx::Relu', 216], ['Net/Linear[fc2]/onnx::Gemm', 1188]]), (220527, [['Net/Linear[fc1]/onnx::Gemm', 219136], ['Net/ReLU[relu]/onnx::Relu', 214], ['Net/Linear[fc2]/onnx::Gemm', 1177]]), (218466, [['Net/Linear[fc1]/onnx::Gemm', 217088], ['Net/ReLU[relu]/onnx::Relu', 212], ['Net/Linear[fc2]/onnx::Gemm', 1166]]), (216405, [['Net/Linear[fc1]/onnx::Gemm', 215040], ['Net/ReLU[relu]/onnx::Relu', 210], ['Net/Linear[fc2]/onnx::Gemm', 1155]]), (214344, [['Net/Linear[fc1]/onnx::Gemm', 212992], ['Net/ReLU[relu]/onnx::Relu', 208], ['Net/Linear[fc2]/onnx::Gemm', 1144]]), (212283, [['Net/Linear[fc1]/onnx::Gemm', 210944], ['Net/ReLU[relu]/onnx::Relu', 206], ['Net/Linear[fc2]/onnx::Gemm', 1133]]), (210222, [['Net/Linear[fc1]/onnx::Gemm', 208896], ['Net/ReLU[relu]/onnx::Relu', 204], ['Net/Linear[fc2]/onnx::Gemm', 1122]]), (208161, [['Net/Linear[fc1]/onnx::Gemm', 206848], ['Net/ReLU[relu]/onnx::Relu', 202], ['Net/Linear[fc2]/onnx::Gemm', 1111]]), (206100, [['Net/Linear[fc1]/onnx::Gemm', 204800], ['Net/ReLU[relu]/onnx::Relu', 200], ['Net/Linear[fc2]/onnx::Gemm', 1100]]), (204039, [['Net/Linear[fc1]/onnx::Gemm', 202752], ['Net/ReLU[relu]/onnx::Relu', 198], ['Net/Linear[fc2]/onnx::Gemm', 1089]]), (201978, [['Net/Linear[fc1]/onnx::Gemm', 200704], ['Net/ReLU[relu]/onnx::Relu', 196], ['Net/Linear[fc2]/onnx::Gemm', 1078]]), (199917, [['Net/Linear[fc1]/onnx::Gemm', 198656], ['Net/ReLU[relu]/onnx::Relu', 194], ['Net/Linear[fc2]/onnx::Gemm', 1067]]), (197856, [['Net/Linear[fc1]/onnx::Gemm', 196608], ['Net/ReLU[relu]/onnx::Relu', 192], ['Net/Linear[fc2]/onnx::Gemm', 1056]]), (195795, [['Net/Linear[fc1]/onnx::Gemm', 194560], ['Net/ReLU[relu]/onnx::Relu', 190], ['Net/Linear[fc2]/onnx::Gemm', 1045]]), (193734, [['Net/Linear[fc1]/onnx::Gemm', 192512], ['Net/ReLU[relu]/onnx::Relu', 188], ['Net/Linear[fc2]/onnx::Gemm', 1034]]), (191673, [['Net/Linear[fc1]/onnx::Gemm', 190464], ['Net/ReLU[relu]/onnx::Relu', 186], ['Net/Linear[fc2]/onnx::Gemm', 1023]]), (189612, [['Net/Linear[fc1]/onnx::Gemm', 188416], ['Net/ReLU[relu]/onnx::Relu', 184], ['Net/Linear[fc2]/onnx::Gemm', 1012]]), (187551, [['Net/Linear[fc1]/onnx::Gemm', 186368], ['Net/ReLU[relu]/onnx::Relu', 182], ['Net/Linear[fc2]/onnx::Gemm', 1001]]), (185490, [['Net/Linear[fc1]/onnx::Gemm', 184320], ['Net/ReLU[relu]/onnx::Relu', 180], ['Net/Linear[fc2]/onnx::Gemm', 990]]), (183429, [['Net/Linear[fc1]/onnx::Gemm', 182272], ['Net/ReLU[relu]/onnx::Relu', 178], ['Net/Linear[fc2]/onnx::Gemm', 979]]), (181368, [['Net/Linear[fc1]/onnx::Gemm', 180224], ['Net/ReLU[relu]/onnx::Relu', 176], ['Net/Linear[fc2]/onnx::Gemm', 968]]), (179307, [['Net/Linear[fc1]/onnx::Gemm', 178176], ['Net/ReLU[relu]/onnx::Relu', 174], ['Net/Linear[fc2]/onnx::Gemm', 957]]), (177246, [['Net/Linear[fc1]/onnx::Gemm', 176128], ['Net/ReLU[relu]/onnx::Relu', 172], ['Net/Linear[fc2]/onnx::Gemm', 946]]), (175185, [['Net/Linear[fc1]/onnx::Gemm', 174080], ['Net/ReLU[relu]/onnx::Relu', 170], ['Net/Linear[fc2]/onnx::Gemm', 935]]), (173124, [['Net/Linear[fc1]/onnx::Gemm', 172032], ['Net/ReLU[relu]/onnx::Relu', 168], ['Net/Linear[fc2]/onnx::Gemm', 924]]), (171063, [['Net/Linear[fc1]/onnx::Gemm', 169984], ['Net/ReLU[relu]/onnx::Relu', 166], ['Net/Linear[fc2]/onnx::Gemm', 913]]), (169002, [['Net/Linear[fc1]/onnx::Gemm', 167936], ['Net/ReLU[relu]/onnx::Relu', 164], ['Net/Linear[fc2]/onnx::Gemm', 902]]), (166941, [['Net/Linear[fc1]/onnx::Gemm', 165888], ['Net/ReLU[relu]/onnx::Relu', 162], ['Net/Linear[fc2]/onnx::Gemm', 891]]), (164880, [['Net/Linear[fc1]/onnx::Gemm', 163840], ['Net/ReLU[relu]/onnx::Relu', 160], ['Net/Linear[fc2]/onnx::Gemm', 880]]), (162819, [['Net/Linear[fc1]/onnx::Gemm', 161792], ['Net/ReLU[relu]/onnx::Relu', 158], ['Net/Linear[fc2]/onnx::Gemm', 869]]), (160758, [['Net/Linear[fc1]/onnx::Gemm', 159744], ['Net/ReLU[relu]/onnx::Relu', 156], ['Net/Linear[fc2]/onnx::Gemm', 858]]), (158697, [['Net/Linear[fc1]/onnx::Gemm', 157696], ['Net/ReLU[relu]/onnx::Relu', 154], ['Net/Linear[fc2]/onnx::Gemm', 847]]), (156636, [['Net/Linear[fc1]/onnx::Gemm', 155648], ['Net/ReLU[relu]/onnx::Relu', 152], ['Net/Linear[fc2]/onnx::Gemm', 836]]), (154575, [['Net/Linear[fc1]/onnx::Gemm', 153600], ['Net/ReLU[relu]/onnx::Relu', 150], ['Net/Linear[fc2]/onnx::Gemm', 825]]), (152514, [['Net/Linear[fc1]/onnx::Gemm', 151552], ['Net/ReLU[relu]/onnx::Relu', 148], ['Net/Linear[fc2]/onnx::Gemm', 814]]), (150453, [['Net/Linear[fc1]/onnx::Gemm', 149504], ['Net/ReLU[relu]/onnx::Relu', 146], ['Net/Linear[fc2]/onnx::Gemm', 803]]), (148392, [['Net/Linear[fc1]/onnx::Gemm', 147456], ['Net/ReLU[relu]/onnx::Relu', 144], ['Net/Linear[fc2]/onnx::Gemm', 792]]), (146331, [['Net/Linear[fc1]/onnx::Gemm', 145408], ['Net/ReLU[relu]/onnx::Relu', 142], ['Net/Linear[fc2]/onnx::Gemm', 781]]), (144270, [['Net/Linear[fc1]/onnx::Gemm', 143360], ['Net/ReLU[relu]/onnx::Relu', 140], ['Net/Linear[fc2]/onnx::Gemm', 770]]), (142209, [['Net/Linear[fc1]/onnx::Gemm', 141312], ['Net/ReLU[relu]/onnx::Relu', 138], ['Net/Linear[fc2]/onnx::Gemm', 759]]), (140148, [['Net/Linear[fc1]/onnx::Gemm', 139264], ['Net/ReLU[relu]/onnx::Relu', 136], ['Net/Linear[fc2]/onnx::Gemm', 748]]), (138087, [['Net/Linear[fc1]/onnx::Gemm', 137216], ['Net/ReLU[relu]/onnx::Relu', 134], ['Net/Linear[fc2]/onnx::Gemm', 737]]), (136026, [['Net/Linear[fc1]/onnx::Gemm', 135168], ['Net/ReLU[relu]/onnx::Relu', 132], ['Net/Linear[fc2]/onnx::Gemm', 726]]), (133965, [['Net/Linear[fc1]/onnx::Gemm', 133120], ['Net/ReLU[relu]/onnx::Relu', 130], ['Net/Linear[fc2]/onnx::Gemm', 715]]), (131904, [['Net/Linear[fc1]/onnx::Gemm', 131072], ['Net/ReLU[relu]/onnx::Relu', 128], ['Net/Linear[fc2]/onnx::Gemm', 704]]), (129843, [['Net/Linear[fc1]/onnx::Gemm', 129024], ['Net/ReLU[relu]/onnx::Relu', 126], ['Net/Linear[fc2]/onnx::Gemm', 693]]), (127782, [['Net/Linear[fc1]/onnx::Gemm', 126976], ['Net/ReLU[relu]/onnx::Relu', 124], ['Net/Linear[fc2]/onnx::Gemm', 682]]), (125721, [['Net/Linear[fc1]/onnx::Gemm', 124928], ['Net/ReLU[relu]/onnx::Relu', 122], ['Net/Linear[fc2]/onnx::Gemm', 671]]), (123660, [['Net/Linear[fc1]/onnx::Gemm', 122880], ['Net/ReLU[relu]/onnx::Relu', 120], ['Net/Linear[fc2]/onnx::Gemm', 660]]), (121599, [['Net/Linear[fc1]/onnx::Gemm', 120832], ['Net/ReLU[relu]/onnx::Relu', 118], ['Net/Linear[fc2]/onnx::Gemm', 649]]), (119538, [['Net/Linear[fc1]/onnx::Gemm', 118784], ['Net/ReLU[relu]/onnx::Relu', 116], ['Net/Linear[fc2]/onnx::Gemm', 638]]), (117477, [['Net/Linear[fc1]/onnx::Gemm', 116736], ['Net/ReLU[relu]/onnx::Relu', 114], ['Net/Linear[fc2]/onnx::Gemm', 627]]), (115416, [['Net/Linear[fc1]/onnx::Gemm', 114688], ['Net/ReLU[relu]/onnx::Relu', 112], ['Net/Linear[fc2]/onnx::Gemm', 616]]), (113355, [['Net/Linear[fc1]/onnx::Gemm', 112640], ['Net/ReLU[relu]/onnx::Relu', 110], ['Net/Linear[fc2]/onnx::Gemm', 605]]), (111294, [['Net/Linear[fc1]/onnx::Gemm', 110592], ['Net/ReLU[relu]/onnx::Relu', 108], ['Net/Linear[fc2]/onnx::Gemm', 594]]), (109233, [['Net/Linear[fc1]/onnx::Gemm', 108544], ['Net/ReLU[relu]/onnx::Relu', 106], ['Net/Linear[fc2]/onnx::Gemm', 583]]), (107172, [['Net/Linear[fc1]/onnx::Gemm', 106496], ['Net/ReLU[relu]/onnx::Relu', 104], ['Net/Linear[fc2]/onnx::Gemm', 572]]), (105111, [['Net/Linear[fc1]/onnx::Gemm', 104448], ['Net/ReLU[relu]/onnx::Relu', 102], ['Net/Linear[fc2]/onnx::Gemm', 561]]), (103050, [['Net/Linear[fc1]/onnx::Gemm', 102400], ['Net/ReLU[relu]/onnx::Relu', 100], ['Net/Linear[fc2]/onnx::Gemm', 550]]), (100989, [['Net/Linear[fc1]/onnx::Gemm', 100352], ['Net/ReLU[relu]/onnx::Relu', 98], ['Net/Linear[fc2]/onnx::Gemm', 539]]), (98928, [['Net/Linear[fc1]/onnx::Gemm', 98304], ['Net/ReLU[relu]/onnx::Relu', 96], ['Net/Linear[fc2]/onnx::Gemm', 528]]), (96867, [['Net/Linear[fc1]/onnx::Gemm', 96256], ['Net/ReLU[relu]/onnx::Relu', 94], ['Net/Linear[fc2]/onnx::Gemm', 517]]), (94806, [['Net/Linear[fc1]/onnx::Gemm', 94208], ['Net/ReLU[relu]/onnx::Relu', 92], ['Net/Linear[fc2]/onnx::Gemm', 506]]), (92745, [['Net/Linear[fc1]/onnx::Gemm', 92160], ['Net/ReLU[relu]/onnx::Relu', 90], ['Net/Linear[fc2]/onnx::Gemm', 495]]), (90684, [['Net/Linear[fc1]/onnx::Gemm', 90112], ['Net/ReLU[relu]/onnx::Relu', 88], ['Net/Linear[fc2]/onnx::Gemm', 484]]), (88623, [['Net/Linear[fc1]/onnx::Gemm', 88064], ['Net/ReLU[relu]/onnx::Relu', 86], ['Net/Linear[fc2]/onnx::Gemm', 473]]), (86562, [['Net/Linear[fc1]/onnx::Gemm', 86016], ['Net/ReLU[relu]/onnx::Relu', 84], ['Net/Linear[fc2]/onnx::Gemm', 462]]), (84501, [['Net/Linear[fc1]/onnx::Gemm', 83968], ['Net/ReLU[relu]/onnx::Relu', 82], ['Net/Linear[fc2]/onnx::Gemm', 451]]), (82440, [['Net/Linear[fc1]/onnx::Gemm', 81920], ['Net/ReLU[relu]/onnx::Relu', 80], ['Net/Linear[fc2]/onnx::Gemm', 440]]), (80379, [['Net/Linear[fc1]/onnx::Gemm', 79872], ['Net/ReLU[relu]/onnx::Relu', 78], ['Net/Linear[fc2]/onnx::Gemm', 429]]), (78318, [['Net/Linear[fc1]/onnx::Gemm', 77824], ['Net/ReLU[relu]/onnx::Relu', 76], ['Net/Linear[fc2]/onnx::Gemm', 418]]), (76257, [['Net/Linear[fc1]/onnx::Gemm', 75776], ['Net/ReLU[relu]/onnx::Relu', 74], ['Net/Linear[fc2]/onnx::Gemm', 407]]), (74196, [['Net/Linear[fc1]/onnx::Gemm', 73728], ['Net/ReLU[relu]/onnx::Relu', 72], ['Net/Linear[fc2]/onnx::Gemm', 396]]), (72135, [['Net/Linear[fc1]/onnx::Gemm', 71680], ['Net/ReLU[relu]/onnx::Relu', 70], ['Net/Linear[fc2]/onnx::Gemm', 385]]), (70074, [['Net/Linear[fc1]/onnx::Gemm', 69632], ['Net/ReLU[relu]/onnx::Relu', 68], ['Net/Linear[fc2]/onnx::Gemm', 374]]), (68013, [['Net/Linear[fc1]/onnx::Gemm', 67584], ['Net/ReLU[relu]/onnx::Relu', 66], ['Net/Linear[fc2]/onnx::Gemm', 363]]), (65952, [['Net/Linear[fc1]/onnx::Gemm', 65536], ['Net/ReLU[relu]/onnx::Relu', 64], ['Net/Linear[fc2]/onnx::Gemm', 352]]), (63891, [['Net/Linear[fc1]/onnx::Gemm', 63488], ['Net/ReLU[relu]/onnx::Relu', 62], ['Net/Linear[fc2]/onnx::Gemm', 341]]), (61830, [['Net/Linear[fc1]/onnx::Gemm', 61440], ['Net/ReLU[relu]/onnx::Relu', 60], ['Net/Linear[fc2]/onnx::Gemm', 330]]), (59769, [['Net/Linear[fc1]/onnx::Gemm', 59392], ['Net/ReLU[relu]/onnx::Relu', 58], ['Net/Linear[fc2]/onnx::Gemm', 319]]), (57708, [['Net/Linear[fc1]/onnx::Gemm', 57344], ['Net/ReLU[relu]/onnx::Relu', 56], ['Net/Linear[fc2]/onnx::Gemm', 308]]), (55647, [['Net/Linear[fc1]/onnx::Gemm', 55296], ['Net/ReLU[relu]/onnx::Relu', 54], ['Net/Linear[fc2]/onnx::Gemm', 297]]), (53586, [['Net/Linear[fc1]/onnx::Gemm', 53248], ['Net/ReLU[relu]/onnx::Relu', 52], ['Net/Linear[fc2]/onnx::Gemm', 286]]), (51525, [['Net/Linear[fc1]/onnx::Gemm', 51200], ['Net/ReLU[relu]/onnx::Relu', 50], ['Net/Linear[fc2]/onnx::Gemm', 275]]), (49464, [['Net/Linear[fc1]/onnx::Gemm', 49152], ['Net/ReLU[relu]/onnx::Relu', 48], ['Net/Linear[fc2]/onnx::Gemm', 264]]), (47403, [['Net/Linear[fc1]/onnx::Gemm', 47104], ['Net/ReLU[relu]/onnx::Relu', 46], ['Net/Linear[fc2]/onnx::Gemm', 253]]), (45342, [['Net/Linear[fc1]/onnx::Gemm', 45056], ['Net/ReLU[relu]/onnx::Relu', 44], ['Net/Linear[fc2]/onnx::Gemm', 242]]), (43281, [['Net/Linear[fc1]/onnx::Gemm', 43008], ['Net/ReLU[relu]/onnx::Relu', 42], ['Net/Linear[fc2]/onnx::Gemm', 231]]), (41220, [['Net/Linear[fc1]/onnx::Gemm', 40960], ['Net/ReLU[relu]/onnx::Relu', 40], ['Net/Linear[fc2]/onnx::Gemm', 220]]), (39159, [['Net/Linear[fc1]/onnx::Gemm', 38912], ['Net/ReLU[relu]/onnx::Relu', 38], ['Net/Linear[fc2]/onnx::Gemm', 209]]), (37098, [['Net/Linear[fc1]/onnx::Gemm', 36864], ['Net/ReLU[relu]/onnx::Relu', 36], ['Net/Linear[fc2]/onnx::Gemm', 198]]), (35037, [['Net/Linear[fc1]/onnx::Gemm', 34816], ['Net/ReLU[relu]/onnx::Relu', 34], ['Net/Linear[fc2]/onnx::Gemm', 187]]), (32976, [['Net/Linear[fc1]/onnx::Gemm', 32768], ['Net/ReLU[relu]/onnx::Relu', 32], ['Net/Linear[fc2]/onnx::Gemm', 176]]), (30915, [['Net/Linear[fc1]/onnx::Gemm', 30720], ['Net/ReLU[relu]/onnx::Relu', 30], ['Net/Linear[fc2]/onnx::Gemm', 165]]), (28854, [['Net/Linear[fc1]/onnx::Gemm', 28672], ['Net/ReLU[relu]/onnx::Relu', 28], ['Net/Linear[fc2]/onnx::Gemm', 154]]), (26793, [['Net/Linear[fc1]/onnx::Gemm', 26624], ['Net/ReLU[relu]/onnx::Relu', 26], ['Net/Linear[fc2]/onnx::Gemm', 143]]), (24732, [['Net/Linear[fc1]/onnx::Gemm', 24576], ['Net/ReLU[relu]/onnx::Relu', 24], ['Net/Linear[fc2]/onnx::Gemm', 132]]), (22671, [['Net/Linear[fc1]/onnx::Gemm', 22528], ['Net/ReLU[relu]/onnx::Relu', 22], ['Net/Linear[fc2]/onnx::Gemm', 121]]), (20610, [['Net/Linear[fc1]/onnx::Gemm', 20480], ['Net/ReLU[relu]/onnx::Relu', 20], ['Net/Linear[fc2]/onnx::Gemm', 110]]), (18549, [['Net/Linear[fc1]/onnx::Gemm', 18432], ['Net/ReLU[relu]/onnx::Relu', 18], ['Net/Linear[fc2]/onnx::Gemm', 99]]), (16488, [['Net/Linear[fc1]/onnx::Gemm', 16384], ['Net/ReLU[relu]/onnx::Relu', 16], ['Net/Linear[fc2]/onnx::Gemm', 88]]), (14427, [['Net/Linear[fc1]/onnx::Gemm', 14336], ['Net/ReLU[relu]/onnx::Relu', 14], ['Net/Linear[fc2]/onnx::Gemm', 77]]), (12366, [['Net/Linear[fc1]/onnx::Gemm', 12288], ['Net/ReLU[relu]/onnx::Relu', 12], ['Net/Linear[fc2]/onnx::Gemm', 66]]), (10305, [['Net/Linear[fc1]/onnx::Gemm', 10240], ['Net/ReLU[relu]/onnx::Relu', 10], ['Net/Linear[fc2]/onnx::Gemm', 55]]), (8244, [['Net/Linear[fc1]/onnx::Gemm', 8192], ['Net/ReLU[relu]/onnx::Relu', 8], ['Net/Linear[fc2]/onnx::Gemm', 44]]), (6183, [['Net/Linear[fc1]/onnx::Gemm', 6144], ['Net/ReLU[relu]/onnx::Relu', 6], ['Net/Linear[fc2]/onnx::Gemm', 33]]), (4122, [['Net/Linear[fc1]/onnx::Gemm', 4096], ['Net/ReLU[relu]/onnx::Relu', 4], ['Net/Linear[fc2]/onnx::Gemm', 22]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474030\n",
      "556470\n",
      "568836\n",
      "461664\n",
      "342126\n",
      "688374\n"
     ]
    }
   ],
   "source": [
    "flop_inflection_sim_comp = flops_2[inflection_sim_comp][0]\n",
    "print(flop_inflection_sim_comp)\n",
    "print(flops_2[0][0] - flop_inflection_sim_comp)\n",
    "flop_inflection_sim = flops_1[inflection_sim][0]\n",
    "print(flop_inflection_sim)\n",
    "print(flops_1[0][0] - flop_inflection_sim)\n",
    "flop_inflection_2_sim = flops_3[inflection_2_sim][0]\n",
    "print(flop_inflection_2_sim)\n",
    "print(flops_3[0][0] - flop_inflection_2_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_sim_comp = []\n",
    "for flops in flops_2:\n",
    "    flops_sim_comp.append(flops[0])\n",
    "flops_sim = []\n",
    "for flops in flops_1:\n",
    "    flops_sim.append(flops[0])\n",
    "flops_2_sim = []\n",
    "for flops in flops_3:\n",
    "    flops_2_sim.append(flops[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "fig = plt.figure()\n",
    "plt.plot(flops_sim_comp, test_accs)\n",
    "\n",
    "plt.suptitle('Accuracy vs FLOPs for Similarity and Complementary pruning')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "fig.savefig('similarandcomp_flops.png', dpi=300, bbox_inches='tight', pad_inches = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(flops_sim, test_accs_small_only)\n",
    "plt.suptitle('Accuracy vs FLOPs for Similar pruning method')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "fig.savefig('justsimilar_flop.png', dpi=300, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(flops_2_sim, test_accs_3)\n",
    "plt.suptitle('Accuracy vs FLOPs for Batch pruning via Similarity')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "fig.savefig('batchpruning2_flop.png', dpi=300, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
